{
  "117_Law of Large Numbers for Bayesian twolayer Neural Network trained with Variational Inference": {
    "title": "Law of Large Numbers for Bayesian two-layer Neural Network trained with Variational Inference",
    "abstract": "We provide a rigorous analysis of training by variational inference (VI) of Bayesian neural networks in the two-layer and infinite-width case. We consider a regression problem with a regularized evidence lower bound (ELBO) which is decomposed into the expected log-likelihood of the data and the Kullback-Leibler (KL) divergence between the a priori distribution and the variational posterior. With an appropriate weighting of the KL, we prove a law of large numbers for three different training schemes: (i) the idealized case with exact estimation of a multiple Gaussian integral from the reparametrization trick, (ii) a minibatch scheme using Monte Carlo sampling, commonly known as Bayes by Backprop, and (iii) a new and computationally cheaper algorithm which we introduce as Minimal VI. An important result is that all methods converge to the same mean-field limit. Finally, we illustrate our results numerically and discuss the need for the derivation of a central limit theorem.",
    "url": "https://proceedings.mlr.press/v195/descours23a.html",
    "id": "https://proceedings.mlr.press/v195/descours23a.html",
    "pdf": "https://proceedings.mlr.press/v195/descours23a/descours23a.pdf",
    "authors": {
      "0_Arnaud Descours": "Arnaud Descours",
      "1_Tom Huix": "Tom Huix",
      "2_Arnaud Guillin": "Arnaud Guillin",
      "3_Manon Michel": "Manon Michel",
      "4_\u00c9ric Moulines": "\u00c9ric Moulines",
      "5_Boris Nectoux": "Boris Nectoux"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v195/descours23a/descours23a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Sixth Conference on Learning Theory,\u00a0PMLR 195:4657-4695,\u00a02023.",
    "supplemental": "",
    "metric_match": 17.5,
    "metric_thres": 10.0
  },
  "21_Implicit Balancing and Regularization Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing": {
    "title": "Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing",
    "abstract": "Recently, there has been significant progress in understanding the convergence and generalization properties of gradient-based methods for training overparameterized learning models. However, many aspects including the role of small random initialization and how the various parameters of the model are coupled during gradient-based updates to facilitate good generalization, remain largely mysterious. A series of recent papers have begun to study this role for non-convex formulations of symmetric Positive Semi-Definite (PSD) matrix sensing problems which involve reconstructing a low-rank PSD matrix from a few linear measurements. The underlying symmetry/PSDness is crucial to existing convergence and generalization guarantees for this problem. In this paper, we study a general overparameterized low-rank matrix sensing problem where one wishes to reconstruct an asymmetric rectangular low-rank matrix from a few linear measurements. We prove that an overparameterized model trained via factorized gradient descent converges to the low-rank matrix generating the measurements. We show that in this setting, factorized gradient descent enjoys two implicit properties: (1) coupling of the trajectory of gradient descent where the factors are coupled in various ways throughout the gradient update trajectory and (2) an algorithmic regularization property where the iterates show a propensity towards low-rank models despite the overparameterized nature of the factorized model. These two implicit properties in turn allow us to show that the gradient descent trajectory from small random initialization moves towards solutions that are both globally optimal and generalize well.",
    "url": "https://proceedings.mlr.press/v195/soltanolkotabi23a.html",
    "id": "https://proceedings.mlr.press/v195/soltanolkotabi23a.html",
    "pdf": "https://proceedings.mlr.press/v195/soltanolkotabi23a/soltanolkotabi23a.pdf",
    "authors": {
      "0_Mahdi Soltanolkotabi": "Mahdi Soltanolkotabi",
      "1_Dominik St\u00f6ger": "Dominik St\u00f6ger",
      "2_Changzhi Xie": "Changzhi Xie"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v195/soltanolkotabi23a/soltanolkotabi23a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Sixth Conference on Learning Theory,\u00a0PMLR 195:5140-5142,\u00a02023.",
    "supplemental": "",
    "metric_match": 15.257,
    "metric_thres": 10.0
  },
  "254_Multilevel Gaussian Graphical Models Conditional on Covariates": {
    "title": "Multi-level Gaussian Graphical Models Conditional on Covariates",
    "abstract": "We address the problem of learning the structure of a high-dimensional Gaussian graphical model conditional on covariates, when each sample belongs to groups at multiple levels of hierarchy. The existing statistical methods for learning covariate-conditioned Gaussian  graphical models focused on learning the aggregate behavior of inputs and outputs in a single-layer network. We propose a statistical model called multi-level conditional Gaussian graphical models for modeling multi-level output networks influenced by both individual-level and group-level inputs. We describe a decomposition of our model into a product of two components, one for sum variables and the other for difference variables derived from the original variables. This decomposition leads to an efficient learning algorithm for both complete data and incomplete data with randomly missing individual observations, as the expensive repeated computation of the partition function can be avoided. We demonstrate our method on simulated data and real-world data in finance and genomics.",
    "url": "https://proceedings.mlr.press/v108/kim20d.html",
    "id": "https://proceedings.mlr.press/v108/kim20d.html",
    "pdf": "https://proceedings.mlr.press/v108/kim20d/kim20d.pdf",
    "authors": {
      "0_Gi Bum Kim": "Gi Bum Kim",
      "1_Seyoung Kim": "Seyoung Kim"
    },
    "pmlr_pdf": "http://proceedings.mlr.press/v108/kim20d/kim20d.pdf",
    "github_url": "",
    "conf_info": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,\u00a0PMLR 108:4216-4225,\u00a02020.",
    "supplemental": "",
    "metric_match": 23.924,
    "metric_thres": 10.0
  },
  "250_Simulator Calibration under Covariate Shift with Kernels": {
    "title": "Simulator Calibration under Covariate Shift with Kernels",
    "abstract": "We propose a novel calibration method for computer simulators, dealing with the problem of covariate shift.Covariate shift is the situation where input distributions for training and test are different, and ubiquitous in applications of simulations. Our approach is based on Bayesian inference with kernel mean embedding of distributions, and on the use of an importance-weighted reproducing kernel for covariate shift adaptation.We provide a theoretical analysis for the proposed method, including a novel theoretical result for conditional mean embedding, as well as empirical investigations suggesting its effectiveness in practice.The experiments include calibration of a widely used simulator for industrial manufacturing processes, where we also demonstrate how the proposed method may be useful for sensitivity analysis of model parameters.",
    "url": "https://proceedings.mlr.press/v108/kisamori20a.html",
    "id": "https://proceedings.mlr.press/v108/kisamori20a.html",
    "pdf": "https://proceedings.mlr.press/v108/kisamori20a/kisamori20a.pdf",
    "authors": {
      "0_Keiichi Kisamori": "Keiichi Kisamori",
      "1_Motonobu Kanagawa": "Motonobu Kanagawa",
      "2_Keisuke Yamazaki": "Keisuke Yamazaki"
    },
    "pmlr_pdf": "http://proceedings.mlr.press/v108/kisamori20a/kisamori20a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,\u00a0PMLR 108:1244-1253,\u00a02020.",
    "supplemental": "",
    "metric_match": 20.333,
    "metric_thres": 10.0
  },
  "235_Risk Bounds for Learning Multiple Components with PermutationInvariant Losses": {
    "title": "Risk Bounds for Learning Multiple Components with Permutation-Invariant Losses",
    "abstract": "This paper proposes a simple approach to derive efficient error bounds for learning multiple components with sparsity-inducing regularization. We show that for such regularization schemes, known decompositions of the Rademacher complexity over the components can be used in a more efficient manner to result in tighter bounds without too much effort. We give examples of application to switching regression and center-based clustering/vector quantization. Then, the complete workflow is illustrated on the problem of subspace clustering, for which decomposition results were not previously available. For all these problems, the proposed approach yields risk bounds with mild dependencies on the number of components and completely removes this dependence for nonconvex regularization schemes that could not be handled by previous methods.",
    "url": "https://proceedings.mlr.press/v108/lauer20a.html",
    "id": "https://proceedings.mlr.press/v108/lauer20a.html",
    "pdf": "https://proceedings.mlr.press/v108/lauer20a/lauer20a.pdf",
    "authors": {
      "0_Fabien Lauer": "Fabien Lauer"
    },
    "pmlr_pdf": "http://proceedings.mlr.press/v108/lauer20a/lauer20a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,\u00a0PMLR 108:1178-1187,\u00a02020.",
    "supplemental": "",
    "metric_match": 20.312,
    "metric_thres": 10.0
  },
  "328_Measuring Mutual Information Between All Pairs of Variables in Subquadratic Complexity": {
    "title": "Measuring Mutual Information Between All Pairs of Variables in Subquadratic Complexity",
    "abstract": "Finding associations between pairs of variables in large datasets is crucial for various disciplines. The brute force method for solving this problem requires computing the mutual information between $\\binom{N}{2}$ pairs. In this paper, we consider the problem of finding pairs of variables with high mutual information in sub-quadratic complexity. This problem is analogous to the nearest neighbor search, where the goal is to find pairs among $N$ variables that are similar to each other. To solve this problem, we develop a new algorithm for finding associations based on constructing a decision tree that assigns a hash to each variable, in a way that for pairs with higher mutual information, the chance of having the same hash is higher. For any $1 \\leq \\lambda \\leq 2$, we prove that in the case of binary data, we can reduce the number of necessary mutual information computations for finding all pairs satisfying $I(X, Y) > 2- \\lambda$ from $O(N^2)$ to $O(N^\\lambda)$,  where $I(X,Y)$ is the empirical mutual information between variables $X$ and $Y$. Finally, we confirmed our theory by experiments on simulated and real data. The implementation of our method and experiments is publicly available at \\href{https://github.com/mohimanilab/HashMI}{https://github.com/mohimanilab/HashMI}.",
    "url": "https://proceedings.mlr.press/v108/ferdosi20a.html",
    "id": "https://proceedings.mlr.press/v108/ferdosi20a.html",
    "pdf": "https://proceedings.mlr.press/v108/ferdosi20a/ferdosi20a.pdf",
    "authors": {
      "0_Mohsen Ferdosi": "Mohsen Ferdosi",
      "1_Arash Gholamidavoodi": "Arash Gholamidavoodi",
      "2_Hosein Mohimani": "Hosein Mohimani"
    },
    "pmlr_pdf": "http://proceedings.mlr.press/v108/ferdosi20a/ferdosi20a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,\u00a0PMLR 108:4399-4409,\u00a02020.",
    "supplemental": "",
    "metric_match": 18.764,
    "metric_thres": 10.0
  },
  "81_Halpern Iteration for NearOptimal and ParameterFree Monotone Inclusion and Strong Solutions to Variational Inequalities": {
    "title": "Halpern Iteration for Near-Optimal and Parameter-Free Monotone Inclusion and Strong Solutions to Variational Inequalities",
    "abstract": " We leverage the connections between nonexpansive maps, monotone Lipschitz operators, and proximal mappings to obtain near-optimal (i.e., optimal up to poly-log factors in terms of iteration complexity) and parameter-free methods for solving monotone inclusion problems. These results immediately translate into near-optimal guarantees for approximating strong solutions to variational inequality problems, approximating convex-concave min-max optimization problems, and minimizing the norm of the gradient in min-max optimization problems. Our analysis is based on a novel and simple potential-based proof of convergence of Halpern iteration, a classical iteration for finding fixed points of nonexpansive maps.  Additionally, we provide a series of algorithmic reductions that highlight connections between different problem classes and lead to lower bounds that certify near-optimality of the studied methods.",
    "url": "https://proceedings.mlr.press/v125/diakonikolas20a.html",
    "id": "https://proceedings.mlr.press/v125/diakonikolas20a.html",
    "pdf": "https://proceedings.mlr.press/v125/diakonikolas20a/diakonikolas20a.pdf",
    "authors": {
      "0_Jelena Diakonikolas": "Jelena Diakonikolas"
    },
    "pmlr_pdf": "http://proceedings.mlr.press/v125/diakonikolas20a/diakonikolas20a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Third Conference on Learning Theory,\u00a0PMLR 125:1428-1451,\u00a02020.",
    "supplemental": "",
    "metric_match": 18.764,
    "metric_thres": 10.0
  },
  "51_Approximate is Good Enough Probabilistic Variants of Dimensional and Margin Complexity": {
    "title": "Approximate is Good Enough: Probabilistic Variants of Dimensional and Margin Complexity",
    "abstract": " We present and study approximate notions of dimensional and margin complexity, which correspond to the minimal dimension or norm of an embedding required to {\\em approximate}, rather then exactly represent, a given hypothesis class.  We show that such notions are not only sufficient for learning using linear predictors or a kernel, but unlike the exact variants, are also necessary. Thus they are better suited for discussing limitations of linear or kernel methods.",
    "url": "https://proceedings.mlr.press/v125/kamath20b.html",
    "id": "https://proceedings.mlr.press/v125/kamath20b.html",
    "pdf": "https://proceedings.mlr.press/v125/kamath20b/kamath20b.pdf",
    "authors": {
      "0_Pritish Kamath": "Pritish Kamath",
      "1_Omar Montasser": "Omar Montasser",
      "2_Nathan Srebro": "Nathan Srebro"
    },
    "pmlr_pdf": "http://proceedings.mlr.press/v125/kamath20b/kamath20b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Third Conference on Learning Theory,\u00a0PMLR 125:2236-2262,\u00a02020.",
    "supplemental": "",
    "metric_match": 16.25,
    "metric_thres": 10.0
  }
}