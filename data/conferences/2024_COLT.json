{
  "0_GapFree Clustering Sensitivity and Robustness of SDP": {
    "title": "Gap-Free Clustering: Sensitivity and Robustness of SDP",
    "abstract": "We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous convex relaxation approaches achieving exact recovery do not allow any small clusters of size $o(\\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bounds of potential independent interest. Our results are robust to certain semirandom settings that are challenging for alternative algorithms. Using our gap-free clustering procedure, we obtain efficient algorithms for the problem of clustering with a faulty oracle with superior query complexities, notably achieving $o(n^2)$ sample complexity even in the presence of a large number of small clusters. Our gap-free clustering procedure also leads to improved algorithms for recursive clustering.",
    "url": "https://proceedings.mlr.press/v247/zurek24a.html",
    "id": "https://proceedings.mlr.press/v247/zurek24a.html",
    "pdf": "https://proceedings.mlr.press/v247/zurek24a/zurek24a.pdf",
    "authors": {
      "0_Matthew Zurek": "Matthew Zurek",
      "1_Yudong Chen": "Yudong Chen"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/zurek24a/zurek24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5231-5300,\u00a02024.",
    "supplemental": ""
  },
  "1_Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing Extended Abstract": {
    "title": "Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing (Extended Abstract)",
    "abstract": "We consider the problem of parameter estimation in a high-dimensional generalized linear model. Spectral methods obtained via the principal eigenvector of a suitable data-dependent matrix provide a simple yet surprisingly effective solution. However, despite their wide use, a rigorous performance characterization, as well as a principled way to preprocess the data, are available only for unstructured (i.i.d. Gaussian and Haar orthogonal) designs. In contrast, real-world data matrices are highly structured and exhibit non-trivial correlations. To address the problem, we consider correlated Gaussian designs capturing the anisotropic nature of the features via a covariance matrix $\\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators. This allows us to identify the optimal preprocessing that minimizes the number of samples needed for parameter estimation. Surprisingly, such preprocessing is universal across a broad set of statistical models, which partly addresses a conjecture on optimal spectral estimators for rotationally invariant designs. Our principled approach vastly improves upon previous heuristic methods, including for designs common in computational imaging and genetics. The proposed methodology, based on approximate message passing, is broadly applicable and opens the way to the precise characterization of spiked matrices and of the corresponding spectral methods in a variety of settings. ",
    "url": "https://proceedings.mlr.press/v247/zhang24c.html",
    "id": "https://proceedings.mlr.press/v247/zhang24c.html",
    "pdf": "https://proceedings.mlr.press/v247/zhang24c/zhang24c.pdf",
    "authors": {
      "0_Yihan Zhang": "Yihan Zhang",
      "1_Hong Chang Ji": "Hong Chang Ji",
      "2_Ramji Venkataramanan": "Ramji Venkataramanan",
      "3_Marco Mondelli": "Marco Mondelli"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/zhang24c/zhang24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5224-5230,\u00a02024.",
    "supplemental": ""
  },
  "2_Optimal MultiDistribution Learning": {
    "title": "Optimal Multi-Distribution Learning",
    "abstract": "Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc.  Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$,  we propose a novel algorithm that yields an $\\varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $\\frac{d+k}{\\varepsilon^2}$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely through an empirical risk minimization oracle. Additionally, we establish the necessity of randomization, unveiling a large sample size barrier when only deterministic hypotheses are permitted. These findings successfully resolve three open problems presented in COLT 2023 (i.e., Problems 1, 3 and 4 of Awasthi et al. 2023).    ",
    "url": "https://proceedings.mlr.press/v247/zhang24b.html",
    "id": "https://proceedings.mlr.press/v247/zhang24b.html",
    "pdf": "https://proceedings.mlr.press/v247/zhang24b/zhang24b.pdf",
    "authors": {
      "0_Zihan Zhang": "Zihan Zhang",
      "1_Wenhao Zhan": "Wenhao Zhan",
      "2_Yuxin Chen": "Yuxin Chen",
      "3_Simon S Du": "Simon S Du",
      "4_Jason D Lee": "Jason D Lee"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/zhang24b/zhang24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5220-5223,\u00a02024.",
    "supplemental": ""
  },
  "3_Settling the sample complexity of online reinforcement learning": {
    "title": "Settling the sample complexity of online reinforcement learning",
    "abstract": "A central issue lying at the heart of online reinforcement learning (RL) is data efficiency.  While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a \u201clarge-sample\u201d regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.   We settle this problem for finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of MVP (Monotonic Value Propagation), an optimistic model-based algorithm proposed by Zhang et al., achieves a regret on the order of $$\\min\\big\\{  \\sqrt{SAH^3K}, \\,HK \\big\\},$$ where $S$ is the number of states, $A$ is the number of actions, $H$ is the horizon length, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size K, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\\varepsilon$-accuracy) of $\\frac{SAH^3}{\\varepsilon^2}$ up to log factor, which is minimax-optimal for the full epsilon-range. Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in a novel analysis paradigm to decouple complicated statistical dependency \u2014 a long-standing challenge facing the analysis of online RL in the sample-hungry regime.   ",
    "url": "https://proceedings.mlr.press/v247/zhang24a.html",
    "id": "https://proceedings.mlr.press/v247/zhang24a.html",
    "pdf": "https://proceedings.mlr.press/v247/zhang24a/zhang24a.pdf",
    "authors": {
      "0_Zihan Zhang": "Zihan Zhang",
      "1_Yuxin Chen": "Yuxin Chen",
      "2_Jason D Lee": "Jason D Lee",
      "3_Simon S Du": "Simon S Du"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/zhang24a/zhang24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5213-5219,\u00a02024.",
    "supplemental": ""
  },
  "4_Fast twotimescale stochastic gradient method with applications in reinforcement learning": {
    "title": "Fast two-time-scale stochastic gradient method with applications in reinforcement learning",
    "abstract": "Two-time-scale optimization is a framework introduced in Zeng et al. (2024) that abstracts a range of policy evaluation and policy optimization problems in reinforcement learning (RL). Akin to bi-level optimization under a particular type of stochastic oracle, the two-time-scale optimization framework has an upper level objective whose gradient evaluation depends on the solution of a lower level problem, which is to find the root of a strongly monotone operator. In this work, we propose a new method for solving two-time-scale optimization that achieves significantly faster convergence than the prior arts. The key idea of our approach is to leverage an averaging step to improve the estimates of the operators in both lower and upper levels before using them to update the decision variables. These additional averaging steps eliminate the direct coupling between the main variables, enabling the accelerated performance of our algorithm. We characterize the finite-time convergence rates of the proposed algorithm under various conditions of the underlying objective function, including strong convexity, convexity, Polyak-Lojasiewicz condition, and general non-convexity. These rates significantly improve over the best-known complexity of the standard two-time-scale stochastic approximation algorithm. When applied to RL, we show how the proposed algorithm specializes to novel online sample-based methods that surpass or match the performance of the existing state of the art. Finally, we support our theoretical results with numerical simulations in RL.",
    "url": "https://proceedings.mlr.press/v247/zeng24a.html",
    "id": "https://proceedings.mlr.press/v247/zeng24a.html",
    "pdf": "https://proceedings.mlr.press/v247/zeng24a/zeng24a.pdf",
    "authors": {
      "0_Sihan Zeng": "Sihan Zeng",
      "1_Thinh Doan": "Thinh Doan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/zeng24a/zeng24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5166-5212,\u00a02024.",
    "supplemental": ""
  },
  "5_Counting Stars is ConstantDegree Optimal For Detecting Any Planted Subgraph Extended Abstract": {
    "title": "Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph: Extended Abstract",
    "abstract": "We prove that whenever $p=\\Omega(1)$ and for any graph $H$, counting $O(1)$-stars is optimal among all constant degree polynomial tests in terms of strongly separating an instance of $G(n,p),$ from the union of a random copy of $H$ with an instance of $G(n,p).$ Our work generalizes and extends multiple previous results on the inference abilities of $O(1)$-degree polynomials in the literature.",
    "url": "https://proceedings.mlr.press/v247/yu24a.html",
    "id": "https://proceedings.mlr.press/v247/yu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/yu24a/yu24a.pdf",
    "authors": {
      "0_Xifan Yu": "Xifan Yu",
      "1_Ilias Zadik": "Ilias Zadik",
      "2_Peiyuan Zhang": "Peiyuan Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/yu24a/yu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5163-5165,\u00a02024.",
    "supplemental": ""
  },
  "6_TopK ranking with a monotone adversary": {
    "title": "Top-$K$ ranking with a monotone adversary",
    "abstract": "In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges.  The statistician\u2019s goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph.  The main contribution of this paper is  to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\\log^2(n)$ factor, where $n$ denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations.  On the analytical front, we provide a refined\u00a0$\\ell_\\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the\u00a0$\\ell_\\infty$ error with the spectral properties of the weighted comparison graph. Motivated by this, our algorithmic innovation involves the development of an SDP-based approach to reweight the semi-random graph and meet specified spectral properties. Additionally, we propose a first-order method based on the Matrix Multiplicative Weight Update (MMWU) framework to solve the resulting SDP efficiently in nearly-linear time in the size of the semi-random comparison graph.",
    "url": "https://proceedings.mlr.press/v247/yang24b.html",
    "id": "https://proceedings.mlr.press/v247/yang24b.html",
    "pdf": "https://proceedings.mlr.press/v247/yang24b/yang24b.pdf",
    "authors": {
      "0_Yuepeng Yang": "Yuepeng Yang",
      "1_Antares Chen": "Antares Chen",
      "2_Lorenzo Orecchia": "Lorenzo Orecchia",
      "3_Cong Ma": "Cong Ma"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/yang24b/yang24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5123-5162,\u00a02024.",
    "supplemental": ""
  },
  "7_Multipleoutput composite quantile regression through an optimal transport lens": {
    "title": "Multiple-output composite quantile regression through an optimal transport lens",
    "abstract": "Composite quantile regression has been used to obtain robust estimators of regression coefficients in linear models with good statistical efficiency. By revealing an intrinsic link between the composite quantile regression loss function and the Wasserstein distance from the residuals to the set of quantiles, we establish a generalization of the composite quantile regression to the multiple-output settings. Theoretical convergence rates of the proposed estimator are derived both under the setting where the additive error possesses only a finite $\\ell$-th moment (for $\\ell > 2$) and where it exhibits a sub-Weibull tail. In doing so, we develop novel techniques for analyzing the M-estimation problem that involves Wasserstein-distance in the loss. Numerical studies confirm the practical effectiveness of our proposed procedure.",
    "url": "https://proceedings.mlr.press/v247/yang24a.html",
    "id": "https://proceedings.mlr.press/v247/yang24a.html",
    "pdf": "https://proceedings.mlr.press/v247/yang24a/yang24a.pdf",
    "authors": {
      "0_Xuzhi Yang": "Xuzhi Yang",
      "1_Tengyao Wang": "Tengyao Wang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/yang24a/yang24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5076-5122,\u00a02024.",
    "supplemental": ""
  },
  "8_Bridging the Gap Rademacher Complexity in Robust and Standard Generalization": {
    "title": "Bridging the Gap: Rademacher Complexity in Robust and Standard Generalization",
    "abstract": "Training Deep Neural Networks (DNNs) with adversarial examples often results in poor generalization to test-time adversarial data. This paper investigates this issue, known as adversarially robust generalization, through the lens of Rademacher complexity. Building upon the studies by Khim and Loh (2018); Yin et al. (2019), numerous works have been dedicated to this problem, yet achieving a satisfactory bound remains an elusive goal. Existing works on DNNs either apply to a surrogate loss instead of the robust loss or yield bounds that are notably looser compared to their standard counterparts. In the latter case, the bounds have a higher dependency on the width $m$ of the DNNs or the dimension $d$ of the data, with an extra factor of at least $\\mathcal{O}(\\sqrt{m})$ or $\\mathcal{O}(\\sqrt{d})$. This paper presents upper bounds for adversarial Rademacher complexity of DNNs that match the best-known upper bounds in standard settings, as established in the work  of Bartlett et al. (2017), with the dependency on width and dimension being $\\mathcal{O}(\\ln(dm))$. The central challenge addressed is calculating the covering number of adversarial function classes. We aim to construct a new cover that possesses two properties: 1) compatibility with adversarial examples, and 2) precision comparable to covers used in standard settings. To this end, we introduce a new variant of covering number called the \\emph{uniform covering number}, specifically designed and proven to reconcile these two properties. Consequently, our method effectively bridges the gap between Rademacher complexity in robust and standard generalization.",
    "url": "https://proceedings.mlr.press/v247/xiao24a.html",
    "id": "https://proceedings.mlr.press/v247/xiao24a.html",
    "pdf": "https://proceedings.mlr.press/v247/xiao24a/xiao24a.pdf",
    "authors": {
      "0_Jiancong Xiao": "Jiancong Xiao",
      "1_Ruoyu Sun": "Ruoyu Sun",
      "2_Qi Long": "Qi Long",
      "3_Weijie Su": "Weijie Su"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/xiao24a/xiao24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5074-5075,\u00a02024.",
    "supplemental": ""
  },
  "9_Large Stepsize Gradient Descent for Logistic Loss NonMonotonicity of the Loss Improves Optimization Efficiency": {
    "title": "Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency",
    "abstract": "We consider \\emph{gradient descent} (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly \u2014 in $O(\\eta)$ steps, and subsequently achieves an $\\tilde{O}(1 / (\\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an \\emph{accelerated} loss of $\\tilde{O}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{O}(1/T^2)$ acceleration), nonlinear predictors in the \\emph{neural tangent kernel} regime, and online \\emph{stochastic gradient descent} (SGD) with a large stepsize, under suitable separability conditions.",
    "url": "https://proceedings.mlr.press/v247/wu24b.html",
    "id": "https://proceedings.mlr.press/v247/wu24b.html",
    "pdf": "https://proceedings.mlr.press/v247/wu24b/wu24b.pdf",
    "authors": {
      "0_Jingfeng Wu": "Jingfeng Wu",
      "1_Peter L. Bartlett": "Peter L. Bartlett",
      "2_Matus Telgarsky": "Matus Telgarsky",
      "3_Bin Yu": "Bin Yu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wu24b/wu24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5019-5073,\u00a02024.",
    "supplemental": ""
  },
  "10_OracleEfficient Hybrid Online Learning with Unknown Distribution": {
    "title": "Oracle-Efficient Hybrid Online Learning with Unknown Distribution",
    "abstract": "We study the problem of oracle-efficient hybrid online learning when the features are generated by an unknown i.i.d. process and the labels are generated adversarially. Assuming access to an (offline) ERM oracle, we show that there exists a computationally efficient online predictor that achieves a regret upper bounded by $\\tilde{O}(T^{\\frac{3}{4}})$ for a finite-VC class, and upper bounded by $\\tilde{O}(T^{\\frac{p+1}{p+2}})$ for a class with $\\alpha$ fat-shattering dimension $\\alpha^{-p}$. This provides the first known oracle-efficient sublinear regret bounds for hybrid online learning with an unknown feature generation process. In particular, it confirms a conjecture of Lazaric and Munos (2012). We then extend our result to the scenario of shifting distributions with $K$ changes, yielding a regret of order $\\tilde{O}(T^{\\frac{4}{5}}K^{\\frac{1}{5}})$. Finally, we establish a regret of $\\tilde{O}((K^{\\frac{2}{3}}(\\log|\\mathcal{H}|)^{\\frac{1}{3}}+K)\\cdot T^{\\frac{4}{5}})$ for the contextual $K$-armed bandits with a finite policy set $\\mathcal{H}$, i.i.d. generated contexts from an unknown distribution, and adversarially generated costs.",
    "url": "https://proceedings.mlr.press/v247/wu24a.html",
    "id": "https://proceedings.mlr.press/v247/wu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/wu24a/wu24a.pdf",
    "authors": {
      "0_Changlong Wu": "Changlong Wu",
      "1_Jin Sima": "Jin Sima",
      "2_Wojciech Szpankowski": "Wojciech Szpankowski"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wu24a/wu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4992-5018,\u00a02024.",
    "supplemental": ""
  },
  "11_Optimal score estimation via empirical Bayes smoothing": {
    "title": "Optimal score estimation via empirical Bayes smoothing",
    "abstract": "We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss extensions to estimating $\\beta$-H\u00f6lder continuous scores with $\\beta \\leq 1$, as well as the implication of our theory on the sample complexity of score-based generative models.",
    "url": "https://proceedings.mlr.press/v247/wibisono24a.html",
    "id": "https://proceedings.mlr.press/v247/wibisono24a.html",
    "pdf": "https://proceedings.mlr.press/v247/wibisono24a/wibisono24a.pdf",
    "authors": {
      "0_Andre Wibisono": "Andre Wibisono",
      "1_Yihong Wu": "Yihong Wu",
      "2_Kaylee Yingxi Yang": "Kaylee Yingxi Yang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wibisono24a/wibisono24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4958-4991,\u00a02024.",
    "supplemental": ""
  },
  "12_Open problem Convergence of singletimescale meanfield Langevin descentascent for twoplayer zerosum games": {
    "title": "Open problem: Convergence of single-timescale mean-field Langevin descent-ascent for two-player zero-sum games",
    "abstract": "Let a smooth function $f: T^d \\times T^d \\to \\mathbb{R}$ over the $d$-torus and $\\beta>0$. Consider the min-max objective functional $F_\\beta(\\mu, \\nu) = \\iint f d\\mu d\\nu + \\beta^{-1} H(\\mu) - \\beta^{-1} H(\\nu)$ over $\\mathcal{P}(T^d) \\times \\mathcal{P}(T^d)$, where $H$ denotes the negative differential entropy. Its unique saddle point defines the entropy-regularized mixed Nash equilibrium of a two-player zero-sum game, and its Wasserstein gradient descent-ascent flow $(\\mu_t, \\nu_t)$ corresponds to the mean-field limit of a Langevin descent-ascent dynamics. Do $\\mu_t$ and $\\nu_t$ converge (weakly, say) as $t \\to \\infty$, for any $f$ and $\\beta$? This rather natural qualitative question is still open, and it is not clear whether it can be addressed using the tools currently available for the analysis of dynamics in Wasserstein space. Even though the simple trick of using a different timescale for the ascent versus the descent is known to guarantee convergence, we propose this question as a toy setting to further our understanding of the Wasserstein geometry for optimization.",
    "url": "https://proceedings.mlr.press/v247/wang24c.html",
    "id": "https://proceedings.mlr.press/v247/wang24c.html",
    "pdf": "https://proceedings.mlr.press/v247/wang24c/wang24c.pdf",
    "authors": {
      "0_Guillaume Wang": "Guillaume Wang",
      "1_L\u00e9na\u00efc Chizat": "L\u00e9na\u00efc Chizat"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wang24c/wang24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5345-5350,\u00a02024.",
    "supplemental": ""
  },
  "13_Nonlinear spiked covariance matrices and signal propagation in deep neural networks": {
    "title": "Nonlinear spiked covariance matrices and signal propagation in deep neural networks",
    "abstract": "Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the \u201cspike\u201d eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over training and characterize the alignment of the target function with the spike eigenvector of the CK on test data.",
    "url": "https://proceedings.mlr.press/v247/wang24b.html",
    "id": "https://proceedings.mlr.press/v247/wang24b.html",
    "pdf": "https://proceedings.mlr.press/v247/wang24b/wang24b.pdf",
    "authors": {
      "0_Zhichao Wang": "Zhichao Wang",
      "1_Denny Wu": "Denny Wu",
      "2_Zhou Fan": "Zhou Fan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wang24b/wang24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4891-4957,\u00a02024.",
    "supplemental": ""
  },
  "14_Efficient Algorithms for Attributed Graph Alignment with Vanishing Edge Correlation Extended Abstract": {
    "title": "Efficient Algorithms for Attributed Graph Alignment with Vanishing Edge Correlation Extended Abstract",
    "abstract": "Graph alignment refers to the task of finding the vertex correspondence between two correlated graphs of $n$ vertices. Extensive study has been done on polynomial-time algorithms for the graph alignment problem under the Erd\u0151s\u2013R\u00e9nyi graph pair model, where the two graphs are Erd\u0151s\u2013R\u00e9nyi graphs with edge probability $q_\\mathrm{u}$, correlated under certain vertex correspondence. To achieve exact recovery of the correspondence, all existing algorithms at least require the edge correlation coefficient $\\rho_\\mathrm{u}$ between the two graphs to be \\emph{non-vanishing} as $n\\rightarrow\\infty$. Moreover, it is conjectured that no polynomial-time algorithm can achieve exact recovery under vanishing edge correlation $\\rho_\\mathrm{u}<1/\\mathrm{polylog}(n)$. In this paper, we show that with a vanishing amount of additional \\emph{attribute information}, exact recovery is polynomial-time feasible under \\emph{vanishing} edge correlation $\\rho_\\mathrm{u} \\ge n^{-\\Theta(1)}$. We identify a \\emph{local} tree structure, which incorporates one layer of user information and one layer of attribute information, and apply the subgraph counting technique to such structures. A polynomial-time algorithm is proposed that recovers the vertex correspondence for most of the vertices, and then refines the output to achieve exact recovery. The consideration of attribute information is motivated by real-world applications like LinkedIn and Twitter, where user attributes like birthplace and education background can aid alignment.",
    "url": "https://proceedings.mlr.press/v247/wang24a.html",
    "id": "https://proceedings.mlr.press/v247/wang24a.html",
    "pdf": "https://proceedings.mlr.press/v247/wang24a/wang24a.pdf",
    "authors": {
      "0_Ziao Wang": "Ziao Wang",
      "1_Weina Wang": "Weina Wang",
      "2_Lele Wang": "Lele Wang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wang24a/wang24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4889-4890,\u00a02024.",
    "supplemental": ""
  },
  "15_Nearly Optimal Regret for Decentralized Online Convex Optimization": {
    "title": "Nearly Optimal Regret for Decentralized Online Convex Optimization",
    "abstract": "We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$. The primary technique is to design an online accelerated gossip strategy that enjoys a faster average consensus among local learners. Furthermore, by carefully exploiting the spectral properties of a specific network topology, we enhance the lower bounds for convex and strongly convex functions to $\\Omega(n\\rho^{-1/4}\\sqrt{T})$ and $\\Omega(n\\rho^{-1/2})$, respectively. These lower bounds suggest that our algorithms are nearly optimal in terms of $T$, $n$, and $\\rho$.",
    "url": "https://proceedings.mlr.press/v247/wan24a.html",
    "id": "https://proceedings.mlr.press/v247/wan24a.html",
    "pdf": "https://proceedings.mlr.press/v247/wan24a/wan24a.pdf",
    "authors": {
      "0_Yuanyu Wan": "Yuanyu Wan",
      "1_Tong Wei": "Tong Wei",
      "2_Mingli Song": "Mingli Song",
      "3_Lijun Zhang": "Lijun Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/wan24a/wan24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4862-4888,\u00a02024.",
    "supplemental": ""
  },
  "16_Pruning is Optimal for Learning Sparse Features in HighDimensions": {
    "title": "Pruning is Optimal for Learning Sparse Features in High-Dimensions",
    "abstract": "While it is commonly observed in practice that pruning networks to a certain level of sparsity can improve the quality of the features, a theoretical explanation of this phenomenon remains elusive.  In this work, we investigate this by demonstrating that a broad class of statistical models can be optimally learned using pruned neural networks trained with gradient descent, in high-dimensions.  We consider learning both single-index and multi-index models of the form $y = \\sigma^*(\\boldsymbol{V}^{\\top} \\boldsymbol{x}) + \\epsilon$,  where $\\sigma^*$  is a degree-$p$ polynomial,  and $\\boldsymbol{V} \\in \\mathbbm{R}^{d \\times r}$ with $r \\ll d$, is the matrix containing relevant model directions. We assume that $\\boldsymbol{V}$ satisfies a certain $\\ell_q$-sparsity condition for matrices and show that pruning neural networks proportional to the sparsity level of $\\boldsymbol{V}$ improves their sample complexity compared to unpruned networks.  Furthermore, we establish  Correlational Statistical Query (CSQ) lower bounds in this setting, which take the sparsity level of $\\boldsymbol{V}$ into account. We show that if the sparsity level of $\\boldsymbol{V}$ exceeds a certain threshold, training pruned networks with a gradient descent algorithm achieves the sample complexity suggested by the CSQ lower bound.  In the same scenario, however,  our results imply that basis-independent methods such as models trained via standard gradient descent initialized with rotationally invariant random weights can provably achieve only suboptimal sample complexity.",
    "url": "https://proceedings.mlr.press/v247/vural24a.html",
    "id": "https://proceedings.mlr.press/v247/vural24a.html",
    "pdf": "https://proceedings.mlr.press/v247/vural24a/vural24a.pdf",
    "authors": {
      "0_Nuri Mert Vural": "Nuri Mert Vural",
      "1_Murat A Erdogdu": "Murat A Erdogdu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/vural24a/vural24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4787-4861,\u00a02024.",
    "supplemental": ""
  },
  "17_Active Learning with Simple Questions": {
    "title": "Active Learning with Simple Questions",
    "abstract": "We consider an active learning setting where a learner is presented with a pool $S$ of $n$ unlabeled examples belonging to a domain $\\mathcal X$ and asks queries to find the underlying labeling that agrees with a target concept $h^\\ast \\in \\mathcal H$.  In contrast to traditional active learning that queries a single example for its label, we study more general \\emph{region queries} that allow the learner to pick a subset of the domain $T \\subset \\mathcal X$ and a target label $y$ and ask a labeler whether $h^\\ast(x) = y $ for every example in the set $T \\cap S$. Such more powerful queries allow us to bypass the limitations of traditional active learning and use significantly fewer rounds of interactions to learn but can potentially lead to a significantly more complex query language. Our main contribution is quantifying the trade-off between the number of queries and the complexity of the query language used by the learner. We measure the complexity of the region queries via the VC dimension of the family of regions. We show that given any hypothesis class $\\H$ with VC dimension $d$, one can design a region query family $Q$ with VC dimension $6d$ such that for every set of $n$ examples $S \\subset \\X$ and every $h^* \\in \\H$, a learner can submit $O(d\\log n)$ queries from $Q$ to a labeler and perfectly label $S$. We show a matching lower bound by designing a hypothesis class $\\H$ with VC dimension $d$ and a dataset $S \\subset \\X$ of size $n$ such that any learning algorithm using any query class with VC dimension $(d-2)/3$ must make $\\poly(n)$ queries to label $S$ perfectly. Finally, we focus on well-studied hypothesis classes including unions of intervals, high-dimensional boxes, and $d$-dimensional halfspaces, and obtain stronger results. In particular, we design learning algorithms that (i) are computationally efficient and (ii) work even when the queries are not answered based on the learner\u2019s pool of examples $S$ but on some unknown superset $L$ of $S$. ",
    "url": "https://proceedings.mlr.press/v247/vasilis24a.html",
    "id": "https://proceedings.mlr.press/v247/vasilis24a.html",
    "pdf": "https://proceedings.mlr.press/v247/vasilis24a/vasilis24a.pdf",
    "authors": {
      "0_Kontonis Vasilis": "Kontonis Vasilis",
      "1_Ma Mingchen": "Ma Mingchen",
      "2_Tzamos Christos": "Tzamos Christos"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/vasilis24a/vasilis24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3064-3098,\u00a02024.",
    "supplemental": ""
  },
  "18_Open Problem Order Optimal Regret Bounds for KernelBased Reinforcement Learning": {
    "title": "Open Problem: Order Optimal Regret Bounds for Kernel-Based Reinforcement Learning",
    "abstract": "Reinforcement Learning (RL) has shown great empirical success in various application domains. The theoretical aspects of the problem have been extensively studied over past decades, particularly under tabular and linear Markov Decision Process structures. Recently, non-linear function approximation using kernel-based prediction has gained traction. This approach is particularly interesting as it naturally extends the linear structure, and helps explain the behavior of neural-network-based models at their infinite width limit. The analytical results however do not adequately address the performance guarantees for this case. We will highlight this open problem, overview existing partial results, and discuss related challenges.",
    "url": "https://proceedings.mlr.press/v247/vakili24a.html",
    "id": "https://proceedings.mlr.press/v247/vakili24a.html",
    "pdf": "https://proceedings.mlr.press/v247/vakili24a/vakili24a.pdf",
    "authors": {
      "0_Sattar Vakili": "Sattar Vakili"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/vakili24a/vakili24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5340-5344,\u00a02024.",
    "supplemental": ""
  },
  "19_Improved Hardness Results for Learning Intersections of Halfspaces": {
    "title": "Improved Hardness Results for Learning Intersections of Halfspaces",
    "abstract": "We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. Strikingly little is known about this problem. For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige\u2019s 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces\u00a0(KS06, KS09, DS16). With intersections of fewer halfspaces being only ruled out under less standard assumptions\u00a0(DV21) (such as the existence of local pseudo-random generators with large stretch). We significantly narrow this gap by showing that even learning $\\omega(\\log \\log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on worst-case lattice problems (namely that SVP and SIVP are hard to approximate within polynomial factors). Further, we give unconditional hardness results in the statistical query framework. Specifically, we show that for any $k$ (even constant), learning $k$ halfspaces in dimension $N$ requires accuracy $N^{-\\Omega(k)}$, or exponentially many queries \u2013 in particular ruling out SQ algorithms with polynomial accuracy for $\\omega(1)$ halfspaces. To the best of our knowledge this is the first unconditional hardness result for learning a super-constant number of halfspaces. Our lower bounds are obtained in a unified way via a novel connection we make between intersections of halfspaces and the so-called parallel pancakes distribution\u00a0(DKS17, PLBR19, BRST21) that has been at the heart of many lower bound constructions in (robust)  high-dimensional statistics in the past few years.",
    "url": "https://proceedings.mlr.press/v247/tiegel24a.html",
    "id": "https://proceedings.mlr.press/v247/tiegel24a.html",
    "pdf": "https://proceedings.mlr.press/v247/tiegel24a/tiegel24a.pdf",
    "authors": {
      "0_Stefan Tiegel": "Stefan Tiegel"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/tiegel24a/tiegel24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4764-4786,\u00a02024.",
    "supplemental": ""
  },
  "20_Second Order Methods for Bandit Optimization and Control": {
    "title": "Second Order Methods for Bandit Optimization and Control",
    "abstract": "Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty.  While tight regret bounds for general convex losses have been established,  existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data. In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that satisfy a condition we call $\\kappa$-convexity. This class contains a wide range of practically relevant loss functions including linear losses, quadratic losses, and generalized linear models. In addition to optimal regret,  this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression. Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with memory. We show that for loss functions with a certain affine structure, the extended algorithm attains optimal regret. This leads to an algorithm with optimal regret for bandit LQ problem under a fully adversarial noise model, thereby resolving an open question posed in Grade et. al. 2020 and Sun et. al. 2023. Finally, we show that the more general problem of BCO with (non-affine) memory is harder. We derive a  $\\tilde{\\Omega}(T^{2/3})$ regret lower bound, even under the assumption of smooth and quadratic losses.",
    "url": "https://proceedings.mlr.press/v247/suggala24a.html",
    "id": "https://proceedings.mlr.press/v247/suggala24a.html",
    "pdf": "https://proceedings.mlr.press/v247/suggala24a/suggala24a.pdf",
    "authors": {
      "0_Arun Suggala": "Arun Suggala",
      "1_Y Jennifer Sun": "Y Jennifer Sun",
      "2_Praneeth Netrapalli": "Praneeth Netrapalli",
      "3_Elad Hazan": "Elad Hazan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/suggala24a/suggala24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4691-4763,\u00a02024.",
    "supplemental": ""
  },
  "21_A nonbacktracking method for long matrix and tensor completion": {
    "title": "A non-backtracking method for long matrix and tensor completion",
    "abstract": " We consider the problem of low-rank rectangular matrix completion in the regime where the matrix $M$  of size $n\\times m$ is \u201clong\", i.e., the aspect ratio $m/n$ diverges to infinity. Such matrices are of particular interest in the study of tensor completion, where they arise from the unfolding of a low-rank tensor. In the case where the sampling probability is $\\frac{d}{\\sqrt{mn}}$, we propose a new spectral algorithm for recovering the singular values and left singular vectors of the original matrix $M$ based on a variant of the standard non-backtracking operator of a suitably defined bipartite weighted random graph, which we call a \\textit{non-backtracking wedge operator}. When $d$ is above a Kesten-Stigum-type sampling threshold, our algorithm recovers a correlated version of the singular value decomposition of $M$ with quantifiable error bounds. This is the first result in the regime of bounded $d$ for weak recovery and the first for weak consistency when $d\\to\\infty$ arbitrarily slowly without any polylog factors. As an application, for low-CP-rank orthogonal $k$-tensor completion,  we efficiently achieve weak recovery with sample size $O(n^{k/2})$ and weak consistency with sample size $\\omega(n^{k/2})$. A similar result is obtained for low-multilinear-rank tensor completion with $O(n^{k/2})$ many samples.",
    "url": "https://proceedings.mlr.press/v247/stephan24a.html",
    "id": "https://proceedings.mlr.press/v247/stephan24a.html",
    "pdf": "https://proceedings.mlr.press/v247/stephan24a/stephan24a.pdf",
    "authors": {
      "0_Ludovic Stephan": "Ludovic Stephan",
      "1_Yizhe Zhu": "Yizhe Zhu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/stephan24a/stephan24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4636-4690,\u00a02024.",
    "supplemental": ""
  },
  "22_Fast sampling from constrained spaces using the Metropolisadjusted Mirror Langevin algorithm": {
    "title": "Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm",
    "abstract": "We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al, 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the error tolerance for approximate constrained sampling.",
    "url": "https://proceedings.mlr.press/v247/srinivasan24a.html",
    "id": "https://proceedings.mlr.press/v247/srinivasan24a.html",
    "pdf": "https://proceedings.mlr.press/v247/srinivasan24a/srinivasan24a.pdf",
    "authors": {
      "0_Vishwak Srinivasan": "Vishwak Srinivasan",
      "1_Andre Wibisono": "Andre Wibisono",
      "2_Ashia Wilson": "Ashia Wilson"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/srinivasan24a/srinivasan24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4593-4635,\u00a02024.",
    "supplemental": ""
  },
  "23_A  NonAdaptive Algorithm for the Quantitative Group Testing Problem": {
    "title": "A  Non-Adaptive Algorithm for the Quantitative Group Testing Problem",
    "abstract": "Consider an $n$-dimensional binary feature vector with $k$ non-zero entries. The vector can be interpreted as the incident vector corresponding to $n$ items out of which $k$ items are \\emph{defective}. The \\emph{quantitative group testing} (QGT) problem aims at learning this binary feature vector by queries on subsets of the items that return the total number of defective items. We consider this problem under the \\emph{non-adaptive} scenario where the queries on subsets are designed collectively and can be executed in parallel. Most of the existing efficient non-adaptive algorithms for the sublinear regime where $k = n^\\alpha$ with $0 < \\alpha < 1$ fall short of the information-theoretic lower bound, with a multiplicative gap of $\\log k$. Recently, a near-optimal non-adaptive algorithm with a decoding complexity of $O(n^3)$ closed this gap. In this work, we present a concatenated construction method yielding a non-adaptive algorithm with a decoding complexity of $O(n^{2\\alpha} + n \\log^2 n)$. The probability of decoding failure is analyzed by establishing a connection between the QGT problem and the so-called \\emph{balls into bins} problem. Our algorithm reduces the gap between the information-theoretic and computational bound for the number of required queries/tests from $\\log k$ to $\\log \\log k$. This narrows the gap in the number of tests for non-adaptive algorithms within the class of algorithms with $o(n^2)$ decoding complexity. Moreover, although our algorithm exhibits a $\\log \\log k$ gap in terms of the number of tests, it is surpassed by the existing asymptotically optimal construction only in scenarios where $k$ is exceptionally large for moderate values of $\\alpha$, such as $k > 10^{27}$ for $\\alpha = 0.7$, thereby highlighting the practical applicability of our proposed concatenated construction.",
    "url": "https://proceedings.mlr.press/v247/soleymani24a.html",
    "id": "https://proceedings.mlr.press/v247/soleymani24a.html",
    "pdf": "https://proceedings.mlr.press/v247/soleymani24a/soleymani24a.pdf",
    "authors": {
      "0_Mahdi Soleymani": "Mahdi Soleymani",
      "1_Tara Javidi": "Tara Javidi"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/soleymani24a/soleymani24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4574-4592,\u00a02024.",
    "supplemental": ""
  },
  "24_Training Dynamics of MultiHead Softmax Attention for InContext Learning Emergence Convergence and Optimality extended abstract": {
    "title": "Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality (extended abstract)",
    "abstract": "We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression.  We establish the global   convergence  of gradient flow under suitable choices of initialization. In addition,  we prove that  an interesting \u201ctask allocation\" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases \u2014 a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase  where the attention parameters converge to a limit.  Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor.  Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models.  The key technique for our convergence  analysis is to map  the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation.   To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model. ",
    "url": "https://proceedings.mlr.press/v247/siyu24a.html",
    "id": "https://proceedings.mlr.press/v247/siyu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/siyu24a/siyu24a.pdf",
    "authors": {
      "0_Chen Siyu": "Chen Siyu",
      "1_Sheen Heejune": "Sheen Heejune",
      "2_Wang Tianhao": "Wang Tianhao",
      "3_Yang Zhuoran": "Yang Zhuoran"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/siyu24a/siyu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4573-4573,\u00a02024.",
    "supplemental": ""
  },
  "25_Improved HighProbability Bounds for the Temporal Difference Learning Algorithm via Exponential Stability": {
    "title": "Improved High-Probability Bounds for the Temporal Difference Learning Algorithm via Exponential Stability",
    "abstract": "In this paper we consider the problem of obtaining sharp bounds for the performance of temporal difference (TD) methods with linear function approximation for policy evaluation in discounted Markov decision processes. We show that a simple algorithm with a universal and instance-independent step size together with Polyak-Ruppert tail averaging is sufficient to obtain near-optimal variance and bias terms. We also provide the respective sample complexity bounds. Our proof technique is based on refined error bounds for linear stochastic approximation together with the novel stability result for the product of random matrices that arise from the TD-type recurrence.",
    "url": "https://proceedings.mlr.press/v247/samsonov24a.html",
    "id": "https://proceedings.mlr.press/v247/samsonov24a.html",
    "pdf": "https://proceedings.mlr.press/v247/samsonov24a/samsonov24a.pdf",
    "authors": {
      "0_Sergey Samsonov": "Sergey Samsonov",
      "1_Daniil Tiapkin": "Daniil Tiapkin",
      "2_Alexey Naumov": "Alexey Naumov",
      "3_Eric Moulines": "Eric Moulines"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/samsonov24a/samsonov24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4511-4547,\u00a02024.",
    "supplemental": ""
  },
  "26_Provable Advantage in Quantum PAC Learning": {
    "title": "Provable Advantage in Quantum PAC Learning",
    "abstract": "We revisit the problem of characterising the complexity of Quantum PAC learning, as introduced by Bshouty and Jackson [SIAM J. Comput. 1998, 28, 1136\u20131153]. Several quantum advantages have been demonstrated in this setting, however, none are generic: they apply to particular concept classes and typically only work when the distribution that generates the data is known. In the general case, it was recently shown by Arunachalam and de Wolf [JMLR, 19 (2018) 1-36] that quantum PAC learners can only achieve constant factor advantages over classical PAC learners. We show that with a natural extension of the definition of quantum PAC learning used by Arunachalam and de Wolf, we can achieve a generic advantage in quantum learning. To be precise, for any concept class $\\mathcal{C}$ of VC dimension $d$, we show there is an $(\\epsilon, \\delta)$-quantum PAC learner with sample complexity \\[{O}\\left(\\frac{1}{\\sqrt{\\epsilon}}\\left[d+ \\log(\\frac{1}{\\delta})\\right]\\log^9(1/\\epsilon)\\right). \\]{Up} to polylogarithmic factors, this is a square root improvement over the classical learning sample complexity. We show the tightness of our result by proving an $\\Omega(d/\\sqrt{\\epsilon})$ lower bound that matches our upper bound up to polylogarithmic factors.",
    "url": "https://proceedings.mlr.press/v247/salmon24a.html",
    "id": "https://proceedings.mlr.press/v247/salmon24a.html",
    "pdf": "https://proceedings.mlr.press/v247/salmon24a/salmon24a.pdf",
    "authors": {
      "0_Wilfred Salmon": "Wilfred Salmon",
      "1_Sergii Strelchuk": "Sergii Strelchuk",
      "2_Tom Gur": "Tom Gur"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/salmon24a/salmon24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4487-4510,\u00a02024.",
    "supplemental": ""
  },
  "27_Online Structured Prediction with FenchelYoung Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss": {
    "title": "Online Structured Prediction with Fenchel\u2013Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss",
    "abstract": "This paper studies online structured prediction with full-information feedback. For online multiclass classification, Van der Hoeven (2020) established \\emph{finite} surrogate regret bounds, which are independent of the time horizon, by introducing an elegant \\emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \\emph{Fenchel\u2013Young losses}, a large family of surrogate losses that includes the logistic loss for multiclass classification as a special case, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \\emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we obtain a surrogate regret bound of $O(\\| \\bm{U} \\|_\\mathrm{F}^2)$, where $\\bm{U}$ is the best offline linear estimator and $\\| \\cdot \\|_\\mathrm{F}$ denotes the Frobenius norm. This bound is tight up to logarithmic factors and improves the previous bound of $O(d\\| \\bm{U} \\|_\\mathrm{F}^2)$ due to Van der Hoeven (2020) by a factor of $d$, the number of classes.",
    "url": "https://proceedings.mlr.press/v247/sakaue24a.html",
    "id": "https://proceedings.mlr.press/v247/sakaue24a.html",
    "pdf": "https://proceedings.mlr.press/v247/sakaue24a/sakaue24a.pdf",
    "authors": {
      "0_Shinsaku Sakaue": "Shinsaku Sakaue",
      "1_Han Bao": "Han Bao",
      "2_Taira Tsuchiya": "Taira Tsuchiya",
      "3_Taihei Oki": "Taihei Oki"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/sakaue24a/sakaue24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4458-4486,\u00a02024.",
    "supplemental": ""
  },
  "28_Online Learning with Setvalued Feedback": {
    "title": "Online Learning with Set-valued Feedback",
    "abstract": "We study a variant of online multiclass classification where the learner predicts a single label but receives a \\textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \\textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension characterizes online learnability in the agnostic setting and tightly quantifies the minimax regret. Finally, we use our results to establish bounds on the minimax regret for three practical learning settings:  online multilabel ranking,  online multilabel classification, and real-valued prediction with interval-valued response.",
    "url": "https://proceedings.mlr.press/v247/raman24b.html",
    "id": "https://proceedings.mlr.press/v247/raman24b.html",
    "pdf": "https://proceedings.mlr.press/v247/raman24b/raman24b.pdf",
    "authors": {
      "0_Vinod Raman": "Vinod Raman",
      "1_Unique Subedi": "Unique Subedi",
      "2_Ambuj Tewari": "Ambuj Tewari"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/raman24b/raman24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4381-4412,\u00a02024.",
    "supplemental": ""
  },
  "29_Apple Tasting Combinatorial Dimensions and Minimax Rates": {
    "title": "Apple Tasting: Combinatorial Dimensions and Minimax Rates",
    "abstract": "In online binary classification under \\emph{apple tasting} feedback, the learner only observes the true label if it predicts \u201c1\". First studied by Helmbold et al. (2000a),  we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. We show that the Littlestone dimension continues to provide a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by Helmbold et al. (2000a). In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected number of mistakes in the realizable setting.  As a corollary, we use the Effective width to establish a \\emph{trichotomy} of the minimax expected number of mistakes in the realizable setting. In particular, we show that in the realizable setting, the expected number of mistakes of any learner, under apple tasting feedback, can only be either $\\Theta(1), \\Theta(\\sqrt{T})$, or $\\Theta(T)$. This is in contrast to the full-information realizable setting where only $\\Theta(1)$ and $\\Theta(T)$ are possible. ",
    "url": "https://proceedings.mlr.press/v247/raman24a.html",
    "id": "https://proceedings.mlr.press/v247/raman24a.html",
    "pdf": "https://proceedings.mlr.press/v247/raman24a/raman24a.pdf",
    "authors": {
      "0_Vinod Raman": "Vinod Raman",
      "1_Unique Subedi": "Unique Subedi",
      "2_Ananth Raman": "Ananth Raman",
      "3_Ambuj Tewari": "Ambuj Tewari"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/raman24a/raman24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4358-4380,\u00a02024.",
    "supplemental": ""
  },
  "30_Fit Like You Sample SampleEfficient Generalized Score Matching from Fast Mixing Diffusions": {
    "title": "Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Diffusions",
    "abstract": "Score matching is an approach to learning probability distributions parametrized up to a constant of proportionality (e.g., energy-based models). The idea is to fit the score of the distribution rather than the likelihood, thus avoiding the need to evaluate the constant of proportionality. While there\u2019s a clear algorithmic benefit, the statistical cost can be steep: recent work by Koehler et al. (2022) showed that for distributions that have poor isoperimetric properties (a large Poincar{\u00e9} or log-Sobolev constant), score matching is substantially statistically less efficient than maximum likelihood. However, many natural realistic distributions, e.g. multimodal distributions as simple as a mixture of two Gaussians in one dimension have a poor Poincar{\u00e9} constant. In this paper, we show a close connection between the mixing time of a broad class of Markov processes with generator L and stationary distribution p, and an appropriately chosen generalized score matching loss that tries to fit Op. In the special case of O being a gradient operator, and L being the generator of Langevin diffusion, this generalizes and recovers the results from Koehler et al. (2022). This allows us to adapt techniques to speed up Markov chains to construct better score-matching losses. In particular, \"preconditioning\" the diffusion can be translated to an appropriate \"preconditioning\" of the score loss. Lifting the chain by adding a temperature like in simulated tempering can be shown to result in a Gaussian-convolution annealed score matching loss, similar to Song and Ermon (2019). Moreover, we show that if the distribution being learned is a finite mixture of Gaussians in d dimensions with a shared covariance, the sample complexity of annealed score matching is polynomial in the ambient dimension, the diameter of the means, and the smallest and largest eigenvalues of the covariance. To show this we bound the mixing time of a \"continuously tempered\" version of Langevin diffusion for mixtures, which is of standalone interest.",
    "url": "https://proceedings.mlr.press/v247/qin24a.html",
    "id": "https://proceedings.mlr.press/v247/qin24a.html",
    "pdf": "https://proceedings.mlr.press/v247/qin24a/qin24a.pdf",
    "authors": {
      "0_Yilong Qin": "Yilong Qin",
      "1_Andrej Risteski": "Andrej Risteski"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/qin24a/qin24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4413-4457,\u00a02024.",
    "supplemental": ""
  },
  "31_On the Distance from Calibration in Sequential Prediction": {
    "title": "On the Distance from Calibration in Sequential Prediction",
    "abstract": "We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B\u0142{}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants. We prove that there is a forecasting algorithm that achieves an $O(\\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a continuous relaxation of the former. We then show that an $O(\\sqrt{T})$ lower calibration distance can be achieved via a simple minimax argument and a reduction to online learning on a Lipschitz class. On the lower bound side, an $\\Omega(T^{1/3})$ calibration distance is shown to be unavoidable, even when the adversary outputs a sequence of independent random bits, and has an additional ability to early stop (i.e., to stop producing random bits and output the same bit in the remaining steps). Interestingly, without this early stopping, the forecaster can achieve a much smaller calibration distance of $\\mathrm{polylog}(T)$.",
    "url": "https://proceedings.mlr.press/v247/qiao24a.html",
    "id": "https://proceedings.mlr.press/v247/qiao24a.html",
    "pdf": "https://proceedings.mlr.press/v247/qiao24a/qiao24a.pdf",
    "authors": {
      "0_Mingda Qiao": "Mingda Qiao",
      "1_Letian Zheng": "Letian Zheng"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/qiao24a/qiao24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4307-4357,\u00a02024.",
    "supplemental": ""
  },
  "32_Dimensionfree Structured Covariance Estimation": {
    "title": "Dimension-free Structured Covariance Estimation",
    "abstract": "Given a sample of i.i.d. high-dimensional centered random vectors, we consider a problem of estimation of their covariance matrix $\\Sigma$ with an additional assumption that $\\Sigma$ can be represented as a sum of a few Kronecker products of smaller matrices. Under mild conditions, we derive the first non-asymptotic dimension-free high-probability bound on the Frobenius distance between $\\Sigma$ and a widely used penalized permuted least squares estimate. Because of the hidden structure, the established rate of convergence is faster than in the standard covariance estimation problem.",
    "url": "https://proceedings.mlr.press/v247/puchkin24a.html",
    "id": "https://proceedings.mlr.press/v247/puchkin24a.html",
    "pdf": "https://proceedings.mlr.press/v247/puchkin24a/puchkin24a.pdf",
    "authors": {
      "0_Nikita Puchkin": "Nikita Puchkin",
      "1_Maxim Rakhuba": "Maxim Rakhuba"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/puchkin24a/puchkin24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4276-4306,\u00a02024.",
    "supplemental": ""
  },
  "33_SampleOptimal Locally Private Hypothesis Selection and the Provable Benefits of Interactivity": {
    "title": "Sample-Optimal Locally Private Hypothesis Selection and the Provable Benefits of Interactivity",
    "abstract": "We study the problem of hypothesis selection under the constraint of local differential privacy. Given a class $\\mathcal{F}$ of $k$ distributions and a set of i.i.d. samples from an unknown distribution $h$, the goal of hypothesis selection is to pick a distribution $\\hat{f}$ whose total variation distance to $h$ is comparable with the best distribution in $\\mathcal{F}$ (with high probability). We devise an $\\varepsilon$-locally-differentially-private ($\\varepsilon$-LDP) algorithm that uses $\\Theta\\left(\\frac{k}{\\alpha^2\\min \\{\\varepsilon^2,1\\}}\\right)$ samples to guarantee that $d_{TV}(h,\\hat{f})\\leq \\alpha + 9 \\min_{f\\in \\mathcal{F}}d_{TV}(h,f)$ with high probability. This sample complexity is optimal for $varepsilon<1$, matching the lower bound of Gopi et al. (2020). All previously known algorithms for this problem required $\\Omega\\left(\\frac{k\\log k}{\\alpha^2\\min \\{\\varepsilon^2 ,1\\}} \\right)$ samples to work.  Moreover, our result demonstrates the power of interaction for $\\varepsilon$-LDP hypothesis selection. Namely, it breaks the known lower bound of $\\Omega\\left(\\frac{k\\log k}{\\alpha^2 \\varepsilon^2} \\right)$ for the sample complexity of non-interactive hypothesis selection. Our algorithm achieves this using only $\\Theta(\\log \\log k)$ rounds of interaction. To prove our results, we define the notion of \\emph{critical queries} for a Statistical Query Algorithm (SQA) which may be of independent interest. Informally, an SQA is said to use a small number of critical queries if its success relies on the accuracy of only a small number of queries it asks. We then design an LDP algorithm that uses a smaller number of critical queries.",
    "url": "https://proceedings.mlr.press/v247/pour24a.html",
    "id": "https://proceedings.mlr.press/v247/pour24a.html",
    "pdf": "https://proceedings.mlr.press/v247/pour24a/pour24a.pdf",
    "authors": {
      "0_Alireza F. Pour": "Alireza F. Pour",
      "1_Hassan Ashtiani": "Hassan Ashtiani",
      "2_Shahab Asoodeh": "Shahab Asoodeh"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/pour24a/pour24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4240-4275,\u00a02024.",
    "supplemental": ""
  },
  "34_Smooth Lower Bounds for Differentially Private Algorithms via PaddingandPermuting Fingerprinting Codes": {
    "title": "Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes",
    "abstract": "Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don\u2019t know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold. In this work, we present a new framework and tools to generate smooth lower bounds on the sample complexity of differentially private algorithms satisfying very weak accuracy. We illustrate the applicability of our method by providing new lower bounds in various settings: 1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016). 2. A lower bound on the additive error of DP algorithms for approximate k-means clustering and general (k,z)-clustering, as a function of the multiplicative error, which is tight for a constant multiplication error. 3. A lower bound for estimating the top singular vector of a matrix under DP in low-accuracy regimes, which is a special case of DP subspace estimation studied by Singhal and Steinke (NeurIPS 2021). Our main technique is to apply a padding-and-permuting transformation to a fingerprinting code. However, rather than proving our results using a black-box access to an existing fingerprinting code (e.g., Tardos\u2019 code), we develop a new fingerprinting lemma that is stronger than those of Dwork et al. (FOCS 2015) and Bun et al. (SODA 2017), and prove our lower bounds directly from the lemma. Our lemma, in particular, gives a simpler fingerprinting code construction with optimal rate (up to polylogarithmic factors) that is of independent interest.",
    "url": "https://proceedings.mlr.press/v247/peter24a.html",
    "id": "https://proceedings.mlr.press/v247/peter24a.html",
    "pdf": "https://proceedings.mlr.press/v247/peter24a/peter24a.pdf",
    "authors": {
      "0_Naty Peter": "Naty Peter",
      "1_Eliad Tsfadia": "Eliad Tsfadia",
      "2_Jonathan Ullman": "Jonathan Ullman"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/peter24a/peter24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4207-4239,\u00a02024.",
    "supplemental": ""
  },
  "35_The Sample Complexity of Simple Binary Hypothesis Testing": {
    "title": "The Sample Complexity of Simple Binary Hypothesis Testing",
    "abstract": "The sample complexity of simple binary hypothesis testing is the smallest number of i.i.d. samples required to distinguish between two distributions $p$ and $q$ in either: (i) the prior-free setting, with type-I error at most $\\alpha$ and type-II error at most $\\beta$; or (ii) the Bayesian setting, with Bayes error at most $\\delta$ and prior distribution $(\\alpha, 1-\\alpha)$. This problem has only been studied when $\\alpha = \\beta$ (prior-free) or $\\alpha = 1/2$ (Bayesian), and the sample complexity is known to be characterized by the Hellinger divergence between $p$ and $q$, up to multiplicative constants. In this paper, we derive a formula that characterizes the sample complexity (up to multiplicative constants that are independent of $p$, $q$, and all error parameters) for: (i) all $0 \\le \\alpha, \\beta \\le 1/8$ in the prior-free setting; and (ii) all $\\delta \\le \\alpha/4$ in the Bayesian setting. In particular, the formula admits equivalent expressions in terms of certain divergences from the Jensen\u2013Shannon and Hellinger families. The main technical result concerns an $f$-divergence inequality between members of the Jensen\u2013Shannon and Hellinger families, which is proved by a combination of information-theoretic tools and case-by-case analyses. We explore applications of our results to robust and distributed (locally-private and communication-constrained) hypothesis testing.",
    "url": "https://proceedings.mlr.press/v247/pensia24a.html",
    "id": "https://proceedings.mlr.press/v247/pensia24a.html",
    "pdf": "https://proceedings.mlr.press/v247/pensia24a/pensia24a.pdf",
    "authors": {
      "0_Ankit Pensia": "Ankit Pensia",
      "1_Varun Jog": "Varun Jog",
      "2_Po-Ling Loh": "Po-Ling Loh"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/pensia24a/pensia24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4205-4206,\u00a02024.",
    "supplemental": ""
  },
  "36_The sample complexity of multidistribution learning": {
    "title": "The sample complexity of multi-distribution learning",
    "abstract": " Multi-distribution learning generalizes the classic PAC learning to handle data coming from multiple distributions. Given a set of $k$ data distributions and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis that minimizes the maximum population loss over $k$ distributions, up to $\\epsilon$ additive error. In this paper, we settle the sample complexity of multi-distribution learning by giving an algorithm of sample complexity $\\widetilde{O}((d+k)\\epsilon^{-2}) \\cdot (k/\\epsilon)^{o(1)}$. This matches the lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem of Awasthi, Haghtalab and Zhao.",
    "url": "https://proceedings.mlr.press/v247/peng24b.html",
    "id": "https://proceedings.mlr.press/v247/peng24b.html",
    "pdf": "https://proceedings.mlr.press/v247/peng24b/peng24b.pdf",
    "authors": {
      "0_Binghui Peng": "Binghui Peng"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/peng24b/peng24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4185-4204,\u00a02024.",
    "supplemental": ""
  },
  "37_The complexity of approximate coarse correlated equilibrium for incomplete information games": {
    "title": "The complexity of approximate (coarse) correlated equilibrium for incomplete information games",
    "abstract": "     We study the iteration complexity of decentralized learning of approximate correlated equilibria in incomplete information games.  On the negative side, we prove that in extensive-form games, assuming $\\mathsf{PPAD} \\not\\subset \\mathsf{TIME}(n^{\\polylog(n)})$, any polynomial-time learning algorithms must take at least $2^{\\log_2^{1-o(1)}(|\\mathcal{I}|)}$ iterations to converge to the set of $\\epsilon$-approximate correlated equilibrium, where $|\\mathcal{I}|$ is the number of nodes in the game and $\\epsilon > 0$ is an absolute constant. This nearly matches, up to the $o(1)$ term, the algorithms of (Peng and Rubinstein STOC\u20192024, Dagan et al. STOC\u20192024) for learning $\\epsilon$-approximate correlated equilibrium, and resolves an open question of Anagnostides, Kalavasis, Sandholm, and Zampetakis (Anagnostides et al. ITCS 2024). Our lower bound holds even for the easier solution concept of $\\epsilon$-approximate coarse correlated equilibrium. On the positive side, we give uncoupled dynamics that reach $\\epsilon$-approximate correlated equilibria of a Bayesian game in polylogarithmic iterations, without any dependence of the number of types. This demonstrates a separation between Bayesian games and extensive-form games.",
    "url": "https://proceedings.mlr.press/v247/peng24a.html",
    "id": "https://proceedings.mlr.press/v247/peng24a.html",
    "pdf": "https://proceedings.mlr.press/v247/peng24a/peng24a.pdf",
    "authors": {
      "0_Binghui Peng": "Binghui Peng",
      "1_Aviad Rubinstein": "Aviad Rubinstein"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/peng24a/peng24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4158-4184,\u00a02024.",
    "supplemental": ""
  },
  "38_The Limits and Potentials of Local SGD for Distributed Heterogeneous Learning with Intermittent Communication": {
    "title": "The Limits and Potentials of Local SGD for Distributed Heterogeneous Learning with Intermittent Communication",
    "abstract": "Local SGD is a popular optimization method in distributed learning, often outperforming mini-batch SGD. Despite this practical success, proving the efficiency of local SGD has been difficult, creating a significant gap between theory and practice. We provide new lower bounds for local SGD under existing first-order data heterogeneity assumptions, showing these assumptions can not capture local SGD\u2019s effectiveness. We also demonstrate the min-max optimality of accelerated mini-batch SGD under these assumptions. Our findings emphasize the need for improved modeling of data heterogeneity.  Under higher-order assumptions, we provide new upper bounds that verify the dominance of local SGD over mini-batch SGD when data heterogeneity is low.",
    "url": "https://proceedings.mlr.press/v247/patel24a.html",
    "id": "https://proceedings.mlr.press/v247/patel24a.html",
    "pdf": "https://proceedings.mlr.press/v247/patel24a/patel24a.pdf",
    "authors": {
      "0_Kumar Kshitij Patel": "Kumar Kshitij Patel",
      "1_Margalit Glasgow": "Margalit Glasgow",
      "2_Ali Zindari": "Ali Zindari",
      "3_Lingxiao Wang": "Lingxiao Wang",
      "4_Sebastian U Stich": "Sebastian U Stich",
      "5_Ziheng Cheng": "Ziheng Cheng",
      "6_Nirmit Joshi": "Nirmit Joshi",
      "7_Nathan Srebro": "Nathan Srebro"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/patel24a/patel24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4115-4157,\u00a02024.",
    "supplemental": ""
  },
  "39_Depth Separation in NormBounded InfiniteWidth Neural Networks": {
    "title": "Depth Separation in Norm-Bounded Infinite-Width Neural Networks",
    "abstract": "We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\\ell_2$-norm of the weights (sum of squares of all weights in the network). Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded.  Here, we study separation in terms of the sample complexity required for learnability. Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm). We also show that a similar statement in the reverse direction is not possible:  any function learnable with polynomial sample complexity by a norm-controlled depth-2 ReLU network with infinite width is also learnable with polynomial sample complexity by a norm-controlled depth-3 ReLU network.",
    "url": "https://proceedings.mlr.press/v247/parkinson24a.html",
    "id": "https://proceedings.mlr.press/v247/parkinson24a.html",
    "pdf": "https://proceedings.mlr.press/v247/parkinson24a/parkinson24a.pdf",
    "authors": {
      "0_Suzanna Parkinson": "Suzanna Parkinson",
      "1_Greg Ongie": "Greg Ongie",
      "2_Rebecca Willett": "Rebecca Willett",
      "3_Ohad Shamir": "Ohad Shamir",
      "4_Nathan Srebro": "Nathan Srebro"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/parkinson24a/parkinson24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4082-4114,\u00a02024.",
    "supplemental": ""
  },
  "40_Learning sum of diverse features computational hardness and efficient gradientbased training for ridge combinations": {
    "title": "Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations",
    "abstract": "We study the statistical and computational complexity of learning a target function $f_*:\\R^d\\to\\R$ with \\textit{additive structure}, that is, $f_*(x) = \\frac{1}{\\sqrt{M}}\\sum_{m=1}^M f_m(\u27e8x, v_m\u27e9)$, where $f_1,f_2,...,f_M:\\R\\to\\R$ are nonlinear link functions of single-index models (ridge functions) with diverse and near-orthogonal index features $\\{v_m\\}_{m=1}^M$, and the number of additive tasks $M$ grows with the dimensionality $M\\asymp d^\\gamma$ for $\\gamma\\ge 0$. This problem setting is motivated by the classical additive model literature, the recent representation learning theory of two-layer neural network, and large-scale pretraining where the model simultaneously acquires a large number of \u201cskills\u201d that are often \\textit{localized} in distinct parts of the trained network. We prove that a large subset of polynomial $f_*$ can be efficiently learned by gradient descent training of a two-layer neural network, with a polynomial statistical and computational complexity that depends on the number of tasks $M$ and the \\textit{information exponent} of $f_m$, despite the unknown link function and $M$ growing with the dimensionality. We complement this learnability guarantee with computational hardness result by establishing statistical query (SQ) lower bounds for both the correlational SQ and full SQ algorithms.",
    "url": "https://proceedings.mlr.press/v247/oko24a.html",
    "id": "https://proceedings.mlr.press/v247/oko24a.html",
    "pdf": "https://proceedings.mlr.press/v247/oko24a/oko24a.pdf",
    "authors": {
      "0_Kazusato Oko": "Kazusato Oko",
      "1_Yujin Song": "Yujin Song",
      "2_Taiji Suzuki": "Taiji Suzuki",
      "3_Denny Wu": "Denny Wu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/oko24a/oko24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4009-4081,\u00a02024.",
    "supplemental": ""
  },
  "41_Robust Distribution Learning with Local and Global Adversarial Corruptions extended abstract": {
    "title": "Robust Distribution Learning with Local and Global Adversarial Corruptions (extended abstract)",
    "abstract": "We consider learning in an adversarial environment, where an $\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily modified (\\emph{global} corruptions) and the remaining perturbations have average magnitude bounded by $\\rho$ (\\emph{local} corruptions). Given access to $n$ such corrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$ that  minimizes the Wasserstein distance $W_1(\\hat{P}_n,P)$. In fact, we attack the fine-grained task of minimizing $W_1(\\Pi_\\sharp  \\hat{P}_n, \\Pi_\\sharp P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$, with performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to account simultaneously for mean estimation ($k=1$), distribution estimation ($k=d$), as well as the settings interpolating between these two extremes. We characterize the optimal population-limit risk for this task and then develop an efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon k} + \\rho + \\tilde{O}(k\\sqrt{d}n^{-1/k})$ when $P$ has bounded covariance. Our efficient procedure relies on a novel trace norm approximation of an ideal yet intractable 2-Wasserstein projection estimator. We apply this algorithm to robust stochastic optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in Wasserstein distributionally robust optimization.",
    "url": "https://proceedings.mlr.press/v247/nietert24a.html",
    "id": "https://proceedings.mlr.press/v247/nietert24a.html",
    "pdf": "https://proceedings.mlr.press/v247/nietert24a/nietert24a.pdf",
    "authors": {
      "0_Sloan Nietert": "Sloan Nietert",
      "1_Ziv Goldfeld": "Ziv Goldfeld",
      "2_Soroosh Shafiee": "Soroosh Shafiee"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/nietert24a/nietert24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4007-4008,\u00a02024.",
    "supplemental": ""
  },
  "42_Optimistic Information Directed Sampling": {
    "title": "Optimistic Information Directed Sampling",
    "abstract": "We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster et al. (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a less conservative version of the Decision Estimation Coefficient of Foster et al. (2021).",
    "url": "https://proceedings.mlr.press/v247/neu24a.html",
    "id": "https://proceedings.mlr.press/v247/neu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/neu24a/neu24a.pdf",
    "authors": {
      "0_Gergely Neu": "Gergely Neu",
      "1_Matteo Papini": "Matteo Papini",
      "2_Ludovic Schwartz": "Ludovic Schwartz"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/neu24a/neu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3970-4006,\u00a02024.",
    "supplemental": ""
  },
  "43_Exact Mean Square Linear Stability Analysis for SGD": {
    "title": "Exact Mean Square Linear Stability Analysis for SGD",
    "abstract": "The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size that is both necessary and sufficient for the linear stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size B. In particular, we show that the stability threshold is monotonically non-decreasing in the batch size, which means that reducing the batch size can only decrease stability. Furthermore, we show that SGD\u2019s stability threshold is equivalent to that of a mixture process which takes in each iteration a full batch gradient step w.p. 1-p, and a single sample gradient step w.p. $p$, where $p \\approx 1/B$. This indicates that even with moderate batch sizes, SGD\u2019s stability threshold is very close to that of GD\u2019s. We also prove simple necessary conditions for linear stability, which depend on the batch size, and are easier to compute than the precise threshold. Finally, we derive the asymptotic covariance of the dynamics around the minimum, and discuss its dependence on the learning rate. We validate our theoretical findings through experiments on the MNIST dataset.",
    "url": "https://proceedings.mlr.press/v247/mulayoff24a.html",
    "id": "https://proceedings.mlr.press/v247/mulayoff24a.html",
    "pdf": "https://proceedings.mlr.press/v247/mulayoff24a/mulayoff24a.pdf",
    "authors": {
      "0_Rotem Mulayoff": "Rotem Mulayoff",
      "1_Tomer Michaeli": "Tomer Michaeli"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/mulayoff24a/mulayoff24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3915-3969,\u00a02024.",
    "supplemental": ""
  },
  "44_Finding Superspreaders in Network Cascades": {
    "title": "Finding Super-spreaders in Network Cascades",
    "abstract": "Suppose that a cascade (e.g., an epidemic) spreads on an unknown graph, and only the infection times of vertices are observed. What can be learned about the graph from the infection times caused by multiple distinct cascades? Most of the literature on this topic focuses on the task of recovering the \\emph{entire} graph, which requires $\\Omega ( \\log n)$ cascades for an $n$-vertex bounded degree graph. Here we ask a different question: can the important parts of the graph be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large? In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a graph. Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades. Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve. Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $\\log(n) / \\log \\log (n)$ cascades. Surprisingly, this matches (up to $\\log \\log n$ factors) the number of cascades needed to learn the \\emph{entire} graph if it is a tree.",
    "url": "https://proceedings.mlr.press/v247/mossel24a.html",
    "id": "https://proceedings.mlr.press/v247/mossel24a.html",
    "pdf": "https://proceedings.mlr.press/v247/mossel24a/mossel24a.pdf",
    "authors": {
      "0_Elchanan Mossel": "Elchanan Mossel",
      "1_Anirudh Sridhar": "Anirudh Sridhar"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/mossel24a/mossel24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3874-3914,\u00a02024.",
    "supplemental": ""
  },
  "45_Fundamental Limits of NonLinear LowRank Matrix Estimation": {
    "title": "Fundamental Limits of Non-Linear Low-Rank Matrix Estimation",
    "abstract": "We consider the task of estimating a low-rank matrix from non-linear and noisy observations. We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function.  In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as \\(N^{\\frac 12 (1-1/k_F)}\\), where \\(k_F\\){is} the first non-zero Fisher information coefficient of the function.  We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate message-passing algorithm that reaches the MMSE under conditions analogous to the linear version of the problem.  We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE. ",
    "url": "https://proceedings.mlr.press/v247/mergny24a.html",
    "id": "https://proceedings.mlr.press/v247/mergny24a.html",
    "pdf": "https://proceedings.mlr.press/v247/mergny24a/mergny24a.pdf",
    "authors": {
      "0_Pierre Mergny": "Pierre Mergny",
      "1_Justin Ko": "Justin Ko",
      "2_Florent Krzakala": "Florent Krzakala",
      "3_Lenka Zdeborov\u00e1": "Lenka Zdeborov\u00e1"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/mergny24a/mergny24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3873-3873,\u00a02024.",
    "supplemental": ""
  },
  "46_Fast blind and accurate Tuningfree sparse regression with global linear convergence": {
    "title": "Fast, blind, and accurate: Tuning-free sparse regression with global linear convergence",
    "abstract": "Many algorithms for high-dimensional regression problems require the calibration of regularization hyperparameters. This, in turn, often requires the knowledge of the unknown noise variance in order to produce meaningful solutions. Recent works show, however, that there exist certain estimators that are pivotal, i.e., the regularization parameter does not depend on the noise level; the most remarkable example being the square-root lasso. Such estimators have also been shown to exhibit strong connections to distributionally robust optimization. Despite the progress in the design of pivotal estimators, the resulting minimization problem is challenging as both the loss function and the regularization term are non-smooth. To date, the design of fast, robust, and scalable algorithms with strong convergence rate guarantees is still an open problem. This work addresses this problem by showing that an iteratively reweighted least squares (IRLS) algorithm exhibits global linear convergence under the weakest assumption available in the literature. We expect our findings will also have implications for multi-task learning and distributionally robust optimization.",
    "url": "https://proceedings.mlr.press/v247/mayrink-verdun24a.html",
    "id": "https://proceedings.mlr.press/v247/mayrink-verdun24a.html",
    "pdf": "https://proceedings.mlr.press/v247/mayrink-verdun24a/mayrink-verdun24a.pdf",
    "authors": {
      "0_Claudio Mayrink Verdun": "Claudio Mayrink Verdun",
      "1_Oleh Melnyk": "Oleh Melnyk",
      "2_Felix Krahmer": "Felix Krahmer",
      "3_Peter Jung": "Peter Jung"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/mayrink-verdun24a/mayrink-verdun24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3823-3872,\u00a02024.",
    "supplemental": ""
  },
  "47_Lowdegree phase transitions for detecting a planted clique in sublinear time": {
    "title": "Low-degree phase transitions for detecting a planted clique in sublinear time",
    "abstract": "We consider the problem of detecting a planted clique of size $k$ in a random graph on $n$ vertices.  When the size of the clique exceeds $\\Theta(\\sqrt{n})$, polynomial-time algorithms for detection proliferate.  We study faster\u2014namely, sublinear time\u2014algorithms in the high-signal regime when $k = \\Theta(n^{1/2 + \\delta})$, for some $\\delta > 0$.  To this end, we consider algorithms that non-adaptively query a subset $M$ of entries of the adjacency matrix and then compute a low-degree polynomial function of the revealed entries.  We prove a computational phase transition for this class of \\emph{non-adaptive low-degree algorithms}: under the scaling $\\lvert M \\rvert = \\Theta(n^{\\gamma})$, the clique can be detected when $\\gamma > 3(1/2 - \\delta)$ but not when $\\gamma < 3(1/2 - \\delta)$. As a result, the best known runtime for detecting a planted clique, $\\widetilde{O}(n^{3(1/2-\\delta)})$, cannot be improved without looking beyond the non-adaptive low-degree class. Our proof of the lower bound\u2014based on bounding the conditional low-degree likelihood ratio\u2014reveals further structure in non-adaptive detection of a planted clique.  Using (a bound on) the conditional low-degree likelihood ratio as a potential function, we show that for \\emph{every} non-adaptive query pattern, there is a highly structured query pattern of the same size that is at least as effective.",
    "url": "https://proceedings.mlr.press/v247/mardia24a.html",
    "id": "https://proceedings.mlr.press/v247/mardia24a.html",
    "pdf": "https://proceedings.mlr.press/v247/mardia24a/mardia24a.pdf",
    "authors": {
      "0_Jay Mardia": "Jay Mardia",
      "1_Kabir Aladin Verchand": "Kabir Aladin Verchand",
      "2_Alexander S. Wein": "Alexander S. Wein"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/mardia24a/mardia24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3798-3822,\u00a02024.",
    "supplemental": ""
  },
  "48_Harmonics of Learning Universal Fourier Features Emerge in Invariant Networks": {
    "title": "Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks",
    "abstract": "In this work, we formally prove that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group. This provides a mathematical explanation for the emergence of Fourier features \u2013 a ubiquitous phenomenon in both biological and artificial learning systems. The results hold even for non-commutative groups, in which case the Fourier transform encodes all the irreducible unitary group representations. Our findings have consequences for the problem of symmetry discovery. Specifically, we demonstrate that the algebraic structure of an unknown group can be recovered from the weights of a network that is at least approximately invariant within certain bounds. Overall, this work contributes to a foundation for an algebraic learning theory of invariant neural network representations.",
    "url": "https://proceedings.mlr.press/v247/marchetti24a.html",
    "id": "https://proceedings.mlr.press/v247/marchetti24a.html",
    "pdf": "https://proceedings.mlr.press/v247/marchetti24a/marchetti24a.pdf",
    "authors": {
      "0_Giovanni Luca Marchetti": "Giovanni Luca Marchetti",
      "1_Christopher J Hillar": "Christopher J Hillar",
      "2_Danica Kragic": "Danica Kragic",
      "3_Sophia Sanborn": "Sophia Sanborn"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/marchetti24a/marchetti24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3775-3797,\u00a02024.",
    "supplemental": ""
  },
  "49_Projection by Convolution Optimal Sample Complexity for Reinforcement Learning in ContinuousSpace MDPs": {
    "title": "Projection by Convolution: Optimal Sample Complexity for Reinforcement Learning in Continuous-Space MDPs",
    "abstract": "We consider the problem of learning an $\\varepsilon$-optimal policy in a general class of continuous-space Markov decision processes (MDPs) having smooth Bellman operators. Given access to a generative model, we achieve rate-optimal sample complexity by performing a simple, \\emph{perturbed} version of least-squares value iteration with orthogonal trigonometric polynomials as features. Key to our solution is a novel projection technique based on ideas from harmonic analysis.  Our\u00a0$\\widetilde{O}(\\epsilon^{-2-d/(\\nu+1)})$ sample complexity, where $d$ is the dimension of the state-action space and $\\nu$ the order of smoothness, recovers the state-of-the-art result of discretization approaches for the special case of Lipschitz MDPs $(\\nu=0)$. At the same time, for $\\nu\\to\\infty$, it recovers and greatly generalizes the $O(\\epsilon^{-2})$ rate of low-rank MDPs, which are more amenable to regression approaches. In this sense, our result bridges the gap between two popular but conflicting perspectives on continuous-space MDPs.  ",
    "url": "https://proceedings.mlr.press/v247/maran24a.html",
    "id": "https://proceedings.mlr.press/v247/maran24a.html",
    "pdf": "https://proceedings.mlr.press/v247/maran24a/maran24a.pdf",
    "authors": {
      "0_Davide Maran": "Davide Maran",
      "1_Alberto Maria Metelli": "Alberto Maria Metelli",
      "2_Matteo Papini": "Matteo Papini",
      "3_Marcello Restelli": "Marcello Restelli"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/maran24a/maran24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3743-3774,\u00a02024.",
    "supplemental": ""
  },
  "50_Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion": {
    "title": "Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion",
    "abstract": "We study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix $X^\\star \\in \\mathbb{R}^{d\\times d}$ of rank-$r$, parameterized by $UU^{\\top}$, from only a subset of its observed entries. We show that the vanilla gradient descent (GD) with small initialization provably converges to the ground truth $X^\\star$ without requiring any explicit regularization. This convergence result holds true even in the over-parameterized scenario, where the true rank $r$ is unknown and conservatively over-estimated by a search rank $r\u2019\\gg r$. The existing results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank $r$.  In the over-parameterized regime where $r\u2019\\geq r$, we show that, with $\\widetilde\\Omega(dr^9)$ observations, GD with an initial point $\\|U_0\\| \\leq O(\\epsilon)$ converges near-linearly to an $\\epsilon$-neighborhood of $X^\\star$. Consequently, smaller initial points result in increasingly accurate solutions. Surprisingly, neither the convergence rate nor the final accuracy depends on the over-parameterized search rank $r\u2019$, and they are only governed by the true rank $r$. In the exactly-parameterized regime where $r\u2019=r$, we further enhance this result by proving that GD converges at a faster rate to achieve an arbitrarily small accuracy $\\epsilon>0$, provided the initial point satisfies $\\|U_0\\| = O(1/d)$. At the crux of our method lies a novel weakly-coupled leave-one-out analysis, which allows us to establish the global convergence of GD, extending beyond what was previously possible using the classical leave-one-out analysis.",
    "url": "https://proceedings.mlr.press/v247/ma24a.html",
    "id": "https://proceedings.mlr.press/v247/ma24a.html",
    "pdf": "https://proceedings.mlr.press/v247/ma24a/ma24a.pdf",
    "authors": {
      "0_Jianhao Ma": "Jianhao Ma",
      "1_Salar Fattahi": "Salar Fattahi"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/ma24a/ma24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3683-3742,\u00a02024.",
    "supplemental": ""
  },
  "51_Linear bandits with polylogarithmic minimax regret": {
    "title": "Linear bandits with polylogarithmic minimax regret",
    "abstract": " We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector.  We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation  $\\lambda_{\\min} ( V_t ) = \\Omega (\\sqrt{\\lambda_{\\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.",
    "url": "https://proceedings.mlr.press/v247/lumbreras24a.html",
    "id": "https://proceedings.mlr.press/v247/lumbreras24a.html",
    "pdf": "https://proceedings.mlr.press/v247/lumbreras24a/lumbreras24a.pdf",
    "authors": {
      "0_Josep Lumbreras": "Josep Lumbreras",
      "1_Marco Tomamichel": "Marco Tomamichel"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/lumbreras24a/lumbreras24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3644-3682,\u00a02024.",
    "supplemental": ""
  },
  "52_Autobidders with Budget and ROI Constraints Efficiency Regret and Pacing Dynamics": {
    "title": "Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics",
    "abstract": "We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser\u2019s total value over multiple rounds of a repeated auction, subject to budget and return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any \u201cintermediate\u201d auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. Our analysis holds whether or not the bidding dynamics converges to an equilibrium, side-stepping the dearth of provable convergence guarantees in the literature and the hardness result (Chen et al., 2021) which precludes such guarantees for budget-constrained second-price auctions. Our vanishing-regret result extends to an adversarial environment, without any assumptions on the other agents. We adopt a non-standard benchmark: the sequence of bids such that each bid $b_t$ maximizes value for the environment in round $t$. Hence, we side-step the impossibility results for the standard benchmark of best fixed bid (Balseiro and Gur, 2019). When there is only a budget constraint, our algorithm specializes to the autobidding algorithm from (Balseiro and Gur, 2019), and our guarantees specialize to the regret and liquid welfare guarantees from Gaitonde et al. (2023). While our approach to bounding liquid welfare shares a common high-level strategy with Gaitonde et al. (2023), handling the ROI constraint, and particularly both constraints jointly, introduces a variety of new technical challenges. These challenges necessitate a new algorithm, changes to the way liquid welfare bounds are established, and a different methodology for establishing regret properties.",
    "url": "https://proceedings.mlr.press/v247/lucier24a.html",
    "id": "https://proceedings.mlr.press/v247/lucier24a.html",
    "pdf": "https://proceedings.mlr.press/v247/lucier24a/lucier24a.pdf",
    "authors": {
      "0_Brendan Lucier": "Brendan Lucier",
      "1_Sarath Pattathil": "Sarath Pattathil",
      "2_Aleksandrs Slivkins": "Aleksandrs Slivkins",
      "3_Mengxiao Zhang": "Mengxiao Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/lucier24a/lucier24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3642-3643,\u00a02024.",
    "supplemental": ""
  },
  "53_The PredictedUpdates Dynamic Model Offline Incremental and Decremental to Fully Dynamic Transformations": {
    "title": "The Predicted-Updates Dynamic Model: Offline, Incremental, and Decremental to Fully Dynamic Transformations",
    "abstract": "The main bottleneck in designing efficient dynamic algorithms is the unknown nature of the update sequence.  In particular, there are problems where the separation in runtime between the best offline or partially dynamic solutions and the best fully dynamic solutions is polynomial, sometimes even exponential. In this paper, we formulate the \\emph{predicted-updates dynamic model}, one of the first \\emph{beyond-worst-case} models for dynamic algorithms, which generalizes a large set of well-studied dynamic models including the offline dynamic, incremental, and decremental models to the fully dynamic setting when given predictions about the update times of the elements. Our paper models real world settings, in which we often have access to side information that allows us to make coarse predictions about future updates.  We formulate a framework that bridges the gap between fully and offline/partially dynamic, leading to greatly improved runtime bounds over the state-of-the-art dynamic algorithms for a variety of important problems such as triconnectivity, planar digraph all pairs shortest paths, \\(k\\)-edge connectivity, and others, for prediction error of reasonable magnitude. Our simple framework avoids heavy machinery, potentially leading to a new set of dynamic algorithms that are implementable in practice.",
    "url": "https://proceedings.mlr.press/v247/liu24c.html",
    "id": "https://proceedings.mlr.press/v247/liu24c.html",
    "pdf": "https://proceedings.mlr.press/v247/liu24c/liu24c.pdf",
    "authors": {
      "0_Quanquan C. Liu": "Quanquan C. Liu",
      "1_Vaidehi Srinivas": "Vaidehi Srinivas"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/liu24c/liu24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3582-3641,\u00a02024.",
    "supplemental": ""
  },
  "54_Spatial properties of Bayesian unsupervised trees": {
    "title": "Spatial properties of Bayesian unsupervised trees",
    "abstract": "Tree-based methods are popular nonparametric tools for capturing spatial heterogeneity and making predictions in multivariate problems. In unsupervised learning, trees and their ensembles have also been applied to a wide range of statistical inference tasks, such as multi-resolution sketching of distributional variations, localization of high-density regions, and design of efficient data compression schemes. In this paper, we study the spatial adaptation property of Bayesian tree-based methods in the unsupervised setting, with a focus on the density estimation problem. We characterize spatial heterogeneity of the underlying density function by using anisotropic Besov spaces, region-wise anisotropic Besov spaces, and two novel function classes as their extensions. For two types of commonly used prior distributions on trees under the context of unsupervised learning\u2014the optional P{\u00f3}lya tree (Wong and Ma, 2010) and the Dirichlet prior (Lu et al., 2013)\u2014we calculate posterior concentration rates when the density function exhibits different types of heterogeneity. In specific, we show that the posterior concentration rate for trees is near minimax over the anisotropic Besov space. The rate is adaptive in the sense that to achieve such a rate we do not need any prior knowledge of the parameters of the Besov space.",
    "url": "https://proceedings.mlr.press/v247/liu24b.html",
    "id": "https://proceedings.mlr.press/v247/liu24b.html",
    "pdf": "https://proceedings.mlr.press/v247/liu24b/liu24b.pdf",
    "authors": {
      "0_Linxi Liu": "Linxi Liu",
      "1_Li Ma": "Li Ma"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/liu24b/liu24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3556-3581,\u00a02024.",
    "supplemental": ""
  },
  "55_The role of randomness in quantum state certification with unentangled measurements": {
    "title": "The role of randomness in quantum state certification with unentangled measurements",
    "abstract": "Given $n$ copies of an unknown quantum state $\\rho\\in\\mathbb{C}^{d\\times d}$, quantum state certification is the task of determining whether $\\rho=\\rho_0$ or $\\|\\rho-\\rho_0\\|_1>\\varepsilon$, where $\\rho_0$ is a known reference state. We study quantum state certification using unentangled quantum measurements, namely measurements which operate only on one copy of $\\rho$ at a time. When there is a common source of randomness available and the unentangled measurements are chosen based on this randomness, prior work has shown that $\\Theta(d^{3/2}/\\varepsilon^2)$ copies are necessary and sufficient. This holds even when the measurements are allowed to be chosen adaptively. We consider deterministic measurement schemes (as opposed to randomized) and demonstrate that ${\\Theta}(d^2/\\varepsilon^2)$ copies are necessary and sufficient for state certification. This shows a separation between algorithms with and without randomness.  We develop a lower bound framework for both fixed and randomized measurements that relates the hardness of testing to the well-established L\u00fcders rule. More precisely, we obtain lower bounds for randomized and fixed schemes as a function of the eigenvalues of the L\u00fcders channel which characterizes one possible post-measurement state transformation.",
    "url": "https://proceedings.mlr.press/v247/liu24a.html",
    "id": "https://proceedings.mlr.press/v247/liu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/liu24a/liu24a.pdf",
    "authors": {
      "0_Yuhan Liu": "Yuhan Liu",
      "1_Jayadev Acharya": "Jayadev Acharya"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/liu24a/liu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3523-3555,\u00a02024.",
    "supplemental": ""
  },
  "56_Online Policy Optimization in Unknown Nonlinear Systems": {
    "title": "Online Policy Optimization in Unknown Nonlinear Systems",
    "abstract": "We study online policy optimization in nonlinear time-varying systems where the true dynamical models are unknown to the controller. This problem is challenging because, unlike in linear systems, the controller cannot obtain globally accurate estimations of the ground-truth dynamics using local exploration. We propose a meta-framework that combines a general online policy optimization algorithm (\\texttt{ALG}) with a general online estimator of the dynamical system\u2019s model parameters (\\texttt{EST}). We show that if the hypothetical joint dynamics induced by \\texttt{ALG} with \\emph{known} parameters satisfies several desired properties, the joint dynamics under \\emph{inexact} parameters from \\texttt{EST} will be robust to errors. Importantly, the final regret only depends on \\texttt{EST}\u2019s predictions on the visited trajectory, which relaxes a bottleneck on identifying the true parameters globally. To demonstrate our framework, we develop a computationally efficient variant of Gradient-based Adaptive Policy Selection, called Memoryless GAPS (M-GAPS), and use it to instantiate \\texttt{ALG}. Combining \\mbox{M-GAPS} with online gradient descent to instantiate \\texttt{EST} yields (to our knowledge) the first local regret bound for online policy optimization in nonlinear time-varying systems with unknown dynamics.",
    "url": "https://proceedings.mlr.press/v247/lin24a.html",
    "id": "https://proceedings.mlr.press/v247/lin24a.html",
    "pdf": "https://proceedings.mlr.press/v247/lin24a/lin24a.pdf",
    "authors": {
      "0_Yiheng Lin": "Yiheng Lin",
      "1_James A. Preiss": "James A. Preiss",
      "2_Fengze Xie": "Fengze Xie",
      "3_Emile Anand": "Emile Anand",
      "4_Soon-Jo Chung": "Soon-Jo Chung",
      "5_Yisong Yue": "Yisong Yue",
      "6_Adam Wierman": "Adam Wierman"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/lin24a/lin24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3475-3522,\u00a02024.",
    "supplemental": ""
  },
  "57_Optimistic Rates for Learning from Label Proportions": {
    "title": "Optimistic Rates for Learning from Label Proportions",
    "abstract": "We consider a weakly supervised learning problem called Learning from Label Proportions (LLP), where examples are grouped into \"bags\" and only the average label within each bag is revealed to the learner. We study various learning rules for LLP that achieve PAC learning guarantees for classification loss. We establish that the classical Empirical Proportional Risk Minimization (EPRM) learning rule (Yu et al., 2014) achieves fast rates under realizability, but EPRM and similar proportion matching learning rules can fail in the agnostic setting. We also show that (1) a debiased proportional square loss, as well as (2) a recently proposed EasyLLP learning rule (Busa-Fekete et al., 2023) both achieve \"optimistic rates\" (Panchenko, 2002); in both the realizable and agnostic settings, their sample complexity is optimal (up to log factors) in terms of $\\epsilon, \\delta$, and VC dimension.",
    "url": "https://proceedings.mlr.press/v247/li24b.html",
    "id": "https://proceedings.mlr.press/v247/li24b.html",
    "pdf": "https://proceedings.mlr.press/v247/li24b/li24b.pdf",
    "authors": {
      "0_Gene Li": "Gene Li",
      "1_Lin Chen": "Lin Chen",
      "2_Adel Javanmard": "Adel Javanmard",
      "3_Vahab Mirrokni": "Vahab Mirrokni"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/li24b/li24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3437-3474,\u00a02024.",
    "supplemental": ""
  },
  "58_Minimaxoptimal rewardagnostic exploration in reinforcement learning": {
    "title": "Minimax-optimal reward-agnostic exploration in reinforcement learning",
    "abstract": "This paper studies reward-agnostic exploration in reinforcement learning (RL) \u2014 a scenario where the learner is unware of the reward functions during the exploration stage \u2014 and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon inhomogeneous Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of $\\frac{SAH^3}{\\varepsilon^2}$ sample episodes (up to log factor) without guidance of the reward information, our algorithm is able to find $\\varepsilon$-optimal policies for all these reward functions, provided that $\\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\\frac{S^2AH^3}{\\varepsilon^2}$ episodes (up to log factor), our algorithm is able to yield $\\varepsilon$ accuracy for arbitrarily many reward functions (even when they are adversarially designed),  a task commonly dubbed as \u201creward-free exploration.\u201d The novelty of our algorithm design draws on insights from offline RL: the exploration scheme attempts to maximize a critical reward-agnostic quantity that dictates the performance of offline RL, while the policy learning paradigm leverages ideas from sample-optimal offline RL paradigms.  ",
    "url": "https://proceedings.mlr.press/v247/li24a.html",
    "id": "https://proceedings.mlr.press/v247/li24a.html",
    "pdf": "https://proceedings.mlr.press/v247/li24a/li24a.pdf",
    "authors": {
      "0_Gen Li": "Gen Li",
      "1_Yuling Yan": "Yuling Yan",
      "2_Yuxin Chen": "Yuxin Chen",
      "3_Jianqing Fan": "Jianqing Fan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/li24a/li24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3431-3436,\u00a02024.",
    "supplemental": ""
  },
  "59_FollowthePerturbedLeader with Fr\u00e9chettype Tail Distributions Optimality in Adversarial Bandits and BestofBothWorlds": {
    "title": "Follow-the-Perturbed-Leader with Fr\u00e9chet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds",
    "abstract": "This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL) policy in both adversarial and stochastic $K$-armed bandits. Despite the widespread use of the Follow-the-Regularized-Leader (FTRL) framework with various choices of regularization, the FTPL framework, which relies on random perturbations, has not received much attention, despite its inherent simplicity. In adversarial bandits, there has been conjecture that FTPL could potentially achieve $\\mathcal{O}(\\sqrt{KT})$ regrets if perturbations follow a distribution with a Fr\u00e9chet-type tail. Recent work by Honda et al. (2023) showed that FTPL with Fr\u00e9chet distribution with shape $\\alpha=2$ indeed attains this bound and, notably logarithmic regret in stochastic bandits, meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result only partly resolves the above conjecture because their analysis heavily relies on the specific form of the Fr\u00e9chet distribution with this shape. In this paper, we establish a sufficient condition for perturbations to achieve $\\mathcal{O}(\\sqrt{KT})$ regrets in the adversarial setting, which covers, e.g., Fr\u00e9chet, Pareto, and Student-$t$ distributions. We also demonstrate the BOBW achievability of FTPL with certain Fr\u00e9chet-type tail distributions.  Our results contribute not only to resolving existing conjectures through the lens of extreme value theory but also potentially offer insights into the effect of the regularization functions in FTRL through the mapping from FTPL to FTRL.",
    "url": "https://proceedings.mlr.press/v247/lee24a.html",
    "id": "https://proceedings.mlr.press/v247/lee24a.html",
    "pdf": "https://proceedings.mlr.press/v247/lee24a/lee24a.pdf",
    "authors": {
      "0_Jongyeong Lee": "Jongyeong Lee",
      "1_Junya Honda": "Junya Honda",
      "2_Shinji Ito": "Shinji Ito",
      "3_Min-hwan Oh": "Min-hwan Oh"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/lee24a/lee24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3375-3430,\u00a02024.",
    "supplemental": ""
  },
  "60_Inherent limitations of dimensions for characterizing learnability of distribution classes": {
    "title": "Inherent limitations of dimensions for characterizing learnability of distribution classes",
    "abstract": " We consider the long-standing question of finding a parameter of a class of probability distributions that characterizes its PAC learnability. While for many learning tasks (such as binary classification and online learning) there is a notion of dimension whose finiteness is equivalent to learnability within any level of accuracy, we show, rather surprisingly, that such parameter does not exist for distribution learning. Concretely, our results apply for several general notions of characterizing learnability and for several learning tasks. We show that there is no notion of dimension that characterizes the sample complexity of learning distribution classes. We then consider the weaker requirement of only characterizing learnability (rather than the quantitative sample complexity function). We propose some natural requirements for such a characterization and go on to show that there exists no characterization of learnability that satisfies these requirements for classes of distributions. Furthermore, we show that our results hold for various other learning problems. In particular, we show that there is no notion of dimension characterizing PAC-learnability for any of the tasks: classification learning w.r.t. a restricted set of marginal distributions and learnability of classes of real-valued functions with continuous losses.",
    "url": "https://proceedings.mlr.press/v247/lechner24a.html",
    "id": "https://proceedings.mlr.press/v247/lechner24a.html",
    "pdf": "https://proceedings.mlr.press/v247/lechner24a/lechner24a.pdf",
    "authors": {
      "0_Tosca Lechner": "Tosca Lechner",
      "1_Shai Ben-David": "Shai Ben-David"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/lechner24a/lechner24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3353-3374,\u00a02024.",
    "supplemental": ""
  },
  "61_BetterthanKL PACBayes Bounds": {
    "title": "Better-than-KL PAC-Bayes Bounds",
    "abstract": " Let $f(\\theta, X_1),$ $ \u2026,$ $ f(\\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \u2026, X_n$ are independent random variables (data), and $\\theta$ is a random parameter distributed according to some data-dependent \\emph{posterior} distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network, where $f$ is a loss function. Classically, this problem is approached through a \\emph{PAC-Bayes} analysis where, in addition to the posterior, we choose a \\emph{prior} distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the \\emph{complexity} of the learning problem where the de facto standard choice is the Kullback-Leibler (KL) divergence. However, the tightness of this choice has rarely been  questioned.   In this paper, we challenge the tightness of the KL-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound. In particular, we demonstrate new \\emph{high-probability} PAC-Bayes bounds with a novel and \\emph{better-than-KL} divergence that is inspired by Zhang et al. (2022). Our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities. Our result is first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are not known to be strictly better than KL. Thus, we believe our work marks the first step towards identifying optimal rates of PAC-Bayes bounds.",
    "url": "https://proceedings.mlr.press/v247/kuzborskij24a.html",
    "id": "https://proceedings.mlr.press/v247/kuzborskij24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kuzborskij24a/kuzborskij24a.pdf",
    "authors": {
      "0_Ilja Kuzborskij": "Ilja Kuzborskij",
      "1_Kwang-Sung Jun": "Kwang-Sung Jun",
      "2_Yulian Wu": "Yulian Wu",
      "3_Kyoungseok Jang": "Kyoungseok Jang",
      "4_Francesco Orabona": "Francesco Orabona"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kuzborskij24a/kuzborskij24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3325-3352,\u00a02024.",
    "supplemental": ""
  },
  "62_Accelerated ParameterFree Stochastic Optimization": {
    "title": "Accelerated Parameter-Free Stochastic Optimization",
    "abstract": "We propose a method that achieves near-optimal rates for \\emph{smooth} stochastic convex optimization and requires essentially no prior knowledge of problem parameters. This improves on prior work which requires knowing at least the initial distance to optimality $d_0$.   Our method, \\textsc{U-DoG}, combines \\textsc{UniXGrad} (Kavis et al., 2019) and \\textsc{DoG} (Ivgi et al., 2023) with novel iterate stabilization techniques. It requires only loose bounds on $d_0$ and the noise magnitude, provides high probability guarantees under sub-Gaussian noise, and is also near-optimal in the non-smooth case. Our experiments show consistent, strong performance on convex problems and mixed results on neural network training.",
    "url": "https://proceedings.mlr.press/v247/kreisler24a.html",
    "id": "https://proceedings.mlr.press/v247/kreisler24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kreisler24a/kreisler24a.pdf",
    "authors": {
      "0_Itai Kreisler": "Itai Kreisler",
      "1_Maor Ivgi": "Maor Ivgi",
      "2_Oliver Hinder": "Oliver Hinder",
      "3_Yair Carmon": "Yair Carmon"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kreisler24a/kreisler24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3257-3324,\u00a02024.",
    "supplemental": ""
  },
  "63_Simple online learning with consistent oracle": {
    "title": "Simple online learning with consistent oracle",
    "abstract": "We consider online learning in the model where a learning algorithm can access the class only via the \\emph{consistent oracle}\u2014an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al.\u00a0(COLT\u201923). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a computationally intractable problem. Assos et al.\u00a0gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also show that there exists no algorithm in this model that makes less than $3^d$ mistakes. Our algorithm (as well as the algorithm of Assos et al.) solves an open problem by Hasrati and Ben-David\u00a0(ALT\u201923). Namely, it demonstrates that every class of finite Littlestone dimension with recursively enumerable representation admits a computable online learner (that may be undefined on unrealizable samples). ",
    "url": "https://proceedings.mlr.press/v247/kozachinskiy24a.html",
    "id": "https://proceedings.mlr.press/v247/kozachinskiy24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kozachinskiy24a/kozachinskiy24a.pdf",
    "authors": {
      "0_Alexander Kozachinskiy": "Alexander Kozachinskiy",
      "1_Tomasz Steifer": "Tomasz Steifer"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kozachinskiy24a/kozachinskiy24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3241-3256,\u00a02024.",
    "supplemental": ""
  },
  "64_Open Problem Anytime Convergence Rate of Gradient Descent": {
    "title": "Open Problem: Anytime Convergence Rate of Gradient Descent",
    "abstract": "Recent results show that vanilla gradient descent can be accelerated for smooth convex objectives, merely by changing the stepsize sequence. We show that this can lead to surprisingly large errors indefinitely, and therefore ask: Is there any stepsize schedule for gradient descent that accelerates the classic $\\mathcal{O}(1/T)$ convergence rate, at \\emph{any} stopping time $T$?",
    "url": "https://proceedings.mlr.press/v247/kornowski24a.html",
    "id": "https://proceedings.mlr.press/v247/kornowski24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kornowski24a/kornowski24a.pdf",
    "authors": {
      "0_Guy Kornowski": "Guy Kornowski",
      "1_Ohad Shamir": "Ohad Shamir"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kornowski24a/kornowski24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5335-5339,\u00a02024.",
    "supplemental": ""
  },
  "65_Gaussian Cooling and Dikin Walks The InteriorPoint Method for Logconcave Sampling": {
    "title": "Gaussian Cooling and Dikin Walks: The Interior-Point Method for Logconcave Sampling",
    "abstract": " The connections between (convex) optimization and (logconcave) sampling have been considerably enriched in the past decade with many conceptual and mathematical analogies. For instance, the Langevin algorithm can be viewed as a sampling analogue of gradient descent and has condition-number-dependent guarantees on its performance. In the early 1990s, Nesterov and Nemirovski developed the Interior-Point Method (IPM) for convex optimization based on self-concordant barriers, providing efficient algorithms for structured convex optimization, often faster than the general method. This raises the following question: can we develop an analogous IPM for structured sampling problems? In 2012, Kannan and Narayanan proposed the Dikin walk for uniformly sampling polytopes, and an improved analysis was given in 2020 by Laddha-Lee-Vempala. The Dikin walk uses a local metric defined by a self-concordant barrier for linear constraints. Here we generalize this approach by developing and adapting IPM machinery together with the Dikin walk for poly-time sampling algorithms. Our IPM-based sampling framework provides an efficient warm start and goes beyond uniform distributions and linear constraints. We illustrate the approach on important special cases, in particular giving the fastest algorithms to sample uniform, exponential, or Gaussian distributions on a truncated PSD cone. The framework is general and can be applied to other sampling algorithms.",
    "url": "https://proceedings.mlr.press/v247/kook24b.html",
    "id": "https://proceedings.mlr.press/v247/kook24b.html",
    "pdf": "https://proceedings.mlr.press/v247/kook24b/kook24b.pdf",
    "authors": {
      "0_Yunbum Kook": "Yunbum Kook",
      "1_Santosh S. Vempala": "Santosh S. Vempala"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kook24b/kook24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3137-3240,\u00a02024.",
    "supplemental": ""
  },
  "66_Sampling from the MeanField Stationary Distribution": {
    "title": "Sampling from the Mean-Field Stationary Distribution",
    "abstract": " We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term. Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.",
    "url": "https://proceedings.mlr.press/v247/kook24a.html",
    "id": "https://proceedings.mlr.press/v247/kook24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kook24a/kook24a.pdf",
    "authors": {
      "0_Yunbum Kook": "Yunbum Kook",
      "1_Matthew S. Zhang": "Matthew S. Zhang",
      "2_Sinho Chewi": "Sinho Chewi",
      "3_Murat A. Erdogdu": "Murat A. Erdogdu",
      "4_Mufan (Bill) Li": "Mufan (Bill) Li"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kook24a/kook24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3099-3136,\u00a02024.",
    "supplemental": ""
  },
  "67_Convergence of Kinetic Langevin Monte Carlo on Lie groups": {
    "title": "Convergence of Kinetic Langevin Monte Carlo on Lie groups",
    "abstract": " Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the trivialized momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under $W_2$ distance. Only compactness of the Lie group and geodesically $L$-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langevin on curved spaces, and also the first quantitative result that requires no convexity or, at least not explicitly, any common relaxation such as isoperimetry.",
    "url": "https://proceedings.mlr.press/v247/kong24a.html",
    "id": "https://proceedings.mlr.press/v247/kong24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kong24a/kong24a.pdf",
    "authors": {
      "0_Lingkai Kong": "Lingkai Kong",
      "1_Molei Tao": "Molei Tao"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kong24a/kong24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:3011-3063,\u00a02024.",
    "supplemental": ""
  },
  "68_Superconstant Inapproximability of Decision Tree Learning": {
    "title": "Superconstant Inapproximability of Decision Tree Learning",
    "abstract": "We consider the task of properly PAC learning decision trees with queries. Recent work of Koch, Strassle, and Tan showed that the strictest version of this task, where the hypothesis tree T is required to be optimally small, is NP-hard. Their work leaves open the question of whether the task remains intractable if T is only required to be close to optimal, say within a factor of 2, rather than exactly optimal.  We answer this affirmatively and show that the task indeed remains NP-hard even if T is allowed to be within any constant factor of optimal. More generally, our result allows for a smooth tradeoff between the hardness assumption and inapproximability factor. As Koch et al.\u2019s techniques do not appear to be amenable to such a strengthening, we first recover their result with a new and simpler proof, which we couple with a new XOR lemma for decision trees. While there is a large body of work on XOR lemmas for decision trees, our setting necessitates parameters that are extremely sharp and are not known to be attainable by existing such lemmas. Our work also carries new implications for the related problem of Decision Tree Minimization. ",
    "url": "https://proceedings.mlr.press/v247/koch24a.html",
    "id": "https://proceedings.mlr.press/v247/koch24a.html",
    "pdf": "https://proceedings.mlr.press/v247/koch24a/koch24a.pdf",
    "authors": {
      "0_Caleb Koch": "Caleb Koch",
      "1_Carmen Strassle": "Carmen Strassle",
      "2_Li-Yang Tan": "Li-Yang Tan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/koch24a/koch24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2979-3010,\u00a02024.",
    "supplemental": ""
  },
  "69_Learning Intersections of Halfspaces with Distribution Shift Improved Algorithms and SQ Lower Bounds": {
    "title": "Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds",
    "abstract": "  Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with distribution shift (TDS learning), where a learner is given labeled samples from training distribution $\\mathcal{D}$, unlabeled samples from test distribution $\\mathcal{D}\u2019$, and the goal is to output a classifier with low error on $\\mathcal{D}\u2019$ whenever the training samples pass a corresponding test.  Their model deviates from all prior work in that no assumptions are made on $\\mathcal{D}\u2019$.  Instead, the test must accept (with high probability) when the marginals of the training and test distributions are equal.  Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training distributions and prove a variety of new upper bounds including a $2^{(k/\\epsilon)^{O(1)}} \\mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\\epsilon$ (prior work achieved $d^{(k/\\epsilon)^{O(1)}}$).  We work under the mild assumption that the Gaussian training distribution contains at least an $\\epsilon$ fraction of both positive and negative examples ($\\epsilon$-balanced).  We also prove the first set of SQ lower-bounds for any TDS learning problem and show (1) the $\\epsilon$-balanced assumption is necessary for $\\mathsf{poly}(d,1/\\epsilon)$-time TDS learning for a single halfspace and (2) a $d^{\\tilde{\\Omega}(\\log 1/\\epsilon)}$ lower bound for the intersection of two general halfspaces, even with the $\\epsilon$-balanced assumption. Our techniques significantly expand the toolkit for TDS learning.  We use dimension reduction and coverings to give efficient algorithms for computing a localized version of discrepancy distance, a key metric from the domain adaptation literature.",
    "url": "https://proceedings.mlr.press/v247/klivans24b.html",
    "id": "https://proceedings.mlr.press/v247/klivans24b.html",
    "pdf": "https://proceedings.mlr.press/v247/klivans24b/klivans24b.pdf",
    "authors": {
      "0_Adam Klivans": "Adam Klivans",
      "1_Konstantinos Stavropoulos": "Konstantinos Stavropoulos",
      "2_Arsen Vasilyan": "Arsen Vasilyan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/klivans24b/klivans24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2944-2978,\u00a02024.",
    "supplemental": ""
  },
  "70_Testable Learning with Distribution Shift": {
    "title": "Testable Learning with Distribution Shift",
    "abstract": "  We revisit the fundamental problem of learning with distribution shift, in which a learner is given labeled samples from training distribution D, unlabeled samples from test distribution D\u2019 and is asked to output a classifier with low test error.  The standard approach in this setting is to bound the loss of a classifier in terms of some notion of distance between D and D\u2019.  These distances, however, seem difficult to compute and do not lead to efficient algorithms. We depart from this paradigm and define a new model called  testable learning with distribution shift, where we can obtain provably efficient algorithms for certifying the performance of a classifier on a test distribution.  In this model, a learner outputs a classifier with low test error whenever samples from D and D\u2019 pass an associated test; moreover, the test must accept (with high probability) if the marginal of D equals the marginal of D\u2019. We give several positive results for learning well-studied concept classes such as halfspaces, intersections of halfspaces, and decision trees when the marginal of D is Gaussian or uniform on the hypercube.  Prior to our work, no efficient algorithms for these basic cases were known without strong assumptions on D\u2019. For halfspaces in the realizable case (where there exists a halfspace consistent with both D and D\u2019), we combine a moment-matching approach with ideas from active learning to simulate an efficient oracle for estimating disagreement regions. To extend to the non-realizable setting, we apply recent work from testable (agnostic) learning.  More generally, we prove that any function class with low-degree $\\mathcal{L}_2$-sandwiching polynomial approximators can be learned in our model.  Since we require $\\mathcal{L}_2$- sandwiching (instead of the usual $\\mathcal{L}_1$ loss), we cannot directly appeal to convex duality and instead apply constructions from the pseudorandomness literature to obtain the required approximators.  We also provide lower bounds to show that the guarantees we obtain on the performance of our output hypotheses are best possible up to constant factors, as well as a separation showing that realizable learning in our model is incomparable to (ordinary) agnostic learning.",
    "url": "https://proceedings.mlr.press/v247/klivans24a.html",
    "id": "https://proceedings.mlr.press/v247/klivans24a.html",
    "pdf": "https://proceedings.mlr.press/v247/klivans24a/klivans24a.pdf",
    "authors": {
      "0_Adam Klivans": "Adam Klivans",
      "1_Konstantinos Stavropoulos": "Konstantinos Stavropoulos",
      "2_Arsen Vasilyan": "Arsen Vasilyan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/klivans24a/klivans24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2887-2943,\u00a02024.",
    "supplemental": ""
  },
  "71_Lasso with Latents Efficient Estimation Covariate Rescaling and ComputationalStatistical Gaps": {
    "title": "Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps",
    "abstract": "It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations. In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection. Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general. In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation. Moreover, we design a simple, efficient procedure for computing such a \u201csmart scaling.\u201d The sample complexity of the resulting \u201crescaled Lasso\u201d algorithm incurs (in the worst case) quadratic dependence on the sparsity of the underlying signal. While this dependence is not information-theoretically necessary, we give evidence that it is optimal among the class of polynomial-time algorithms, via the method of low-degree polynomials. This argument reveals a new connection between sparse linear regression and a special version of sparse PCA with a \\emph{near-critical negative spike}. The latter problem can be thought of as a real-valued analogue of learning a sparse parity. Using it, we also establish the first computational-statistical gap for the closely related problem of learning a Gaussian Graphical Model.",
    "url": "https://proceedings.mlr.press/v247/kelner24a.html",
    "id": "https://proceedings.mlr.press/v247/kelner24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kelner24a/kelner24a.pdf",
    "authors": {
      "0_Jonathan Kelner": "Jonathan Kelner",
      "1_Frederic Koehler": "Frederic Koehler",
      "2_Raghu Meka": "Raghu Meka",
      "3_Dhruv Rohatgi": "Dhruv Rohatgi"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kelner24a/kelner24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2840-2886,\u00a02024.",
    "supplemental": ""
  },
  "72_Choosing the p in Lp Loss Adaptive Rates for Symmetric Mean Estimation": {
    "title": "Choosing the p in Lp Loss: Adaptive Rates for Symmetric Mean Estimation",
    "abstract": "When we have a univariate distribution that is symmetric around its mean, the mean can be estimated with a rate (sample complexity) much faster than $O(1/\\sqrt{n})$ in many cases. For example, given univariate random variables $Y_1, \\ldots, Y_n$ distributed uniformly on $[\\theta_0 - c, \\theta_0 + c]$, the sample midrange $\\frac{Y_{(n)}+Y_{(1)}}{2}$ maximizes likelihood and has expected error $\\mathbb{E}\\bigl| \\theta_0 - \\frac{Y_{(n)}+Y_{(1)}}{2} \\bigr| \\leq 2c/n$, which is optimal and much lower than the error rate $O(1/\\sqrt{n})$ of the sample mean. What the optimal rate is depends on the distribution and it is generally attained by the maximum likelihood estimator (MLE). However, MLE requires exact knowledge of the underlying distribution; if the underlying distribution is \\emph{unknown}, it is an open question whether an estimator can adapt to the optimal rate. In this paper, we propose an estimator of the symmetric mean $\\theta_0$ with the following properties: it requires no knowledge of the underlying distribution; it has a rate no worse than $1/\\sqrt{n}$ in all cases (assuming a finite second moment) and, when the underlying distribution is compactly supported, our estimator can attain a rate of $n^{-\\frac{1}{{\\alpha}}}$ up to polylog factors, where the rate parameter $\\alpha$ can take on any value in $(0, 2]$ and depends on the moments of the underlying distribution. Our estimator is formed by minimizing the $L_\\gamma$-loss with respect to the data, for a power $\\gamma \\geq 2$ chosen in a data-driven way \u2013 by minimizing a criterion motivated by the asymptotic variance. Our approach can be directly applied to the regression setting where $\\theta_0$ is a function of observed features and motivates the use of $L_\\gamma$ loss function with a data-driven $\\gamma$ in certain settings. ",
    "url": "https://proceedings.mlr.press/v247/kao24a.html",
    "id": "https://proceedings.mlr.press/v247/kao24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kao24a/kao24a.pdf",
    "authors": {
      "0_Yu-Chun Kao": "Yu-Chun Kao",
      "1_Min Xu": "Min Xu",
      "2_Cun-Hui Zhang": "Cun-Hui Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kao24a/kao24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2795-2839,\u00a02024.",
    "supplemental": ""
  },
  "73_Smaller Confidence Intervals From IPW Estimators via DataDependent Coarsening Extended Abstract": {
    "title": "Smaller Confidence Intervals From IPW Estimators via Data-Dependent Coarsening (Extended Abstract)",
    "abstract": "Inverse propensity-score weighted (IPW) estimators are prevalent in causal inference for estimating average treatment effects in observational studies. Under unconfoundedness, given accurate propensity scores and $n$ samples, the size of confidence intervals of IPW estimators scales down with $n$, and, several of their variants improve the rate of scaling. However, neither IPW estimators nor their variants are robust to inaccuracies: even if a single covariate has an $\\epsilon>0$ additive error in the propensity score, the size of confidence intervals of these estimators can increase arbitrarily. Moreover, even without errors, the rate with which the confidence intervals of these estimators go to zero with $n$ can be arbitrarily slow in the presence of extreme propensity scores (those close to 0 or 1). We introduce a family of Coarse IPW (CIPW) estimators that captures existing IPW estimators and their variants. Each CIPW estimator is an IPW estimator on a coarsened covariate space, where certain covariates are merged. Under mild assumptions, e.g., Lipschitzness in expected outcomes and sparsity of extreme propensity scores, we give an efficient algorithm to find a robust estimator: given $\\epsilon$-inaccurate propensity scores and $n$ samples, its confidence interval size scales with $\\epsilon+(1/\\sqrt{n})$. In contrast, under the same assumptions, existing estimators\u2019 confidence interval sizes are $\\Omega(1)$ irrespective of $\\epsilon$ and $n$. Crucially, our estimator is data-dependent and we show that no data-independent CIPW estimator can be robust to inaccuracies. ",
    "url": "https://proceedings.mlr.press/v247/kalavasis24a.html",
    "id": "https://proceedings.mlr.press/v247/kalavasis24a.html",
    "pdf": "https://proceedings.mlr.press/v247/kalavasis24a/kalavasis24a.pdf",
    "authors": {
      "0_Alkis Kalavasis": "Alkis Kalavasis",
      "1_Anay Mehrotra": "Anay Mehrotra",
      "2_Manolis Zampetakis": "Manolis Zampetakis"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/kalavasis24a/kalavasis24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2767-2767,\u00a02024.",
    "supplemental": ""
  },
  "74_Some Constructions of Private Efficient and Optimal KNorm and Elliptic Gaussian Noise": {
    "title": "Some Constructions of Private, Efficient, and Optimal $K$-Norm and Elliptic Gaussian Noise",
    "abstract": "Differentially private computation often begins with a bound on some $d$-dimensional statistic\u2019s $\\ell_p$ sensitivity. For pure differential privacy, the $K$-norm mechanism can improve on this approach using a norm tailored to the statistic\u2019s sensitivity space. Writing down a closed-form description of this optimal norm is often straightforward. However, running the $K$-norm mechanism reduces to uniformly sampling the norm\u2019s unit ball; this ball is a $d$-dimensional convex body, so general sampling algorithms can be slow. Turning to concentrated differential privacy, elliptic Gaussian noise offers similar improvement over spherical Gaussian noise. Once the shape of this ellipse is determined, sampling is easy; however, identifying the best such shape may be hard. This paper solves both problems for the simple statistics of sum, count, and vote. For each statistic, we provide a sampler for the optimal $K$-norm mechanism that runs in time $\\tilde O(d^2)$ and derive a closed-form expression for the optimal shape of elliptic Gaussian noise. The resulting algorithms all yield meaningful accuracy improvements while remaining fast and simple enough to be practical. More broadly, we suggest that problem-specific sensitivity space analysis may be an overlooked tool for private additive noise.",
    "url": "https://proceedings.mlr.press/v247/joseph24a.html",
    "id": "https://proceedings.mlr.press/v247/joseph24a.html",
    "pdf": "https://proceedings.mlr.press/v247/joseph24a/joseph24a.pdf",
    "authors": {
      "0_Matthew Joseph": "Matthew Joseph",
      "1_Alexander Yu": "Alexander Yu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/joseph24a/joseph24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2723-2766,\u00a02024.",
    "supplemental": ""
  },
  "75_Faster Spectral Density Estimation and Sparsification in the Nuclear Norm Extended Abstract": {
    "title": "Faster Spectral Density Estimation and Sparsification in the Nuclear Norm (Extended Abstract)",
    "abstract": " We consider the problem of estimating the spectral density of a normalized graph adjacency matrix. Concretely, given an undirected graph $G = (V, E, w)$ with $n$ nodes and positive edge weights $w \\in \\mathbb{R}^{E}_{> 0}$, the goal is to return eigenvalue estimates $\\widehat{\\lambda}_1 \\le \\cdots\\le \\widehat{\\lambda}_n$ such that \\begin{align*} \\frac{1}{n} \\sum_{i\\in\\{1,\\ldots, n\\}}|\\widehat{\\lambda}_i-\\lambda_i(N_G)|\\le \\varepsilon, \\end{align*} where ${\\lambda}_1(N_G)\\le \\cdots\\le{\\lambda}_n(N_G)$ are the eigenvalues of $G$\u2019s normalized adjacency matrix, $N_G$. This goal is equivalent to requiring that the Wasserstein-1 distance between the uniform distribution on $\\lambda_1, \\ldots, \\lambda_n$ and the uniform distribution on $\\widehat{\\lambda}_1, \\ldots, \\widehat{\\lambda}_n$ is less than $\\varepsilon$. We provide a randomized algorithm that achieves the guarantee above with $O(n\\varepsilon^{-2})$ queries to a degree and neighbor oracle and in $O(n\\varepsilon^{-3})$ time. This improves on previous state-of-the-art methods, including an $O(n\\varepsilon^{-7})$ time algorithm from [Braverman et al., STOC 2022] and, for sufficiently small $\\varepsilon$, a $2^{O(\\varepsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018]. To achieve this result, we introduce a new notion of graph sparsification, which we call \\emph{nuclear sparsification}. We provide an $O(n\\varepsilon^{-2})$-query and $O(n\\varepsilon^{-2})$-time algorithm for computing $O(n\\varepsilon^{-2})$-sparse nuclear sparsifiers. We show that this bound is optimal in both its sparsity and query complexity, and we separate our results from the related notion of additive spectral sparsification. Of independent interest, we show that our sparsification method also yields the first \\emph{deterministic} algorithm for spectral density estimation that scales linearly with $n$ (sublinear in the representation size of the graph).",
    "url": "https://proceedings.mlr.press/v247/jin24a.html",
    "id": "https://proceedings.mlr.press/v247/jin24a.html",
    "pdf": "https://proceedings.mlr.press/v247/jin24a/jin24a.pdf",
    "authors": {
      "0_Yujia Jin": "Yujia Jin",
      "1_Ishani Karmarkar": "Ishani Karmarkar",
      "2_Christopher Musco": "Christopher Musco",
      "3_Aaron Sidford": "Aaron Sidford",
      "4_Apoorv Vikram Singh": "Apoorv Vikram Singh"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/jin24a/jin24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2722-2722,\u00a02024.",
    "supplemental": ""
  },
  "76_Algorithms for meanfield variational inference via polyhedral optimization in the Wasserstein space": {
    "title": "Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space",
    "abstract": "We develop a theory of finite-dimensional polyhedral subsets over the Wasserstein space and optimization of functionals over them via first-order methods.  Our main application is to the problem of mean-field variational inference, which seeks to approximate a distribution $\\pi$ over $\\mathbb{R}^d$ by a product measure $\\pi^\\star$. When $\\pi$ is strongly log-concave and log-smooth, we provide (1) approximation rates certifying that $\\pi^\\star$ is close to the minimizer $\\pi^\\star_\\diamond$ of the KL divergence over a \\emph{polyhedral} set $\\mathcal{P}_\\diamond$, and (2) an algorithm for minimizing $\\text{KL}(\\cdot\\|\\pi)$ over $\\mathcal{P}_\\diamond$ with accelerated complexity $O(\\sqrt \\kappa \\log(\\kappa d/\\varepsilon^2))$, where $\\kappa$ is the condition number of $\\pi$. ",
    "url": "https://proceedings.mlr.press/v247/jiang24a.html",
    "id": "https://proceedings.mlr.press/v247/jiang24a.html",
    "pdf": "https://proceedings.mlr.press/v247/jiang24a/jiang24a.pdf",
    "authors": {
      "0_Yiheng Jiang": "Yiheng Jiang",
      "1_Sinho Chewi": "Sinho Chewi",
      "2_Aram-Alexandre Pooladian": "Aram-Alexandre Pooladian"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/jiang24a/jiang24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2720-2721,\u00a02024.",
    "supplemental": ""
  },
  "77_Offline Reinforcement Learning Role of State Aggregation and Trajectory Data": {
    "title": "Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data",
    "abstract": "We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question of whether bounded  (all-policy) concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The concentrability coefficient in the aggregated Markov Transition Model may grow exponentially with the horizon length, even when the concentrability coefficient in the original MDP is small and the offline data is \\emph{admissible} (i.e., the data distribution equals the occupancy measure of some policy), 3) Under value function realizability, there is a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data, implying that trajectory data offers no extra benefits over admissible data. These three pieces jointly resolve the open problem, though each of them could be of independent interest. ",
    "url": "https://proceedings.mlr.press/v247/jia24a.html",
    "id": "https://proceedings.mlr.press/v247/jia24a.html",
    "pdf": "https://proceedings.mlr.press/v247/jia24a/jia24a.pdf",
    "authors": {
      "0_Zeyu Jia": "Zeyu Jia",
      "1_Alexander Rakhlin": "Alexander Rakhlin",
      "2_Ayush Sekhari": "Ayush Sekhari",
      "3_Chen-Yu Wei": "Chen-Yu Wei"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/jia24a/jia24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2644-2719,\u00a02024.",
    "supplemental": ""
  },
  "78_Closing the ComputationalQuery Depth Gap in Parallel Stochastic Convex Optimization": {
    "title": "Closing the Computational-Query Depth Gap in Parallel Stochastic Convex Optimization",
    "abstract": "We develop a new parallel algorithm for minimizing Lipschitz, convex functions with a stochastic subgradient oracle. The total number of queries made and the query depth, i.e., the number of parallel rounds of queries, match the prior state-of-the-art, [CJJLLST23], while improving upon the computational depth by a polynomial factor for sufficiently small accuracy. When combined with previous state-of-the-art methods our result closes a gap between the best-known query depth and the best-known computational depth of parallel algorithms.  Our method starts with a \\emph{ball acceleration} framework of previous parallel methods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a regularized Gaussian convolution of the function constrained to Euclidean balls. By developing and leveraging new stability properties of the Hessian of this induced function, we depart from prior parallel algorithms and reduce these ball-constrained optimization problems to stochastic unconstrained quadratic minimization problems. Although we are unable to prove concentration of the asymmetric matrices that we use to approximate this Hessian, we nevertheless develop an efficient parallel method for solving these quadratics. Interestingly, our algorithms can be improved using fast matrix multiplication and run in nearly-linear time if the matrix multiplication exponent is 2.",
    "url": "https://proceedings.mlr.press/v247/jambulapati24b.html",
    "id": "https://proceedings.mlr.press/v247/jambulapati24b.html",
    "pdf": "https://proceedings.mlr.press/v247/jambulapati24b/jambulapati24b.pdf",
    "authors": {
      "0_Arun Jambulapati": "Arun Jambulapati",
      "1_Aaron Sidford": "Aaron Sidford",
      "2_Kevin Tian": "Kevin Tian"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/jambulapati24b/jambulapati24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2608-2643,\u00a02024.",
    "supplemental": ""
  },
  "79_BlackBox kto1PCA Reductions Theory and Applications": {
    "title": "Black-Box k-to-1-PCA Reductions: Theory and Applications",
    "abstract": "The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have black-box access to via samples. Motivated by these  settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.  Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work in sample complexity by a $\\mathsf{poly}(k)$ factor.",
    "url": "https://proceedings.mlr.press/v247/jambulapati24a.html",
    "id": "https://proceedings.mlr.press/v247/jambulapati24a.html",
    "pdf": "https://proceedings.mlr.press/v247/jambulapati24a/jambulapati24a.pdf",
    "authors": {
      "0_Arun Jambulapati": "Arun Jambulapati",
      "1_Syamantak Kumar": "Syamantak Kumar",
      "2_Jerry Li": "Jerry Li",
      "3_Shourya Pandey": "Shourya Pandey",
      "4_Ankit Pensia": "Ankit Pensia",
      "5_Kevin Tian": "Kevin Tian"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/jambulapati24a/jambulapati24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2564-2607,\u00a02024.",
    "supplemental": ""
  },
  "80_Adaptive Learning Rate for FollowtheRegularizedLeader Competitive Analysis and BestofBothWorlds": {
    "title": "Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Analysis and Best-of-Both-Worlds",
    "abstract": "Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL\u2019s learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for the learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotone non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\\xi$-approximately monotone non-increasing. Our proposed update rule, referred to as \\textit{stability-penalty matching}, also facilitates the construction of Best-Of-Both-Worlds (BOBW) algorithms for stochastic and adversarial environments. In these environments our results contribute to achieving tighter regret bound and broaden the applicability of algorithms for various settings such as multi-armed bandits, graph bandits, linear bandits, and contextual bandits.",
    "url": "https://proceedings.mlr.press/v247/ito24a.html",
    "id": "https://proceedings.mlr.press/v247/ito24a.html",
    "pdf": "https://proceedings.mlr.press/v247/ito24a/ito24a.pdf",
    "authors": {
      "0_Shinji Ito": "Shinji Ito",
      "1_Taira Tsuchiya": "Taira Tsuchiya",
      "2_Junya Honda": "Junya Honda"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/ito24a/ito24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2522-2563,\u00a02024.",
    "supplemental": ""
  },
  "81_Reconstructing the Geometry of Random Geometric Graphs Extended Abstract": {
    "title": "Reconstructing the Geometry of Random Geometric Graphs (Extended Abstract)",
    "abstract": "Random geometric graphs are random graph models defined on metric spaces. Such a  model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the {\\em manifold} assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\\mathbb{R}^N$.   Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distance",
    "url": "https://proceedings.mlr.press/v247/huang24c.html",
    "id": "https://proceedings.mlr.press/v247/huang24c.html",
    "pdf": "https://proceedings.mlr.press/v247/huang24c/huang24c.pdf",
    "authors": {
      "0_Han Huang": "Han Huang",
      "1_Pakawut Jiradilok": "Pakawut Jiradilok",
      "2_Elchanan Mossel": "Elchanan Mossel"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/huang24c/huang24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2519-2521,\u00a02024.",
    "supplemental": ""
  },
  "82_InformationTheoretic Thresholds for the Alignments of Partially Correlated Graphs": {
    "title": "Information-Theoretic Thresholds for the Alignments of Partially Correlated Graphs",
    "abstract": "This paper studies the problem of recovering the hidden vertex correspondence between two correlated random graphs. We propose the partially correlated Erd\u0151s-R\u00e9nyi graphs model, wherein a pair of induced subgraphs with a certain number are correlated. We investigate the information-theoretic thresholds for recovering the latent correlated subgraphs and the hidden vertex correspondence. We prove that there exists an optimal rate for partial recovery for the number of correlated nodes, above which one can correctly match a fraction of vertices and below which correctly matching any positive fraction is impossible, and we also derive an optimal rate for exact recovery. In the proof of possibility results, we propose correlated functional digraphs, which categorize the edges of the intersection graph into two cases of components, and bound the error probability by lower-order cumulant generating functions. The proof of impossibility results build upon the generalized Fano\u2019s inequality and the recovery thresholds settled in correlated Erd\u0151s-R\u00e9nyi graphs model",
    "url": "https://proceedings.mlr.press/v247/huang24b.html",
    "id": "https://proceedings.mlr.press/v247/huang24b.html",
    "pdf": "https://proceedings.mlr.press/v247/huang24b/huang24b.pdf",
    "authors": {
      "0_Dong Huang": "Dong Huang",
      "1_Xianwen Song": "Xianwen Song",
      "2_Pengkun Yang": "Pengkun Yang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/huang24b/huang24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2494-2518,\u00a02024.",
    "supplemental": ""
  },
  "83_Faster Sampling without Isoperimetry via Diffusionbased Monte Carlo": {
    "title": "Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo",
    "abstract": " To sample from a general target distribution $p_*\\propto e^{-f_*}$ beyond the isoperimetric condition, Huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to Diffusion-based  Monte Carlo (DMC). Specifically,  DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of the original DMC algorithm originates from its redundant design of score estimation, and proposed a more efficient DMC algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation and sampling subproblems accordingly, which are correlated in a recursive manner. Importantly, we show that with a proper design of the segment decomposition, all sampling subproblems will only need to tackle a strongly log-concave distribution, which can be very efficient to solve using the standard sampler (e.g., Langevin Monte Carlo) with a provably rapid convergence rate. As a result, we prove that the gradient complexity of RS-DMC exhibits merely a quasi-polynomial dependency on $\\epsilon$. This finding is highly unexpected as it substantially enhances the prevailing belief of the necessity for exponential gradient complexity in all prior works such as Huang et al. (2023). Under commonly used dissipative conditions, our algorithm is provably much faster than the popular Langevin-based algorithms. Our algorithm design and theoretical framework illuminate a novel direction for addressing sampling problems, which could be of broader applicability in the community. ",
    "url": "https://proceedings.mlr.press/v247/huang24a.html",
    "id": "https://proceedings.mlr.press/v247/huang24a.html",
    "pdf": "https://proceedings.mlr.press/v247/huang24a/huang24a.pdf",
    "authors": {
      "0_Xunpeng Huang": "Xunpeng Huang",
      "1_Difan Zou": "Difan Zou",
      "2_Hanze Dong": "Hanze Dong",
      "3_Yi-An Ma": "Yi-An Ma",
      "4_Tong Zhang": "Tong Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/huang24a/huang24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2438-2493,\u00a02024.",
    "supplemental": ""
  },
  "84_Open Problem Optimal Rates for Stochastic DecisionTheoretic Online Learning Under Differentially Privacy": {
    "title": "Open Problem: Optimal Rates for Stochastic Decision-Theoretic Online Learning Under Differentially Privacy",
    "abstract": "For the stochastic variant of decision-theoretic online learning with $K$ actions, $T$ rounds, and minimum gap $\\Delta_{\\min}$, the optimal, gap-dependent rate of the pseudo-regret is known to be $O \\left( \\frac{\\log K}{\\Delta_{\\min}} \\right)$. We ask to settle the optimal gap-dependent rate for the problem under $\\varepsilon$-differential privacy.",
    "url": "https://proceedings.mlr.press/v247/hu24a.html",
    "id": "https://proceedings.mlr.press/v247/hu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/hu24a/hu24a.pdf",
    "authors": {
      "0_Bingshan Hu": "Bingshan Hu",
      "1_Nishant A. Mehta": "Nishant A. Mehta"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/hu24a/hu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5330-5334,\u00a02024.",
    "supplemental": ""
  },
  "85_On the sample complexity of parameter estimation in logistic regression with normal design": {
    "title": "On the sample complexity of parameter estimation in logistic regression with normal design",
    "abstract": "The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.",
    "url": "https://proceedings.mlr.press/v247/hsu24a.html",
    "id": "https://proceedings.mlr.press/v247/hsu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/hsu24a/hsu24a.pdf",
    "authors": {
      "0_Daniel Hsu": "Daniel Hsu",
      "1_Arya Mazumdar": "Arya Mazumdar"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/hsu24a/hsu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2418-2437,\u00a02024.",
    "supplemental": ""
  },
  "86_AdversariallyRobust Inference on Trees via Belief Propagation": {
    "title": "Adversarially-Robust Inference on Trees via Belief Propagation",
    "abstract": "We introduce and study the problem of posterior inference on tree-structured graphical models in the presence of a malicious adversary who can corrupt some observed nodes. In the well-studied \\emph{broadcasting on trees} model, corresponding to the ferromagnetic Ising model on a $d$-regular tree with zero external field, when a natural signal-to-noise ratio exceeds one (the celebrated \\emph{Kesten-Stigum threshold}), the posterior distribution of the root given the leaves is bounded away from $\\mathrm{Ber}(1/2)$, and carries nontrivial information about the sign of the root. This posterior distribution can be computed exactly via dynamic programming, also known as belief propagation. We first confirm a folklore belief that a malicious adversary who can corrupt an inverse-polynomial fraction of the leaves of their choosing makes this inference impossible. Our main result is that accurate posterior inference about the root vertex given the leaves \\emph{is} possible when the adversary is constrained to make corruptions at a $\\rho$-fraction of randomly-chosen leaf vertices, so long as the signal-to-noise ratio exceeds $O(\\log d)$ and $\\rho \\leq c \\varepsilon$ for some universal $c > 0$. Since inference becomes information-theoretically impossible when $\\rho \\gg \\varepsilon$, this amounts to an information-theoretically optimal fraction of corruptions, up to a constant multiplicative factor. Furthermore, we show that the canonical belief propagation algorithm performs this inference.",
    "url": "https://proceedings.mlr.press/v247/hopkins24a.html",
    "id": "https://proceedings.mlr.press/v247/hopkins24a.html",
    "pdf": "https://proceedings.mlr.press/v247/hopkins24a/hopkins24a.pdf",
    "authors": {
      "0_Samuel B. Hopkins": "Samuel B. Hopkins",
      "1_Anqui Li": "Anqui Li"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/hopkins24a/hopkins24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2389-2417,\u00a02024.",
    "supplemental": ""
  },
  "87_Open problem Direct Sums in Learning Theory": {
    "title": "Open problem: Direct Sums in Learning Theory",
    "abstract": "In computer science, the term \u2019direct sum\u2019 refers to fundamental questions about the scaling of computational or information complexity with respect to multiple task instances. Consider an algorithmic task \\({T} \\){and} a computational resource \\({C} \\). For instance, \\({T} \\){might} be the task of computing a polynomial, with \\({C} \\){representing} the number of arithmetic operations required, or \\({T} \\){could} be a learning task with its sample complexity as \\({C} \\). The direct sum inquiry focuses on the cost of solving\u00a0\\({k} \\){separate} instances of \\({T} \\), particularly how this aggregate cost compares to the resources needed for a single instance. Typically, the cost for multiple instances is at most \\({k} \\){times} the cost of one, since each can be handled independently. However, there are intriguing scenarios where the total cost for \\({k} \\){instances} is less than this linear relationship. Such questions naturally extend to the machine-learning setting in which one may be interested in solving several learning problems at once. This notion of direct sums of learning problems gives rise to various natural questions and interesting problems",
    "url": "https://proceedings.mlr.press/v247/hanneke24c.html",
    "id": "https://proceedings.mlr.press/v247/hanneke24c.html",
    "pdf": "https://proceedings.mlr.press/v247/hanneke24c/hanneke24c.pdf",
    "authors": {
      "0_Steve Hanneke": "Steve Hanneke",
      "1_Shay Moran": "Shay Moran",
      "2_Waknine Tom": "Waknine Tom"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/hanneke24c/hanneke24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5325-5329,\u00a02024.",
    "supplemental": ""
  },
  "88_List Sample Compression and Uniform Convergence": {
    "title": "List Sample Compression and Uniform Convergence",
    "abstract": "List learning is a variant of supervised classification where the learner outputs multiple plausible labels for each instance rather than just one.  We investigate classical principles related to generalization within the context of list learning. Our primary goal is to determine whether classical principles in the PAC setting retain their applicability in the domain of list PAC learning. We focus on uniform convergence (which is the basis of Empirical Risk Minimization) and on sample compression (which is a powerful manifestation of Occam\u2019s Razor). In classical PAC learning, both uniform convergence and sample compression satisfy a form of \u2018completeness\u2019: whenever a class is learnable, it can also be learned by a learning rule that adheres to these principles. We ask whether the same completeness holds true in the list learning setting. We show that uniform convergence remains equivalent to learnability in the list PAC learning setting. In contrast, our findings reveal surprising results regarding sample compression: we prove that when the label space is $Y=\\{0,1,2\\}$, then there are 2-list-learnable classes that cannot be compressed. This refutes the list version of the sample compression conjecture by Littlestone and Warmuth in 1986. We prove an even stronger impossibility result, showing that there are $2$-list-learnable classes that cannot be compressed even when the reconstructed function can work with lists of arbitrarily large size. We prove a similar result for (1-list) PAC learnable classes when the label space is unbounded.",
    "url": "https://proceedings.mlr.press/v247/hanneke24b.html",
    "id": "https://proceedings.mlr.press/v247/hanneke24b.html",
    "pdf": "https://proceedings.mlr.press/v247/hanneke24b/hanneke24b.pdf",
    "authors": {
      "0_Steve Hanneke": "Steve Hanneke",
      "1_Shay Moran": "Shay Moran",
      "2_Waknine Tom": "Waknine Tom"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/hanneke24b/hanneke24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2360-2388,\u00a02024.",
    "supplemental": ""
  },
  "89_The Star Number and Eluder Dimension Elementary Observations About the Dimensions of Disagreement": {
    "title": "The Star Number and Eluder Dimension: Elementary Observations About the Dimensions of Disagreement",
    "abstract": "This article presents a number of elementary observations and relations concerning commonly-studied combinatorial dimensions from the learning theory literature on classification and reinforcement learning: namely, the star number, eluder dimension, VC dimension, Littlestone dimension, threshold dimension, and cardinality of the class. One theme of the work is understanding how these dimensions may be re-expressed as natural dimensions of the convexity space of version spaces. Specifically, we find that the star number is precisely the VC dimension of version spaces (and of their disagreement regions), whereas the eluder dimension is precisely the threshold dimension of version spaces (and of their disagreement regions). We are also interested in understanding direct relations among these dimensions. For instance, we show that there is no infinite concept class with both finite Littlestone dimension and finite star number. Moreover, any infinite concept class must have infinite eluder dimension. In both cases, we also provide quantitative relations to the cardinality of the class. For the latter result, we also show an analogous relation for real-valued functions, where the cardinality of the class is replaced by the $L_\\infty$ covering number. As another relation between star numbers and VC dimension, we provide a simple, precise, and general characterization of the VC dimension of the minimal intersection-closed class containing a given concept class: namely, the 1-centered star number of the original class. Moreover, we generalize this result to provide a unifying approach to the design of certain sample compression schemes, along with a simple combinatorial dimension characterizing its compression size: the minimum star number. We also discuss a number of implications of many of these observations. Though the proofs of the above observations are actually all incredibly simple, it is interesting that such fundamental relations among these well-known quantities appear to have heretofore gone unnoticed in the literature.",
    "url": "https://proceedings.mlr.press/v247/hanneke24a.html",
    "id": "https://proceedings.mlr.press/v247/hanneke24a.html",
    "pdf": "https://proceedings.mlr.press/v247/hanneke24a/hanneke24a.pdf",
    "authors": {
      "0_Steve Hanneke": "Steve Hanneke"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/hanneke24a/hanneke24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2308-2359,\u00a02024.",
    "supplemental": ""
  },
  "90_Prediction from compression for models with infinite memory with applications to hidden Markov and renewal processes": {
    "title": "Prediction from compression for models with infinite memory, with applications to hidden Markov and renewal processes",
    "abstract": "Consider the problem of predicting the next symbol given a sample path of length $n$, whose joint distribution belongs to a distribution class that may have long-term memory. The goal is to compete with the conditional predictor that knows the true model. For both hidden Markov models (HMMs) and renewal processes, we determine the optimal prediction risk in Kullback-Leibler divergence up to universal constant factors.  Extending existing results in finite-order Markov models (Han et al. (2023)) and drawing ideas from universal compression, the proposed estimator has a prediction risk bounded by redundancy of the distribution class and a memory term that accounts for the long-range dependency of the model. Notably, for HMMs with bounded state and observation spaces, a polynomial-time estimator based on dynamic programming is shown to achieve the optimal prediction risk $\\Theta(\\frac{\\log n}{n})$; prior to this work, the only known result of this type is $O(\\frac{1}{\\log n})$ obtained using Markov approximation (Sharan et al. (2018)). Matching minimax lower bounds are obtained by making connections to redundancy and mutual information via a reduction argument.",
    "url": "https://proceedings.mlr.press/v247/han24a.html",
    "id": "https://proceedings.mlr.press/v247/han24a.html",
    "pdf": "https://proceedings.mlr.press/v247/han24a/han24a.pdf",
    "authors": {
      "0_Yanjun Han": "Yanjun Han",
      "1_Tianze Jiang": "Tianze Jiang",
      "2_Yihong Wu": "Yihong Wu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/han24a/han24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2270-2307,\u00a02024.",
    "supplemental": ""
  },
  "91_Beyond Catoni Sharper Rates for HeavyTailed and Robust Mean Estimation": {
    "title": "Beyond Catoni: Sharper Rates for Heavy-Tailed and Robust Mean Estimation",
    "abstract": "We study the fundamental problem of estimating the mean of a $d$-dimensional distribution with covariance $\\Sigma \\preccurlyeq \\sigma^2 I_d$ given $n$ samples. When $d = 1$, \\cite{catoni} showed an estimator with error $(1+o(1)) \\cdot \\sigma \\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{n}}$, with probability $1 - \\delta$, matching the Gaussian error rate. For $d>1$, a natural estimator outputs the center of the minimum enclosing ball of one-dimensional confidence intervals to achieve a $1-\\delta$ confidence radius of $\\sqrt{\\frac{2 d}{d+1}} \\cdot \\sigma \\left(\\sqrt{\\frac{d}{n}} + \\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{n}}\\right)$, incurring a $\\sqrt{\\frac{2d}{d+1}}$-factor loss over the Gaussian rate. When the $\\sqrt{\\frac{d}{n}}$ term dominates by a $\\sqrt{\\log \\frac{1}{\\delta}}$ factor, \\cite{lee2022optimal-highdim} showed an improved estimator matching the Gaussian rate. This raises a natural question: Is the $\\sqrt{\\frac{2 d}{d+1}}$ loss \\emph{necessary} when the $\\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{n}}$ term dominates? We show that the answer is \\emph{no} \u2013 we construct an estimator that improves over the above naive estimator by a constant factor. We also consider robust estimation, where an adversary is allowed to corrupt an $\\epsilon$-fraction of samples arbitrarily: in this case, we show that the above strategy of combining one-dimensional estimates and incurring the $\\sqrt{\\frac{2d}{d+1}}$-factor \\emph{is} optimal in the infinite-sample limit.",
    "url": "https://proceedings.mlr.press/v247/gupta24a.html",
    "id": "https://proceedings.mlr.press/v247/gupta24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gupta24a/gupta24a.pdf",
    "authors": {
      "0_Shivam Gupta": "Shivam Gupta",
      "1_Samuel Hopkins": "Samuel Hopkins",
      "2_Eric Price": "Eric Price"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gupta24a/gupta24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2232-2269,\u00a02024.",
    "supplemental": ""
  },
  "92_Stochastic Constrained Contextual Bandits via Lyapunov Optimization Based Estimation to Decision Framework": {
    "title": "Stochastic Constrained Contextual Bandits via Lyapunov Optimization Based Estimation to Decision Framework",
    "abstract": "This paper studies the problem of stochastic constrained contextual bandits (CCB) under general realizability condition where the expected rewards and costs are within general function classes. We propose LOE2D, a Lyapunov Optimization Based Estimation to Decision framework with online regression oracles for learning reward/constraint. LOE2D establishes $\\Tilde O(T^{\\frac{3}{4}}U^{\\frac{1}{4}})$ regret and constraint violation, which can be further refined to $\\Tilde O(\\min\\{\\sqrt{TU}/\\varepsilon^2, T^{\\frac{3}{4}}U^{\\frac{1}{4}}\\})$ when the Slater condition holds in the underlying offline problem with the Slater \u201cconstant\u201d $ \\varepsilon=\\Omega(\\sqrt{U/T}),$ where $U$ denotes the error bounds of online regression oracles. These results improve LagrangeCBwLC in two aspects: i) our results hold without any prior information while LagrangeCBwLC requires the knowledge of Slater constant to design a proper learning rate; ii) our results hold when $\\varepsilon=\\Omega(\\sqrt{U/T})$ while LagrangeCBwLC requires a constant margin $\\varepsilon=\\Omega(1).$ These improvements stem from two novel techniques: violation-adaptive learning in E2D module and multi-step Lyapunov drift analysis in bounding constraint violation. The experiments further justify LOE2D outperforms the baseline algorithm. ",
    "url": "https://proceedings.mlr.press/v247/guo24a.html",
    "id": "https://proceedings.mlr.press/v247/guo24a.html",
    "pdf": "https://proceedings.mlr.press/v247/guo24a/guo24a.pdf",
    "authors": {
      "0_Hengquan Guo": "Hengquan Guo",
      "1_Xin Liu": "Xin Liu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/guo24a/guo24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2204-2231,\u00a02024.",
    "supplemental": ""
  },
  "93_Community detection in the hypergraph stochastic block model and reconstruction on hypertrees": {
    "title": "Community detection in the hypergraph stochastic block model and reconstruction on hypertrees",
    "abstract": "We study the weak recovery problem on the $r$-uniform hypergraph stochastic block model ($r$-HSBM) with two balanced communities. In this model, $n$ vertices are randomly divided into two communities, and size-$r$ hyperedges are added randomly depending on whether all vertices in the hyperedge are in the same community. The goal of weak recovery is to recover a non-trivial fraction of the communities given the hypergraph. Pal and Zhu (2021); Stephan and Zhu (2022) established that weak recovery is always possible above a natural threshold called the Kesten-Stigum (KS) threshold. For assortative models (i.e., monochromatic hyperedges are preferred), Gu and Polyanskiy (2023) proved that the KS threshold is tight if $r\\le 4$ or the expected degree $d$ is small. For other cases, the tightness of the KS threshold remained open. In this paper we determine the tightness of the KS threshold for a wide range of parameters. We prove that for $r\\le 6$ and $d$ large enough, the KS threshold is tight. This shows that there is no information-computation gap in this regime and partially confirms a conjecture of  Angelini et al. (2015). On the other hand, we show that for $r\\ge 5$, there exist parameters for which the KS threshold is not tight. In particular, for $r\\ge 7$, the KS threshold is not tight if the model is disassortative (i.e., polychromatic hyperedges are preferred) or $d$ is large enough. This provides more evidence supporting the existence of an information-computation gap in these cases. Furthermore, we establish asymptotic bounds on the weak recovery threshold for fixed $r$ and large $d$. We also obtain a number of results regarding the broadcasting on hypertrees (BOHT) model, including the asymptotics of the reconstruction threshold for $r\\ge 7$ and impossibility of robust reconstruction at criticality.",
    "url": "https://proceedings.mlr.press/v247/gu24a.html",
    "id": "https://proceedings.mlr.press/v247/gu24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gu24a/gu24a.pdf",
    "authors": {
      "0_Yuzhou Gu": "Yuzhou Gu",
      "1_Aaradhya Pandey": "Aaradhya Pandey"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gu24a/gu24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2166-2203,\u00a02024.",
    "supplemental": ""
  },
  "94_Principal eigenstate classical shadows": {
    "title": "Principal eigenstate classical shadows",
    "abstract": "Given many copies of an unknown quantum state $\\rho$, we consider the task of learning a classical description of its principal eigenstate. Namely, assuming that $\\rho$ has an eigenstate $|\\phi\u27e9$ with (unknown) eigenvalue $\\lambda > 1/2$, the goal is to learn a (classical shadows style) classical description of $|\\phi\u27e9$ which can later be used to estimate expectation values $\u27e8\\phi |O | \\phi \u27e9$ for any $O$ in some class of observables. We consider the sample-complexity setting in which generating a copy of $\\rho$ is expensive, but joint measurements on many copies of the state are possible. We present a protocol for this task scaling with the principal eigenvalue $\\lambda$ and show that it is optimal within a space of natural approaches, e.g., applying quantum state purification followed by a single-copy classical shadows scheme. Furthermore, when $\\lambda$ is sufficiently close to $1$, the performance of our algorithm is optimal\u2014matching the sample complexity for pure state classical shadows.",
    "url": "https://proceedings.mlr.press/v247/grier24a.html",
    "id": "https://proceedings.mlr.press/v247/grier24a.html",
    "pdf": "https://proceedings.mlr.press/v247/grier24a/grier24a.pdf",
    "authors": {
      "0_Daniel Grier": "Daniel Grier",
      "1_Hakop Pashayan": "Hakop Pashayan",
      "2_Luke Schaeffer": "Luke Schaeffer"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/grier24a/grier24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2122-2165,\u00a02024.",
    "supplemental": ""
  },
  "95_On the Computability of Robust PAC Learning": {
    "title": "On the Computability of Robust PAC Learning",
    "abstract": "We initiate the study of computability requirements for adversarially robust learning. Adversarially robust PAC-type learnability is by now an established field of research. However, the effects of computability requirements in PAC-type frameworks are only just starting to emerge.  We introduce the problem of robust computable PAC (robust CPAC) learning and provide some simple sufficient conditions for this.  We then show that learnability in this setup is not implied by the combination of its components: classes that are both CPAC and robustly PAC learnable are not necessarily robustly CPAC learnable.  Furthermore, we show that the novel framework exhibits some surprising effects: for robust CPAC learnability it is not required that the robust loss is computably evaluable! Towards understanding characterizing properties, we introduce a novel dimension, the computable robust shattering dimension.  We prove that its finiteness is necessary, but not sufficient for robust CPAC learnability.  This might yield novel insights for the corresponding phenomenon in the context of robust PAC learnability, where insufficiency of the robust shattering dimension for learnability has been conjectured, but so far a resolution has remained elusive.",
    "url": "https://proceedings.mlr.press/v247/gourdeau24a.html",
    "id": "https://proceedings.mlr.press/v247/gourdeau24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gourdeau24a/gourdeau24a.pdf",
    "authors": {
      "0_Pascale Gourdeau": "Pascale Gourdeau",
      "1_Lechner. Tosca": "Lechner. Tosca",
      "2_Ruth Urner": "Ruth Urner"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gourdeau24a/gourdeau24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2092-2121,\u00a02024.",
    "supplemental": ""
  },
  "96_Identification of mixtures of discrete product distributions in nearoptimal sample and time complexity": {
    "title": "Identification of mixtures of discrete product distributions in near-optimal sample and time complexity",
    "abstract": "We consider the problem of \\emph{identifying,} from statistics, a distribution of discrete random variables $X_1 \\ldots,X_n$ that is a mixture of $k$ product distributions. The best previous sample complexity for $n \\in O(k)$ was $(1/\\zeta)^{O(k^2 \\log k)}$ (under a mild separation assumption parameterized by $\\zeta$). The best known lower bound was $\\exp(\\Omega(k))$. It is known that $n\\geq 2k-1$ is necessary and sufficient for identification. We show, for any $n\\geq 2k-1$, how to achieve sample complexity and run-time complexity $(1/\\zeta)^{O(k)}$.  We also extend the known lower bound of $e^{\\Omega(k)}$ to match our upper bound across a broad range of $\\zeta$. Our results are obtained by combining (a) a classic method for robust tensor decomposition, (b) a novel way of bounding the condition number of key matrices called Hadamard extensions, by studying their action only on flattened rank-1 tensors.",
    "url": "https://proceedings.mlr.press/v247/gordon24a.html",
    "id": "https://proceedings.mlr.press/v247/gordon24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gordon24a/gordon24a.pdf",
    "authors": {
      "0_Spencer L. Gordon": "Spencer L. Gordon",
      "1_Erik Jahn": "Erik Jahn",
      "2_Bijan Mazaheri": "Bijan Mazaheri",
      "3_Yuval Rabani": "Yuval Rabani",
      "4_Leonard J. Schulman": "Leonard J. Schulman"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gordon24a/gordon24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2071-2091,\u00a02024.",
    "supplemental": ""
  },
  "97_Omnipredictors for regression and the approximate rank of convex functions": {
    "title": "Omnipredictors for regression and the approximate rank of convex functions",
    "abstract": "Consider the supervised learning setting where the goal is to learn to predict labels $\\mathbf y$ given points $\\mathbf x$ from a distribution. An \\textit{omnipredictor} for a class $\\mathcal L$ of loss functions and a class $\\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\\mathcal C$ for every loss in $\\mathcal L$. Since the work of Gopalan et al. (2021) that introduced the notion, there has been a large body of work in the setting of binary labels where $\\mathbf y \\in \\{0, 1\\}$, but much less is known about the regression setting where $\\mathbf y \\in [0,1]$ can be continuous. The naive generalization of the previous approaches to regression is to predict the probability distribution of $y$, discretized to $\\varepsilon$-width intervals. The running time would be exponential in the size of the output of the omnipredictor, which is $1/\\varepsilon$. Our main conceptual contribution is the notion of \\textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approximate rank of the family of loss functions. Thus, improved bounds on the latter yield improved runtimes for learning omnipredictors. Our key technical contribution is a bound of $O(1/\\varepsilon^{2/3})$ on the $\\epsilon$-approximate rank of convex, Lipschitz functions on the interval $[0,1]$, which we show is tight up to a factor of $\\mathrm{polylog} (1/\\epsilon)$.  This yields improved runtimes for learning omnipredictors for the class of all convex, Lipschitz loss functions under weak learnability assumptions about the class $\\mathcal C$. We also give efficient omnipredictors when the loss families have low-degree polynomial approximations, or arise from generalized linear models (GLMs). This translation from sufficient statistics to faster omnipredictors is made possible by lifting the technique of loss outcome indistinguishability introduced by Gopalan et al. (2023a) for Boolean labels to the regression setting.",
    "url": "https://proceedings.mlr.press/v247/gopalan24b.html",
    "id": "https://proceedings.mlr.press/v247/gopalan24b.html",
    "pdf": "https://proceedings.mlr.press/v247/gopalan24b/gopalan24b.pdf",
    "authors": {
      "0_Parikshit Gopalan": "Parikshit Gopalan",
      "1_Princewill Okoroafor": "Princewill Okoroafor",
      "2_Prasad Raghavendra": "Prasad Raghavendra",
      "3_Abhishek Sherry": "Abhishek Sherry",
      "4_Mihir Singhal": "Mihir Singhal"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gopalan24b/gopalan24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2027-2070,\u00a02024.",
    "supplemental": ""
  },
  "98_On Computationally Efficient MultiClass Calibration": {
    "title": "On Computationally Efficient Multi-Class Calibration",
    "abstract": "Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: \\emph{Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$?} Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.  Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of \\emph{projected smooth calibration} for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \\subseteq [k]$: \\emph{e.g. is this an image of an animal?} It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability.  Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting. This allows us to use kernel methods to design efficient algorithms, and also to use known hardness results for agnostic learning based on the hardness of refuting random CSPs to show lower bounds. ",
    "url": "https://proceedings.mlr.press/v247/gopalan24a.html",
    "id": "https://proceedings.mlr.press/v247/gopalan24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gopalan24a/gopalan24a.pdf",
    "authors": {
      "0_Parikshit Gopalan": "Parikshit Gopalan",
      "1_Lunjia Hu": "Lunjia Hu",
      "2_Guy N. Rothblum": "Guy N. Rothblum"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gopalan24a/gopalan24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1983-2026,\u00a02024.",
    "supplemental": ""
  },
  "99_Mirror Descent Algorithms with Nearly DimensionIndependent Rates for DifferentiallyPrivate Stochastic SaddlePoint Problems extended abstract": {
    "title": "Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems extended abstract",
    "abstract": "We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\\varepsilon, \\delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} + \\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability. This result provides evidence of the near-optimality of the approach. Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest.",
    "url": "https://proceedings.mlr.press/v247/gonzalez24a.html",
    "id": "https://proceedings.mlr.press/v247/gonzalez24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gonzalez24a/gonzalez24a.pdf",
    "authors": {
      "0_Tomas Gonzalez": "Tomas Gonzalez",
      "1_Cristobal Guzman": "Cristobal Guzman",
      "2_Courtney Paquette": "Courtney Paquette"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gonzalez24a/gonzalez24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1982-1982,\u00a02024.",
    "supplemental": ""
  },
  "100_Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions": {
    "title": "Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions",
    "abstract": "One of the most natural approaches to reinforcement learning (RL) with function approximation is value iteration, which inductively generates approximations to the optimal value function by solving a sequence of regression problems. To ensure the success of value iteration, it is typically assumed that Bellman completeness holds, which ensures that these regression problems are well- specified. We study the problem of learning an optimal policy under Bellman completeness in the online model of RL with linear function approximation. In the linear setting, while statistically efficient algorithms are known under Bellman completeness (e.g., (Jiang et al., 2017; Zanette et al., 2020a)), these algorithms all rely on the principle of global optimism which requires solving a nonconvex optimization problem. In particular, it has remained open as to whether computationally efficient algorithms exist. In this paper we give the first polynomial-time algorithm for RL under linear Bellman completeness when the number of actions is any constant.",
    "url": "https://proceedings.mlr.press/v247/golowich24a.html",
    "id": "https://proceedings.mlr.press/v247/golowich24a.html",
    "pdf": "https://proceedings.mlr.press/v247/golowich24a/golowich24a.pdf",
    "authors": {
      "0_Noah Golowich": "Noah Golowich",
      "1_Ankur Moitra": "Ankur Moitra"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/golowich24a/golowich24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1939-1981,\u00a02024.",
    "supplemental": ""
  },
  "101_On Convex Optimization with SemiSensitive Features": {
    "title": "On Convex Optimization with Semi-Sensitive Features",
    "abstract": "We study the differentially private (DP) empirical risk minimization (ERM) problem under the \\emph{semi-sensitive DP} setting where only some features are sensitive. This generalizes the Label DP setting where only the label is sensitive. We give improved upper and lower bounds on the excess risk for DP-ERM. In particular, we show that the error only scales polylogarithmically in terms of the sensitive domain size, improving upon previous results that scale polynomially in the size of the sensitive domain (Ghazi et al., NeurIPS 2021).",
    "url": "https://proceedings.mlr.press/v247/ghazi24a.html",
    "id": "https://proceedings.mlr.press/v247/ghazi24a.html",
    "pdf": "https://proceedings.mlr.press/v247/ghazi24a/ghazi24a.pdf",
    "authors": {
      "0_Badih Ghazi": "Badih Ghazi",
      "1_Pritish Kamath": "Pritish Kamath",
      "2_Ravi Kumar": "Ravi Kumar",
      "3_Pasin Manurangsi": "Pasin Manurangsi",
      "4_Raghu Meka": "Raghu Meka",
      "5_Chiyuan Zhang": "Chiyuan Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/ghazi24a/ghazi24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1916-1938,\u00a02024.",
    "supplemental": ""
  },
  "102_\u03b5 uAdaptive Regret Minimization in HeavyTailed Bandits": {
    "title": "$(\u03b5, u)$-Adaptive Regret Minimization in Heavy-Tailed Bandits",
    "abstract": "Heavy-tailed distributions naturally arise in several settings, from finance to telecommunications. While regret minimization under subgaussian or bounded rewards has been widely studied, learning with heavy-tailed distributions only gained popularity over the last decade. In this paper, we consider the setting in which the reward distributions have finite absolute raw moments of maximum order $1+\\epsilon$, uniformly bounded by a constant $u<+\\infty$, for some $\\epsilon \\in (0,1]$. In this setting, we study the regret minimization problem when $\\epsilon$ and $u$ are unknown to the learner and it has to adapt. First, we show that adaptation comes at a cost and derive two negative results proving that the same regret guarantees of the non-adaptive case cannot be achieved with no further assumptions. Then, we devise and analyze a fully data-driven trimmed mean estimator and propose a novel adaptive regret minimization algorithm, \\texttt{AdaR-UCB}, that leverages such an estimator. Finally, we show that \\texttt{AdaR-UCB} is the first algorithm that, under a known distributional assumption, enjoys regret guarantees nearly matching those of the non-adaptive heavy-tailed case.",
    "url": "https://proceedings.mlr.press/v247/genalti24a.html",
    "id": "https://proceedings.mlr.press/v247/genalti24a.html",
    "pdf": "https://proceedings.mlr.press/v247/genalti24a/genalti24a.pdf",
    "authors": {
      "0_Gianmarco Genalti": "Gianmarco Genalti",
      "1_Lupo Marsigli": "Lupo Marsigli",
      "2_Nicola Gatti": "Nicola Gatti",
      "3_Alberto Maria Metelli": "Alberto Maria Metelli"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/genalti24a/genalti24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1882-1915,\u00a02024.",
    "supplemental": ""
  },
  "103_Adversarial Online Learning with Temporal Feedback Graphs": {
    "title": "Adversarial Online Learning with Temporal Feedback Graphs",
    "abstract": "We study a variant of prediction with expert advice where the learner\u2019s action at round $t$ is only allowed to depend on losses on a specific subset of the rounds (where the structure of which rounds\u2019 losses are visible at time $t$ is provided by a directed \u201cfeedback graph\u201d known to the learner). We present a novel learning algorithm for this setting based on a strategy of partitioning the losses across sub-cliques of this graph. We complement this with a lower bound that is tight in many practical settings, and which we conjecture to be within a constant factor of optimal. For the important class of transitive feedback graphs, we prove that this algorithm is efficiently implementable and obtains the optimal regret bound (up to a universal constant).",
    "url": "https://proceedings.mlr.press/v247/gatmiry24b.html",
    "id": "https://proceedings.mlr.press/v247/gatmiry24b.html",
    "pdf": "https://proceedings.mlr.press/v247/gatmiry24b/gatmiry24b.pdf",
    "authors": {
      "0_Khashayar Gatmiry": "Khashayar Gatmiry",
      "1_Jon Schneider": "Jon Schneider"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gatmiry24b/gatmiry24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:4548-4572,\u00a02024.",
    "supplemental": ""
  },
  "104_Sampling Polytopes with Riemannian HMC Faster Mixing via the Lewis Weights Barrier": {
    "title": "Sampling Polytopes with Riemannian HMC: Faster Mixing via the Lewis Weights Barrier",
    "abstract": "We analyze Riemannian Hamiltonian Monte Carlo (RHMC) on a manifold endowed with the metric defined by the Hessian of a convex barrier function and apply it to sample a polytope defined by $m$ inequalities in $\\R^n$. The advantage of RHMC over Euclidean methods such as the ball walk, hit-and-run and the Dikin walk is in its ability to take longer steps. However, in all previous work, the mixing rate of RHMC has a linear dependence on the number of inequalities. We introduce a hybrid of the Lewis weight barrier and the standard logarithmic barrier and prove that the mixing rate for the corresponding RHMC is bounded by $\\tilde O(m^{1/3}n^{4/3})$, improving on the previous best bound of $\\tilde O(mn^{2/3})$ (based on the log barrier). This continues the general parallels between optimization and sampling, with the latter typically leading to new tools and requiring more refined analysis. To prove our main results, we overcomes several challenges relating to the smoothness of Hamiltonian curves and self-concordance properties of the barrier. In the process, we give a general framework for the analysis of Markov chains on Riemannian manifolds, derive new smoothness bounds on Hamiltonian curves, a central topic of comparison geometry, and extend self-concordance theory to the infinity norm, which gives sharper bounds; these properties all appear to be of independent interest.",
    "url": "https://proceedings.mlr.press/v247/gatmiry24a.html",
    "id": "https://proceedings.mlr.press/v247/gatmiry24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gatmiry24a/gatmiry24a.pdf",
    "authors": {
      "0_Khashayar Gatmiry": "Khashayar Gatmiry",
      "1_Jonathan Kelner": "Jonathan Kelner",
      "2_Santosh S. Vempala": "Santosh S. Vempala"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gatmiry24a/gatmiry24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1796-1881,\u00a02024.",
    "supplemental": ""
  },
  "105_Safe Linear Bandits over Unknown Polytopes": {
    "title": "Safe Linear Bandits over Unknown Polytopes",
    "abstract": "The safe linear bandit problem (SLB) is an online approach to linear programming with unknown objective and unknown \\emph{roundwise} constraints, under stochastic bandit feedback of rewards and safety risks of actions. We study the tradeoffs between efficacy and smooth safety costs of SLBs over polytopes, and the role of aggressive {doubly-optimistic play} in avoiding the strong assumptions made by extant pessimistic-optimistic approaches.  We first elucidate an inherent hardness in SLBs due the lack of knowledge of constraints: there exist \u2018easy\u2019 instances, for which suboptimal extreme points have large \u2018gaps\u2019, but on which SLB methods must still incur $\\Omega(\\sqrt{T})$ regret or safety violations, due to an inability to resolve unknown optima to arbitrary precision. We then analyse a natural doubly-optimistic strategy for the safe linear bandit problem, \\textsc{doss}, which uses optimistic estimates of both reward and safety risks to select actions, and show that despite the lack of knowledge of constraints or feasible points, \\textsc{doss} simultaneously obtains tight instance-dependent $O(\\log^2 T)$ bounds on efficacy regret, and $\\widetilde O(\\sqrt{T})$ bounds on safety violations, thus attaining near Pareto-optimality. Further, when safety is demanded to a finite precision, violations improve to $O(\\log^2 T).$ These results rely on a novel dual analysis of linear bandits: we argue that \\textsc{doss} proceeds by activating noisy versions of at least $d$ constraints in each round, which allows us to separately analyse rounds where a \u2018poor\u2019 set of constraints is activated, and rounds where \u2018good\u2019 sets of constraints are activated. The costs in the former are controlled to $O(\\log^2 T)$ by developing new dual notions of gaps, based on global sensitivity analyses of linear programs, that quantify the suboptimality of each such set of constraints. The latter costs are controlled to $O(1)$ by explicitly analysing the solutions of optimistic play. ",
    "url": "https://proceedings.mlr.press/v247/gangrade24a.html",
    "id": "https://proceedings.mlr.press/v247/gangrade24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gangrade24a/gangrade24a.pdf",
    "authors": {
      "0_Aditya Gangrade": "Aditya Gangrade",
      "1_Tianrui Chen": "Tianrui Chen",
      "2_Venkatesh Saligrama": "Venkatesh Saligrama"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gangrade24a/gangrade24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1755-1795,\u00a02024.",
    "supplemental": ""
  },
  "106_Agnostic Active Learning of Single Index Models with Linear Sample Complexity": {
    "title": "Agnostic Active Learning of Single Index Models with Linear Sample Complexity",
    "abstract": "  We study active learning methods for single index models of the form $F({\\bm x}) = f(\u27e8{\\bm w}, {\\bm x}\u27e9)$, where $f:\\mathbb{R} \\to \\mathbb{R}$ and ${\\bx,\\bm w} \\in \\mathbb{R}^d$. In addition to their theoretical interest as simple examples of non-linear neural networks, single index models have received significant recent attention due to applications in scientific machine learning like surrogate modeling for partial differential equations (PDEs). Such applications require sample-efficient active learning methods that are robust to adversarial noise. I.e., that work even in the challenging agnostic learning setting. We provide two main results on agnostic active learning of single index models. First, when $f$ is known and Lipschitz, we show that $\\tilde{O}(d)$ samples collected via {statistical leverage score sampling} are sufficient to learn a near-optimal single index model. Leverage score sampling is simple to implement, efficient, and already widely used for actively learning linear models. Our result requires no assumptions on the data distribution, is optimal up to log factors, and improves quadratically on a recent ${O}(d^{2})$ bound of Gajjar et. al 2023. Second, we show that $\\tilde{O}(d)$ samples suffice even in the more difficult setting when $f$ is \\emph{unknown}. Our results leverage tools from high dimensional probability, including Dudley\u2019s inequality and dual Sudakov minoration, as well as a novel, distribution-aware discretization of the class of Lipschitz functions.",
    "url": "https://proceedings.mlr.press/v247/gajjar24a.html",
    "id": "https://proceedings.mlr.press/v247/gajjar24a.html",
    "pdf": "https://proceedings.mlr.press/v247/gajjar24a/gajjar24a.pdf",
    "authors": {
      "0_Aarshvi Gajjar": "Aarshvi Gajjar",
      "1_Wai Ming Tai": "Wai Ming Tai",
      "2_Xu Xingyu": "Xu Xingyu",
      "3_Chinmay Hegde": "Chinmay Hegde",
      "4_Christopher Musco": "Christopher Musco",
      "5_Yi Li": "Yi Li"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/gajjar24a/gajjar24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1715-1754,\u00a02024.",
    "supplemental": ""
  },
  "107_Online Newton Method for Bandit Convex Optimisation Extended Abstract": {
    "title": "Online Newton Method for Bandit Convex Optimisation Extended Abstract",
    "abstract": "We introduce a computationally efficient algorithm for zeroth-order bandit convex optimisation and prove that in the adversarial setting its regret is at most $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$ is the dimension and $n$ is the time horizon.  In the stochastic setting the bound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in [d^{-1/2}, d^{-1/4}]$ is a constant that depends on the geometry of the constraint set and the desired computational properties.",
    "url": "https://proceedings.mlr.press/v247/fokkema24a.html",
    "id": "https://proceedings.mlr.press/v247/fokkema24a.html",
    "pdf": "https://proceedings.mlr.press/v247/fokkema24a/fokkema24a.pdf",
    "authors": {
      "0_Hidde Fokkema": "Hidde Fokkema",
      "1_Dirk Van der Hoeven": "Dirk Van der Hoeven",
      "2_Tor Lattimore": "Tor Lattimore",
      "3_Jack J. Mayo": "Jack J. Mayo"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/fokkema24a/fokkema24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1713-1714,\u00a02024.",
    "supplemental": ""
  },
  "108_Computationinformation gap in highdimensional clustering": {
    "title": "Computation-information gap in high-dimensional clustering",
    "abstract": "We investigate the existence of a fundamental computation-information gap for the problem of clustering a mixture of isotropic Gaussian in the high-dimensional regime, where the ambient dimension $p$ is larger than the number $n$ of points. The existence of a computation-information gap in a specific Bayesian high-dimensional asymptotic regime has been conjectured by Lesieur et. al (2016) based on the replica heuristic from statistical physics.  We provide  evidence of the existence of such a gap generically in the high-dimensional regime $p\\geq n$, by (i)  proving a non-asymptotic low-degree polynomials computational barrier for clustering in high-dimension, matching the performance of the best known polynomial time algorithms, and by (ii) establishing that the information barrier for clustering is smaller than the computational barrier, when the number $K$ of clusters is large enough.  These results are in contrast with the (moderately) low-dimensional regime $n\\geq \\text{poly}(p,K)$, where there is no computation-information gap for clustering a mixture of isotropic Gaussian.  In order to prove our low-degree computational barrier, we develop sophisticated combinatorial arguments to upper-bound the mixed moments of the signal under a Bernoulli Bayesian model.",
    "url": "https://proceedings.mlr.press/v247/even24a.html",
    "id": "https://proceedings.mlr.press/v247/even24a.html",
    "pdf": "https://proceedings.mlr.press/v247/even24a/even24a.pdf",
    "authors": {
      "0_Bertrand Even": "Bertrand Even",
      "1_Christophe Giraud": "Christophe Giraud",
      "2_Nicolas Verzelen": "Nicolas Verzelen"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/even24a/even24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1646-1712,\u00a02024.",
    "supplemental": ""
  },
  "109_Contraction of Markovian Operators in Orlicz Spaces and Error Bounds for Markov Chain Monte Carlo Extended Abstract": {
    "title": "Contraction of Markovian Operators in Orlicz Spaces and Error Bounds for Markov Chain Monte Carlo (Extended Abstract)",
    "abstract": " We introduce a novel concept of convergence for Markovian processes within Orlicz spaces, extending beyond the conventional approach associated with $L_p$ spaces. After showing that Markovian operators are contractive in Orlicz spaces, our technical contribution is an upper bound on their contraction coefficient, which admits a closed-form expression. The bound is tight in some settings, and it recovers well-known results, such as the connection between contraction and ergodicity, ultra-mixing and Doeblin\u2019s minorisation. Moreover, we can define a notion of convergence of Markov processes in Orlicz spaces, which depends on the corresponding contraction coefficient.  The key novelty comes from duality considerations: the convergence of a Markovian process determined by $K$ depends on the contraction coefficient of its dual $K^\\star$, which can in turn be bounded by considering appropriate nested norms of densities of $K^\\star$ with respect to the stationary measure. Our approach stands out as the first of its kind, as it does not rely on the existence of a spectral gap. Specialising our approach to $L_p$ spaces leads to a significant improvement upon classical Riesz-Thorin\u2019s interpolation methods. We present the following applications of the proposed framework: \\begin{enumerate} \\item Tighter bounds on the mixing time of Markovian processes: one can relate the contraction coefficient of the dual operator to the mixing time of the corresponding Markov chain regardless of the norm chosen. Consequently, our tighter bound on the contraction coefficient implies a tighter bound on the mixing time. We offer a result that provides an intuitive understanding of what it means to be close in a specific norm (relating the probability of any event with the probability of the same event under the stationary measure $\\pi$ and a $\\psi$-Orlicz/Amemiya-norm). We then focus on $L_p$ norms and show that asking for a bounded norm with larger $p$ guarantees a faster decay in the probability. This is particularly relevant for exponentially decaying probabilities under $\\pi$. Moreover, by exploiting the flexibility offered by Orlicz spaces, we can tackle settings where the stationary distribution is heavy-tailed, a severely under-studied setup. \\item Improved concentration bounds for MCMC methods leading to improved lower bounds on the burn-in period: by leveraging $L_p$-norms with large $p$ and our results on the contraction coefficient, similar to the approach undertaken for the mixing times, we can provide improved exponential concentration bounds for MCMC methods. \\item Improved concentration bounds for sequences of Markovian random variables: we show how our results can be used to outperform existing bounds based on a change of measure technique for random variables with a Markovian dependence. In particular, we can prove exponential concentration in new settings (inaccessible to earlier approaches) and improve the rate in others. \\end{enumerate}",
    "url": "https://proceedings.mlr.press/v247/esposito24a.html",
    "id": "https://proceedings.mlr.press/v247/esposito24a.html",
    "pdf": "https://proceedings.mlr.press/v247/esposito24a/esposito24a.pdf",
    "authors": {
      "0_Amedeo Roberto Esposito": "Amedeo Roberto Esposito",
      "1_Marco Mondelli": "Marco Mondelli"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/esposito24a/esposito24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1643-1645,\u00a02024.",
    "supplemental": ""
  },
  "110_Topological Expressivity of ReLU Neural Networks": {
    "title": "Topological Expressivity of ReLU Neural Networks",
    "abstract": "We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplification. This provides a mathematically rigorous explanation why deeper networks are better equipped to handle complex and topologically rich data sets.",
    "url": "https://proceedings.mlr.press/v247/ergen24a.html",
    "id": "https://proceedings.mlr.press/v247/ergen24a.html",
    "pdf": "https://proceedings.mlr.press/v247/ergen24a/ergen24a.pdf",
    "authors": {
      "0_Ekin Ergen": "Ekin Ergen",
      "1_Moritz Grillo": "Moritz Grillo"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/ergen24a/ergen24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1599-1642,\u00a02024.",
    "supplemental": ""
  },
  "111_The Real Price of Bandit Information in Multiclass Classification": {
    "title": "The Real Price of Bandit Information in Multiclass Classification",
    "abstract": "We revisit the classical problem of multiclass classification with bandit feedback (Kakade, Shalev-Shwartz and Tewari, 2008), where each input classifies to one of $K$ possible labels and feedback is restricted to whether the predicted label is correct or not. Our primary inquiry is with regard to the dependency on the number of labels $K$, and whether $T$-step regret bounds in this setting can be improved beyond the $\\smash{\\sqrt{KT}}$ dependence exhibited by existing algorithms.  Our main contribution is in showing that the minimax regret of bandit multiclass is in fact more nuanced, and is of the form $\\smash{\\widetilde{\\Theta}(\\min |\\mathcal{H}| + \\sqrt{T}, \\sqrt{KT \\log |\\mathcal{H}|})}$, where $\\mathcal{H}$ is the underlying (finite) hypothesis class. In particular, we present a new bandit classification algorithm that guarantees regret $\\smash{\\widetilde{O}(|\\mathcal{H}|+\\sqrt{T})}$, improving over classical algorithms for moderately-sized hypothesis classes, and give a matching lower bound establishing tightness of the upper bounds (up to log-factors) in all parameter regimes.",
    "url": "https://proceedings.mlr.press/v247/erez24a.html",
    "id": "https://proceedings.mlr.press/v247/erez24a.html",
    "pdf": "https://proceedings.mlr.press/v247/erez24a/erez24a.pdf",
    "authors": {
      "0_Liad Erez": "Liad Erez",
      "1_Alon Cohen": "Alon Cohen",
      "2_Tomer Koren": "Tomer Koren",
      "3_Yishay Mansour": "Yishay Mansour",
      "4_Shay Moran": "Shay Moran"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/erez24a/erez24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1573-1598,\u00a02024.",
    "supplemental": ""
  },
  "112_Minimax Linear Regression under the Quantile Risk": {
    "title": "Minimax Linear Regression under the Quantile Risk",
    "abstract": "We study the problem of designing minimax procedures in linear regression under the quantile risk. We start by considering the realizable setting with independent Gaussian noise, where for any given noise level and distribution of inputs, we obtain the \\emph{exact} minimax quantile risk for a rich family of error functions and establish the minimaxity of OLS. This improves on the lower bounds obtained by Lecue and Mendelson (2016) and Mendelson (2017) for the special case of square error, and provides us with a lower bound on the minimax quantile risk over larger sets of distributions.  Under the square error and a fourth moment assumption on the distribution of inputs, we show that this lower bound is tight over a larger class of problems. Specifically, we prove a matching upper bound on the worst-case quantile risk of a variant of the procedure proposed by Lecue and Lerasle (2020), thereby establishing its minimaxity, up to absolute constants. We illustrate the usefulness of our approach by extending this result to all $p$-th power error functions for $p \\in (2, \\infty)$. Along the way, we develop a generic analogue to the classical Bayesian method for lower bounding the minimax risk when working with the quantile risk, as well as a tight characterization of the quantiles of the smallest eigenvalue of the sample covariance matrix.",
    "url": "https://proceedings.mlr.press/v247/el-hanchi24a.html",
    "id": "https://proceedings.mlr.press/v247/el-hanchi24a.html",
    "pdf": "https://proceedings.mlr.press/v247/el-hanchi24a/el-hanchi24a.pdf",
    "authors": {
      "0_Ayoub El Hanchi": "Ayoub El Hanchi",
      "1_Chris Maddison": "Chris Maddison",
      "2_Murat Erdogdu": "Murat Erdogdu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/el-hanchi24a/el-hanchi24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1516-1572,\u00a02024.",
    "supplemental": ""
  },
  "113_On sampling diluted SpinGlasses using  Glauber Dynamics": {
    "title": "On sampling diluted Spin-Glasses using  Glauber Dynamics",
    "abstract": " {\\em Spin-glasses}  are natural  Gibbs distributions that have  been studied in theoretical computer science for many decades. Recently,   they have been gaining renewed attention from the community  as they emerge naturally in {\\em neural computation} and {\\em learning},  {\\em network inference}, {\\em optimisation}  and many other areas. Here we consider the {\\em {2-spin model}}  at inverse temperature $\\beta$ when the underlying graph is an instance of $G(n,d/n)$, i.e., the random graph on $n$ vertices such that  each edge appears independently with probability $d/n$,  where the expected degree $d=\\Theta(1)$. We study the problem of efficiently  sampling from the aforementioned distribution using the well-known Markov chain called {\\em Glauber dynamics}. For a certain range of $\\beta$, that depends only on the expected degree $d$ of the graph, and for typical instances of the {2-spin model} on $G(n,d/n)$, we show that the corresponding (single-site) Glauber dynamics exhibits mixing time $O\\left(n^{2+\\frac{3}{\\log^2 d}}\\right)$.  The range of   $\\beta$  for which we obtain our rapid mixing result corresponds to the expected influence being smaller than $1/d$.  We establish our results by utilising the well-known {\\em path-coupling} technique. In the standard setting of Glauber dynamics on $G(n,d/n)$ one has to deal with the so-called effect of high degree vertices. % in the path-coupling analysis. Here, with the spin-glasses, rather than considering  vertex-degrees, it is more natural to use a different measure on the vertices of the graph, that  we call {\\em aggregate influence}.  We  build  on the block-construction approach proposed by  [Dyer, Flaxman, Frieze and Vigoda: 2006] to circumvent the problem with the high degrees in the path-coupling analysis. Specifically, to obtain our results, we first  establish rapid mixing for an  appropriately defined block-dynamics. We design this dynamics such that vertices of large aggregate influence are placed deep inside their blocks. Then, we  obtain  rapid mixing for the (single-site) Glauber dynamics  by utilising  a comparison argument. ",
    "url": "https://proceedings.mlr.press/v247/efthymiou24a.html",
    "id": "https://proceedings.mlr.press/v247/efthymiou24a.html",
    "pdf": "https://proceedings.mlr.press/v247/efthymiou24a/efthymiou24a.pdf",
    "authors": {
      "0_Charilaos Efthymiou": "Charilaos Efthymiou",
      "1_Kostas Zampetakis": "Kostas Zampetakis"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/efthymiou24a/efthymiou24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1501-1515,\u00a02024.",
    "supplemental": ""
  },
  "114_An informationtheoretic lower bound in timeuniform estimation": {
    "title": "An information-theoretic lower bound in time-uniform estimation",
    "abstract": "We present an information-theoretic lower bound for the problem of parameter estimation with time-uniform coverage guarantees. We use a reduction to sequential testing to obtain stronger lower bounds that capture the hardness of the time-uniform setting. In the case of location model estimation and logistic regression, our lower bound is $\\Omega(\\sqrt{n^{-1}\\log \\log n})$, which is tight up to constant factors in typical settings.",
    "url": "https://proceedings.mlr.press/v247/duchi24a.html",
    "id": "https://proceedings.mlr.press/v247/duchi24a.html",
    "pdf": "https://proceedings.mlr.press/v247/duchi24a/duchi24a.pdf",
    "authors": {
      "0_John Duchi": "John Duchi",
      "1_Saminul Haque": "Saminul Haque"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/duchi24a/duchi24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1486-1500,\u00a02024.",
    "supplemental": ""
  },
  "115_Universal Lower Bounds and Optimal Rates Achieving Minimax Clustering Error in SubExponential Mixture Models": {
    "title": "Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models",
    "abstract": "Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd\u2019s algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through Chernoff information, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such mixtures, we establish that Bregman hard clustering, a variant of Lloyd\u2019s algorithm employing a Bregman divergence, is rate optimal.",
    "url": "https://proceedings.mlr.press/v247/dreveton24a.html",
    "id": "https://proceedings.mlr.press/v247/dreveton24a.html",
    "pdf": "https://proceedings.mlr.press/v247/dreveton24a/dreveton24a.pdf",
    "authors": {
      "0_Maximilien Dreveton": "Maximilien Dreveton",
      "1_Alperen G\u00f6zeten": "Alperen G\u00f6zeten",
      "2_Matthias Grossglauser": "Matthias Grossglauser",
      "3_Patrick Thiran": "Patrick Thiran"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/dreveton24a/dreveton24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1451-1485,\u00a02024.",
    "supplemental": ""
  },
  "116_Physicsinformed machine learning as a kernel method": {
    "title": "Physics-informed machine learning as a kernel method",
    "abstract": "Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer $\\hat f_n$ of the regularized risk and show that $\\hat f_n$ converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.",
    "url": "https://proceedings.mlr.press/v247/doumeche24a.html",
    "id": "https://proceedings.mlr.press/v247/doumeche24a.html",
    "pdf": "https://proceedings.mlr.press/v247/doumeche24a/doumeche24a.pdf",
    "authors": {
      "0_Nathan Doum\u00e8che": "Nathan Doum\u00e8che",
      "1_Francis Bach": "Francis Bach",
      "2_G\u00e9rard Biau": "G\u00e9rard Biau",
      "3_Claire Boyer": "Claire Boyer"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/doumeche24a/doumeche24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1399-1450,\u00a02024.",
    "supplemental": ""
  },
  "117_On the Growth of Mistakes in Differentially Private Online Learning A Lower Bound Perspective": {
    "title": "On the Growth of Mistakes in Differentially Private Online Learning: A Lower Bound Perspective",
    "abstract": " In this paper, we provide lower bounds for Differentially Private (DP) Online Learning algorithms. Our result shows that, for a broad class of\u00a0$(\\epsilon,\\delta)$-DP online algorithms, for number of rounds $T$ such that $\\log T\\leq O\\left(1 / \\delta\\right)$, the expected number of mistakes incurred by the algorithm grows as \\(\\Omega\\left(\\log T\\right)\\). This matches the upper bound obtained by Golowich and Livni (2021) and is in contrast to non-private online learning where the number of mistakes is independent of \\(T\\).  To the best of our knowledge, our work is the first result towards settling lower bounds for DP\u2013Online learning and partially addresses the open question in Sanyal and Ramponi (2022).",
    "url": "https://proceedings.mlr.press/v247/dmitriev24a.html",
    "id": "https://proceedings.mlr.press/v247/dmitriev24a.html",
    "pdf": "https://proceedings.mlr.press/v247/dmitriev24a/dmitriev24a.pdf",
    "authors": {
      "0_Daniil Dmitriev": "Daniil Dmitriev",
      "1_Krist\u00f3f Szab\u00f3": "Krist\u00f3f Szab\u00f3",
      "2_Amartya Sanyal": "Amartya Sanyal"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/dmitriev24a/dmitriev24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1379-1398,\u00a02024.",
    "supplemental": ""
  },
  "118_Efficiently Learning OneHiddenLayer ReLU Networks via SchurPolynomials": {
    "title": "Efficiently Learning One-Hidden-Layer ReLU Networks via SchurPolynomials",
    "abstract": "We study the problem of PAC learning a linear combination of $k$ ReLU activations  under the standard Gaussian distribution on $\\mathbb{R}^d$ with respect to the square loss.  Our main result is an efficient algorithm for this learning task with sample and  computational complexity $(dk/\\epsilon)^{O(k)}$, where $\\epsilon>0$ is the target accuracy.  Prior work had given an algorithm for this problem with complexity $(dk/\\epsilon)^{h(k)}$,  where the function $h(k)$ scales super-polynomially in $k$. Interestingly,  the complexity of our algorithm is near-optimal within the class of  Correlational Statistical Query algorithms.  At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions.  Its analysis makes essential use of the theory of Schur polynomials  to show that the higher-moment error tensors are small given that the lower-order ones are.",
    "url": "https://proceedings.mlr.press/v247/diakonikolas24c.html",
    "id": "https://proceedings.mlr.press/v247/diakonikolas24c.html",
    "pdf": "https://proceedings.mlr.press/v247/diakonikolas24c/diakonikolas24c.pdf",
    "authors": {
      "0_Ilias Diakonikolas": "Ilias Diakonikolas",
      "1_Daniel M. Kane": "Daniel M. Kane"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/diakonikolas24c/diakonikolas24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1364-1378,\u00a02024.",
    "supplemental": ""
  },
  "119_Statistical Query Lower Bounds for Learning Truncated Gaussians": {
    "title": "Statistical Query Lower Bounds for Learning Truncated Gaussians",
    "abstract": "We study the problem of estimating the mean of an identity covariance Gaussian in the  truncated setting, in the regime when the truncation set comes from a low-complexity  family $\\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set  $S \\subseteq \\mathbb{R}^d$, we are given access to samples from the distribution  $\\mathcal{N}(\\bm{\\mu}, \\vec{I})$ truncated to the set $S$. The goal is to estimate $\\bm{\\mu}$  within accuracy $\\epsilon>0$ in $\\ell_2$-norm. Our main result is a Statistical Query (SQ)  lower bound suggesting a super-polynomial information-computation gap for this task. In  more detail, we show that the complexity of any SQ algorithm for this problem is  $d^{\\mathrm{poly}(1/\\epsilon)}$, even when the class $\\mathcal{C}$ is simple so that $\\mathrm{poly}(d/\\epsilon)$  samples information-theoretically suffice. Concretely, our SQ lower bound applies when  $\\mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension and Gaussian  surface are small. As a corollary of our construction, it also  follows that the complexity of the previously known algorithm for this task is qualitatively best possible.",
    "url": "https://proceedings.mlr.press/v247/diakonikolas24b.html",
    "id": "https://proceedings.mlr.press/v247/diakonikolas24b.html",
    "pdf": "https://proceedings.mlr.press/v247/diakonikolas24b/diakonikolas24b.pdf",
    "authors": {
      "0_Ilias Diakonikolas": "Ilias Diakonikolas",
      "1_Daniel M. Kane": "Daniel M. Kane",
      "2_Thanasis Pittas": "Thanasis Pittas",
      "3_Nikos Zarifis": "Nikos Zarifis"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/diakonikolas24b/diakonikolas24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1336-1363,\u00a02024.",
    "supplemental": ""
  },
  "120_Testable Learning of General Halfspaces with Adversarial Label Noise": {
    "title": "Testable Learning of General Halfspaces with Adversarial Label Noise",
    "abstract": "We study the task of testable learning of general \u2014 not necessarily homogeneous \u2014 halfspaces with adversarial label noise with respect to the Gaussian distribution. In the testable learning framework, the goal is to develop a tester-learner such that if the data passes the tester, then one can trust the output of the robust learner on the data. Our main result is the first polynomial time tester-learner for general halfspaces that achieves dimension-independent misclassification error. At the heart of our approach is a new methodology to reduce testable learning of general halfspaces to testable learning of \\snew{nearly} homogeneous halfspaces that may be of broader interest. ",
    "url": "https://proceedings.mlr.press/v247/diakonikolas24a.html",
    "id": "https://proceedings.mlr.press/v247/diakonikolas24a.html",
    "pdf": "https://proceedings.mlr.press/v247/diakonikolas24a/diakonikolas24a.pdf",
    "authors": {
      "0_Ilias Diakonikolas": "Ilias Diakonikolas",
      "1_Daniel Kane": "Daniel Kane",
      "2_Sihan Liu": "Sihan Liu",
      "3_Nikos Zarifis": "Nikos Zarifis"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/diakonikolas24a/diakonikolas24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1308-1335,\u00a02024.",
    "supplemental": ""
  },
  "121_Is Efficient PAC Learning Possible with an Oracle That Responds Yes or No": {
    "title": "Is Efficient PAC Learning Possible with an Oracle That Responds \"Yes\" or \"No\"?",
    "abstract": "The \\emph{empirical risk minimization (ERM)} principle has been highly impactful in machine learning, leading both to near-optimal theoretical guarantees for ERM-based learning algorithms as well as driving many of the recent empirical successes in deep learning. In this paper, we investigate  the question of whether the ability to perform ERM, which computes a hypothesis minimizing empirical risk on a given dataset, is necessary for efficient learning: in particular, is there a weaker oracle than ERM  which can nevertheless enable learnability? We answer this question affirmatively, showing that in the realizable setting of PAC learning for binary classification, a concept class can be learned using an oracle which only returns a \\emph{single bit} indicating whether a given dataset is realizable by some concept in the class. The sample complexity and oracle complexity of our algorithm depend polynomially on the VC dimension of the hypothesis class, thus showing that there is only a polynomial price to pay for use of our weaker oracle. Our results extend to the agnostic learning setting with a slight strengthening of the oracle, as well as to the partial concept, multiclass and real-valued learning settings. In the setting of partial concept classes, prior to our work no oracle-efficient algorithms were known, even with a standard ERM oracle. Thus, our results address a question of Alon et al. (2021) who asked whether there are algorithmic principles which enable efficient learnability in this setting.",
    "url": "https://proceedings.mlr.press/v247/daskalakis24a.html",
    "id": "https://proceedings.mlr.press/v247/daskalakis24a.html",
    "pdf": "https://proceedings.mlr.press/v247/daskalakis24a/daskalakis24a.pdf",
    "authors": {
      "0_Constantinos Daskalakis": "Constantinos Daskalakis",
      "1_Noah Golowich": "Noah Golowich"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/daskalakis24a/daskalakis24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1263-1307,\u00a02024.",
    "supplemental": ""
  },
  "122_ComputationalStatistical Gaps in Gaussian SingleIndex Models Extended Abstract": {
    "title": "Computational-Statistical Gaps in Gaussian Single-Index Models (Extended Abstract)",
    "abstract": "Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.  While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\\Omega(d^{k^\\star/2})$ samples, where $k^\\star$ is a \u201cgenerative\u201d exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace algorithm. Therefore, our results provide evidence of a sharp computational-to-statistical gap (under both the SQ and LDP class) whenever $k^\\star>2$. To complete the study, we construct smooth and Lipschitz deterministic target functions with arbitrarily large generative exponents $k^\\star$.",
    "url": "https://proceedings.mlr.press/v247/damian24a.html",
    "id": "https://proceedings.mlr.press/v247/damian24a.html",
    "pdf": "https://proceedings.mlr.press/v247/damian24a/damian24a.pdf",
    "authors": {
      "0_Alex Damian": "Alex Damian",
      "1_Loucas Pillaud-Vivien": "Loucas Pillaud-Vivien",
      "2_Jason Lee": "Jason Lee",
      "3_Joan Bruna": "Joan Bruna"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/damian24a/damian24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1262-1262,\u00a02024.",
    "supplemental": ""
  },
  "123_Refined Sample Complexity for Markov Games with Independent Linear Function Approximation Extended Abstract": {
    "title": "Refined Sample Complexity for Markov Games with Independent Linear Function Approximation (Extended Abstract)",
    "abstract": "Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the \u201ccurse of multi-agents\u201d (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023). While these works resolved the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\\max}$ \u2013 which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time. This paper first refines the AVLPR framework by Wang et al. (2023), with an insight of designing \\textit{data-dependent} (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent linear function approximations, we propose novel \\textit{action-dependent bonuses} to cover occasionally extreme estimation errors. With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\\text{poly}(A_{\\max})$ dependency simultaneously.",
    "url": "https://proceedings.mlr.press/v247/dai24a.html",
    "id": "https://proceedings.mlr.press/v247/dai24a.html",
    "pdf": "https://proceedings.mlr.press/v247/dai24a/dai24a.pdf",
    "authors": {
      "0_Yan Dai": "Yan Dai",
      "1_Qiwen Cui": "Qiwen Cui",
      "2_Simon S. Du": "Simon S. Du"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/dai24a/dai24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1260-1261,\u00a02024.",
    "supplemental": ""
  },
  "124_Learnability Gaps of Strategic Classification": {
    "title": "Learnability Gaps of Strategic Classification",
    "abstract": "In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards and bank accounts to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. Various settings, based on what and when information is known, have been explored in strategic classification. In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning.  We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation graph $G^\\star$) is known and during training time the learner has access to both the pre-manipulation data and post-manipulation data. We provide nearly tight sample complexity and regret bounds, offering significant improvements over prior results. Then, we relax the fully informative setting by introducing two natural types of uncertainty.  First, following Ahmadi et al. (2023), we consider the setting in which the learner only has access to the post-manipulation data. We improve the results of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower bound raised by them.  Our second relaxation of the fully informative setting introduces uncertainty to the manipulation structure. That is, we assume that the manipulation graph is unknown but belongs to a known class of graphs. We provide nearly tight bounds on the learning complexity in various unknown manipulation graph settings. Notably, our algorithm in this setting is of independent interest and can be applied to other problems such as multi-label learning.",
    "url": "https://proceedings.mlr.press/v247/cohen24c.html",
    "id": "https://proceedings.mlr.press/v247/cohen24c.html",
    "pdf": "https://proceedings.mlr.press/v247/cohen24c/cohen24c.pdf",
    "authors": {
      "0_Lee Cohen": "Lee Cohen",
      "1_Yishay Mansour": "Yishay Mansour",
      "2_Shay Moran": "Shay Moran",
      "3_Han Shao": "Han Shao"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/cohen24c/cohen24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1223-1259,\u00a02024.",
    "supplemental": ""
  },
  "125_Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries": {
    "title": "Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries",
    "abstract": "One of the most basic problems for studying the \u201cprice of privacy over time\u201d is the so called {\\em private counter problem}, introduced by Dwork et al. (2010) and Chan et al. (2011).  In this problem, we aim to track the number of {\\em events} that occur over time, while hiding the existence of every single event. More specifically, in every time step $t\\in[T]$ we learn (in an online fashion) that $\\Delta_t\\geq 0$ new events have occurred, and must respond with an estimate $n_t\\approx\\sum_{j=1}^t \\Delta_j$. The privacy requirement is that {\\em all of the outputs together}, across all time steps, satisfy {\\em event level} differential privacy.  The main question here is how our error needs to depend on the total number of time steps $T$ and the total number of events $n$. Dwork et al. (2015) showed an upper bound of $O\\left(\\log(T)+\\log^2(n)\\right)$, and Henzinger et al. (2023) showed a lower bound of $\\Omega\\left( \\min\\{\\log n, \\log T\\} \\right)$. We show a new lower bound of $\\Omega\\left(\\min\\{n,\\log T\\}\\right)$, which is tight w.r.t. the dependence on $T$, and is tight in the sparse case where $\\log^2 n=O(\\log T)$. Our lower bound has the following implications: \\begin{itemize} \\item We show that our lower bound extends to the {\\em online thresholds} problem, where the goal is to privately answer many \u201cquantile queries\u201d when these queries are presented one-by-one. This resolves an open question of Bun et al. (2017). \\item Our lower bound implies, for the first time, a separation between the number of mistakes obtainable by a private online learner and a non-private online learner. This partially resolves a COLT\u201922 open question published by Sanyal and Ramponi. \\item Our lower bound also yields the first separation between the standard model of private online learning and a recently proposed relaxed variant of it, called {\\em private online prediction}. \\end{itemize} ",
    "url": "https://proceedings.mlr.press/v247/cohen24b.html",
    "id": "https://proceedings.mlr.press/v247/cohen24b.html",
    "pdf": "https://proceedings.mlr.press/v247/cohen24b/cohen24b.pdf",
    "authors": {
      "0_Edith Cohen": "Edith Cohen",
      "1_Xin Lyu": "Xin Lyu",
      "2_Jelani Nelson": "Jelani Nelson",
      "3_Tam\u00e1s Sarl\u00f3s": "Tam\u00e1s Sarl\u00f3s",
      "4_Uri Stemmer": "Uri Stemmer"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/cohen24b/cohen24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1200-1222,\u00a02024.",
    "supplemental": ""
  },
  "126_Statistical curriculum learning An elimination algorithm achieving an oracle risk": {
    "title": "Statistical curriculum learning: An elimination algorithm achieving an oracle risk",
    "abstract": "We consider a statistical version of curriculum learning (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner. We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound. We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal.",
    "url": "https://proceedings.mlr.press/v247/cohen24a.html",
    "id": "https://proceedings.mlr.press/v247/cohen24a.html",
    "pdf": "https://proceedings.mlr.press/v247/cohen24a/cohen24a.pdf",
    "authors": {
      "0_Omer Cohen": "Omer Cohen",
      "1_Ron Meir": "Ron Meir",
      "2_Nir Weinberger": "Nir Weinberger"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/cohen24a/cohen24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1142-1199,\u00a02024.",
    "supplemental": ""
  },
  "127_RiskSensitive Online Algorithms Extended Abstract": {
    "title": "Risk-Sensitive Online Algorithms (Extended Abstract)",
    "abstract": "We study the design of risk-sensitive online algorithms, in which risk measures are used in the competitive analysis of randomized online algorithms. We introduce the CVaR$_\\delta$-competitive ratio ($\\delta$-CR) using the conditional value-at-risk of an algorithm\u2019s cost, which measures the expectation of the $(1-\\delta)$-fraction of worst outcomes against the offline optimal cost, and use this measure to study three online optimization problems: continuous-time ski rental, discrete-time ski rental, and one-max search. The structure of the optimal $\\delta$-CR and algorithm varies significantly between problems: we prove that the optimal $\\delta$-CR for continuous-time ski rental is $2-2^{-\\Theta(\\frac{1}{1-\\delta})}$, obtained by an algorithm described by a delay differential equation. In contrast, in discrete-time ski rental with buying cost $B$, there is an abrupt phase transition at $\\delta = 1 - \\Theta(\\frac{1}{\\log B})$, after which the classic deterministic strategy is optimal. Similarly, one-max search exhibits a phase transition at $\\delta = \\frac{1}{2}$, after which the classic deterministic strategy is optimal; we also obtain an algorithm that is asymptotically optimal as $\\delta \\todown 0$ that arises as the solution to a delay differential equation.",
    "url": "https://proceedings.mlr.press/v247/christianson24a.html",
    "id": "https://proceedings.mlr.press/v247/christianson24a.html",
    "pdf": "https://proceedings.mlr.press/v247/christianson24a/christianson24a.pdf",
    "authors": {
      "0_Nicolas Christianson": "Nicolas Christianson",
      "1_Bo Sun": "Bo Sun",
      "2_Steven Low": "Steven Low",
      "3_Adam Wierman": "Adam Wierman"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/christianson24a/christianson24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1140-1141,\u00a02024.",
    "supplemental": ""
  },
  "128_Undetectable Watermarks for Language Models": {
    "title": "Undetectable Watermarks for Language Models",
    "abstract": "Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.  Prior works have suggested methods of embedding watermarks in model outputs, by *noticeably* altering the output distribution. We ask: Is it possible to introduce a watermark without incurring *any detectable* change to the output distribution? To this end, we introduce a cryptographically-inspired notion of undetectable watermarks for language models.  That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.",
    "url": "https://proceedings.mlr.press/v247/christ24a.html",
    "id": "https://proceedings.mlr.press/v247/christ24a.html",
    "pdf": "https://proceedings.mlr.press/v247/christ24a/christ24a.pdf",
    "authors": {
      "0_Miranda Christ": "Miranda Christ",
      "1_Sam Gunn": "Sam Gunn",
      "2_Or Zamir": "Or Zamir"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/christ24a/christ24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1125-1139,\u00a02024.",
    "supplemental": ""
  },
  "129_The power of an adversary in Glauber dynamics": {
    "title": "The power of an adversary in Glauber dynamics",
    "abstract": "Glauber dynamics are a natural model of dynamics of dependent systems. While originally introduced in statistical physics, they have found important applications in the study of social networks, computer vision and other domains.  In this work, we introduce a model of corrupted Glauber dynamics whereby instead of updating according to the prescribed conditional probabilities, some of the vertices and their updates are controlled by an adversary. We study the effect of such corruptions on global features of the system. Among the questions we study are: How many nodes need to be controlled in order to change the average statistics of the system in polynomial time? And how many nodes are needed to obstruct approximate convergence of the dynamics? Given a specific budget, how can the adversary choose nodes to control to maximize the overall effect? Our results can be viewed as studying the robustness of classical sampling methods and are thus related to robust inference. The proofs connect to classical theory of Glauber dynamics from statistical physics. ",
    "url": "https://proceedings.mlr.press/v247/chin24a.html",
    "id": "https://proceedings.mlr.press/v247/chin24a.html",
    "pdf": "https://proceedings.mlr.press/v247/chin24a/chin24a.pdf",
    "authors": {
      "0_Byron Chin": "Byron Chin",
      "1_Ankur Moitra": "Ankur Moitra",
      "2_Elchanan Mossel": "Elchanan Mossel",
      "3_Colin Sandon": "Colin Sandon"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chin24a/chin24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1102-1124,\u00a02024.",
    "supplemental": ""
  },
  "130_New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions": {
    "title": "New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions",
    "abstract": " We develop a new technique for proving distribution testing lower bounds for properties defined by inequalities on the individual bin probabilities (such as monotonicity and log-concavity). The basic idea is to find a base distribution $Q$ where these inequalities barely hold in many places. We then find two different ensembles of distributions that modify $Q$ in slightly different ways. We use a moment matching construction so that each ensemble has the same bin moments (in particular the expectation over the choice of distribution $p$ of $p_{i}^t$ is the same for the two ensembles for small integers $t$). We show that this makes it impossible to distinguish between the two ensembles with a small number of samples. On the other hand, we construct them so that one ensemble will tweak Q in such a way that it may violate the defining inequalities of the property in question in many places, while the second ensembles does not. Since any valid tester for this property must be able to reliably distinguish these ensembles, we obtain a lower bound of testing the property. Roughly speaking, if we can construct Q which nearly violates the defining inequalities in n places and if the desired error $\\epilon$ is small enough relative to n, we hope to obtain a lower bound of roughly $\\frac{n}{\\epsilon^2}$ up to log factors. In particular, we obtain a lower bound of $\\Omega( \\min(n,(1/\\epsilon)/ \\log^3(1/\\epsilon))\\allowbreak / ( \\epsilon^2 \\log^7(1/\\epsilon)))$  for monotonicity testing on $[n]$ and $\\Omega(\\log^{-7}(1/\\epsilon) \\epsilon^{-2} \\min(n,\\epsilon^{-1/2}\\log^{-3/2}(1/\\epsilon)))$ for log-concavity testing on $[n]$, the latter of which matches known upper bounds to within logarithmic factors. More generally, for monotonicity testing on $[n]^d$, we have the lower bound of $2^{-O(d)}d^{-d} \\epsilon^{-2} \\log^{-7}(1/\\epsilon) \\min(n,d \\epsilon^{-1} \\log^{-3}(1/\\epsilon))^d$.",
    "url": "https://proceedings.mlr.press/v247/cheng24a.html",
    "id": "https://proceedings.mlr.press/v247/cheng24a.html",
    "pdf": "https://proceedings.mlr.press/v247/cheng24a/cheng24a.pdf",
    "authors": {
      "0_Yuqian Cheng": "Yuqian Cheng",
      "1_Daniel Kane": "Daniel Kane",
      "2_Zheng Zhicheng": "Zheng Zhicheng"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/cheng24a/cheng24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:2768-2794,\u00a02024.",
    "supplemental": ""
  },
  "131_Open Problem BlackBox Reductions and Adaptive Gradient Methods for Nonconvex Optimization": {
    "title": "Open Problem: Black-Box Reductions and Adaptive Gradient Methods for Nonconvex Optimization",
    "abstract": "We describe an open problem: reduce offline nonconvex stochastic optimization to regret minimization in online convex optimization. The conjectured reduction aims to make progress on explaining the success of adaptive gradient methods for deep learning. A prize of 500 dollars is offered to the winner.",
    "url": "https://proceedings.mlr.press/v247/chen24e.html",
    "id": "https://proceedings.mlr.press/v247/chen24e.html",
    "pdf": "https://proceedings.mlr.press/v247/chen24e/chen24e.pdf",
    "authors": {
      "0_Xinyi Chen": "Xinyi Chen",
      "1_Elad Hazan": "Elad Hazan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chen24e/chen24e.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5317-5324,\u00a02024.",
    "supplemental": ""
  },
  "132_Scalefree Adversarial Reinforcement Learning": {
    "title": "Scale-free Adversarial Reinforcement Learning",
    "abstract": "This paper initiates the study of scale-free learning in Markov Decision Processes (MDPs), where the scale of rewards/losses is unknown to the learner. We design a generic algorithmic framework, \\underline{S}cale \\underline{C}lipping \\underline{B}ound (\\texttt{SCB}), and instantiate this framework in both the adversarial Multi-armed Bandit (MAB) setting and the adversarial MDP setting. Through this framework, we achieve the first minimax optimal expected regret bound and the first high-probability regret bound in scale-free adversarial MABs, resolving an open problem raised in \\cite{hadiji2020adaptation}. On adversarial MDPs, our framework also give birth to the first scale-free RL algorithm with a $\\tilde{\\mathcal{O}}(\\sqrt{T})$ high-probability regret guarantee.",
    "url": "https://proceedings.mlr.press/v247/chen24d.html",
    "id": "https://proceedings.mlr.press/v247/chen24d.html",
    "pdf": "https://proceedings.mlr.press/v247/chen24d/chen24d.pdf",
    "authors": {
      "0_Mingyu Chen": "Mingyu Chen",
      "1_Xuezhou Zhang": "Xuezhou Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chen24d/chen24d.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1068-1101,\u00a02024.",
    "supplemental": ""
  },
  "133_NearOptimal Learning and Planning in Separated Latent MDPs": {
    "title": "Near-Optimal Learning and Planning in Separated Latent MDPs",
    "abstract": "We study computational and statistical aspects of learning  Latent Markov Decision Processes (LMDPs). In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs. To sidestep known impossibility results, we consider several notions of $\\delta$-separation of the constituent MDPs. The main thrust of this paper is in establishing a nearly-sharp \\textit{statistical threshold} for the horizon length necessary for efficient learning. On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold. We further show a near-matching time complexity lower bound under the exponential time hypothesis.",
    "url": "https://proceedings.mlr.press/v247/chen24c.html",
    "id": "https://proceedings.mlr.press/v247/chen24c.html",
    "pdf": "https://proceedings.mlr.press/v247/chen24c/chen24c.pdf",
    "authors": {
      "0_Fan Chen": "Fan Chen",
      "1_Constantinos Daskalakis": "Constantinos Daskalakis",
      "2_Noah Golowich": "Noah Golowich",
      "3_Alexander Rakhlin": "Alexander Rakhlin"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chen24c/chen24c.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:995-1067,\u00a02024.",
    "supplemental": ""
  },
  "134_A faster and simpler algorithm for learning shallow networks": {
    "title": "A faster and simpler algorithm for learning shallow networks",
    "abstract": "We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. recently gave the first algorithm for this problem to run in $\\mathrm{poly}(d,1/\\epsilon)$ time when $k = O(1)$, where $\\epsilon$ is the target error. More precisely, their algorithm runs in time $(d/\\epsilon)^{\\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d k/\\epsilon)^{O(k^2)}$.",
    "url": "https://proceedings.mlr.press/v247/chen24b.html",
    "id": "https://proceedings.mlr.press/v247/chen24b.html",
    "pdf": "https://proceedings.mlr.press/v247/chen24b/chen24b.pdf",
    "authors": {
      "0_Sitan Chen": "Sitan Chen",
      "1_Shyam Narayanan": "Shyam Narayanan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chen24b/chen24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:981-994,\u00a02024.",
    "supplemental": ""
  },
  "135_On Finding Small HyperGradients in Bilevel Optimization Hardness Results and Improved Analysis": {
    "title": "On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness Results and Improved Analysis",
    "abstract": "Bilevel optimization reveals the inner structure of otherwise oblique optimization problems, such as hyperparameter tuning, neural architecture search, and meta-learning.  A common goal in bilevel optimization is to minimize a hyper-objective that implicitly depends on the solution set of the lower-level function. Although this hyper-objective approach is widely used, its theoretical properties have not been thoroughly investigated in cases where \\textit{the lower-level functions lack strong convexity}.  In this work, we first provide hardness results to show that the goal of finding stationary points of the hyper-objective for nonconvex-convex bilevel optimization can be intractable for zero-respecting algorithms. Then we study a class of tractable nonconvex-nonconvex bilevel problems when the lower-level function satisfies the Polyak-\u0141{}ojasiewicz (PL) condition. We show a simple first-order algorithm can achieve complexity bounds of $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ and $\\tilde{\\mathcal{O}}(\\epsilon^{-6})$ in the deterministic, partially stochastic, and fully stochastic setting respectively.",
    "url": "https://proceedings.mlr.press/v247/chen24a.html",
    "id": "https://proceedings.mlr.press/v247/chen24a.html",
    "pdf": "https://proceedings.mlr.press/v247/chen24a/chen24a.pdf",
    "authors": {
      "0_Lesi Chen": "Lesi Chen",
      "1_Jing Xu": "Jing Xu",
      "2_Jingzhao Zhang": "Jingzhao Zhang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chen24a/chen24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:947-980,\u00a02024.",
    "supplemental": ""
  },
  "136_Dual VC Dimension Obstructs Sample Compression by Embeddings": {
    "title": "Dual VC Dimension Obstructs Sample Compression by Embeddings",
    "abstract": "This work studies embedding of arbitrary VC classes in well-behaved VC classes, focusing particularly on extremal classes. Our main result expresses an impossibility: such embeddings necessarily require a significant increase in dimension. In particular, we prove that for every $d$ there is a class with VC dimension $d$ that cannot be embedded in any extremal class of VC dimension smaller than exponential in $d$. In addition to its independent interest, this result has an important implication in learning theory, as it reveals a fundamental limitation of one of the most extensively studied approaches to tackling the long-standing sample compression conjecture. Concretely, the approach proposed by Floyd and Warmuth entails embedding any given VC class into an extremal class of a comparable dimension, and then applying an optimal sample compression scheme for extremal classes. However, our results imply that this strategy would in some cases result in a sample compression scheme at least exponentially larger than what is predicted by the sample compression conjecture. The above implications follow from a general result we prove: any extremal class with VC dimension $d$ has dual VC dimension at most $2d+1$. This bound is exponentially smaller than the classical bound $2^{d+1}-1$ of Assouad, which applies to general concept classes (and is known to be unimprovable for some classes). We in fact prove a stronger result, establishing that $2d+1$ upper bounds the dual Radon number of extremal classes. This theorem represents an abstraction of the classical Radon theorem for convex sets, extending its applicability to a wider combinatorial framework, without relying on the specifics of Euclidean convexity. The proof utilizes the topological method and is primarily based on variants of the Topological Radon Theorem.",
    "url": "https://proceedings.mlr.press/v247/chase24a.html",
    "id": "https://proceedings.mlr.press/v247/chase24a.html",
    "pdf": "https://proceedings.mlr.press/v247/chase24a/chase24a.pdf",
    "authors": {
      "0_Zachary Chase": "Zachary Chase",
      "1_Bogdan Chornomaz": "Bogdan Chornomaz",
      "2_Steve Hanneke": "Steve Hanneke",
      "3_Shay Moran": "Shay Moran",
      "4_Amir Yehudayoff": "Amir Yehudayoff"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chase24a/chase24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:923-946,\u00a02024.",
    "supplemental": ""
  },
  "137_Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension": {
    "title": "Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension",
    "abstract": "In the well-studied agnostic model of learning, the goal of a learner\u2013 given examples from an arbitrary joint distribution on $\\mathbb{R}^d \\times \\{\\pm 1\\}$\u2013 is to output a hypothesis that is competitive (to within $\\epsilon$) of the best fitting concept from some class.  In order to escape strong hardness results for learning even simple concept classes in this model, we introduce a smoothed analysis framework where we require a learner to compete only with the best classifier that is robust to small random Gaussian perturbation. This subtle change allows us to give a wide array of learning results for any concept that (1) depends on a low-dimensional subspace (aka multi-index model) and (2) has a bounded Gaussian surface area.  This class includes functions of halfspaces and (low-dimensional) convex sets, cases that are only known to be learnable in non-smoothed settings with respect to highly structured distributions such as Gaussians. Perhaps surprisingly, our analysis also yields new results for traditional non-smoothed frameworks such as learning with margin.  In particular, we obtain the first algorithm for agnostically learning intersections of $k$-halfspaces in time  $k^{\\poly(\\frac{\\log k}{\\epsilon \\gamma}) }$ where $\\gamma$ is the margin parameter.   Before our work, the best-known runtime was exponential in $k$ (Arriaga and Vempala, 1999). ",
    "url": "https://proceedings.mlr.press/v247/chandrasekaran24a.html",
    "id": "https://proceedings.mlr.press/v247/chandrasekaran24a.html",
    "pdf": "https://proceedings.mlr.press/v247/chandrasekaran24a/chandrasekaran24a.pdf",
    "authors": {
      "0_Gautam Chandrasekaran": "Gautam Chandrasekaran",
      "1_Adam Klivans": "Adam Klivans",
      "2_Vasilis Kontonis": "Vasilis Kontonis",
      "3_Raghu Meka": "Raghu Meka",
      "4_Konstantinos Stavropoulos": "Konstantinos Stavropoulos"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chandrasekaran24a/chandrasekaran24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:876-922,\u00a02024.",
    "supplemental": ""
  },
  "138_NonClashing Teaching Maps for Balls in Graphs": {
    "title": "Non-Clashing Teaching Maps for Balls in Graphs",
    "abstract": "Recently, Kirkpatrick et al.\u00a0[ALT 2019] and  Fallat et al.\u00a0[JMLR 2023] introduced non-clashing teaching and showed it to be the most efficient machine teaching model satisfying the benchmark for collusion-avoidance set by Goldman and Mathias. A teaching map $T$ for a concept class $\\mathcal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \\in \\mathcal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a teaching set $T(C)$, $C \\in \\mathcal{C}$. The non-clashing teaching dimension $\\text{NCTD}(\\mathcal{C})$ of $\\mathcal{C}$ is the minimum size of an NCTM for $\\mathcal{C}$. $\\text{NCTM}^+$ and $\\text{NCTD}^+(\\mathcal{C})$ are defined analogously, except the teacher may only use positive examples.\nWe study NCTMs and $\\text{NCTM}^+\\text{s}$ for the concept class $\\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem $\\text{B-NCTD}^+$ for $\\text{NCTD}^+$ is NP-complete in split, co-bipartite, and bipartite graphs. Surprisingly, we even prove that, unless the ETH fails, $\\text{B-NCTD}^+$ does not admit an algorithm running in time $2^{2^{o(\\mathtt{vc})}}\\cdot n^{\\mathcal{O}(1)}$, nor a kernelization algorithm outputting a kernel with $2^{o(\\mathtt{vc})}$ vertices, where $\\mathtt{vc}$ is the vertex cover number of $G$. We complement these lower bounds with matching upper bounds. These are extremely rare results: it is only the second problem in NP to admit such a tight double-exponential lower bound parameterized by $\\mathtt{vc}$, and only one of very few problems to admit such an ETH-based conditional lower bound on the number of vertices in a kernel. For trees, interval graphs, cycles, and trees of cycles, we derive $\\text{NCTM}^+\\text{s}$ or NCTMs for $\\mathcal{B}(G)$ of size proportional to its VC-dimension. For Gromov-hyperbolic graphs, we design an approximate $\\text{NCTM}^+$ for $\\mathcal{B}(G)$ of size $2$, in which only pairs of balls with Hausdorff distance larger than some constant must satisfy the non-clashing condition.",
    "url": "https://proceedings.mlr.press/v247/chalopin24a.html",
    "id": "https://proceedings.mlr.press/v247/chalopin24a.html",
    "pdf": "https://proceedings.mlr.press/v247/chalopin24a/chalopin24a.pdf",
    "authors": {
      "0_J\u00e9r\u00e9mie Chalopin": "J\u00e9r\u00e9mie Chalopin",
      "1_Victor Chepoi": "Victor Chepoi",
      "2_Fionn Mc\u00a0Inerney": "Fionn Mc\u00a0Inerney",
      "3_S\u00e9bastien Ratel": "S\u00e9bastien Ratel"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/chalopin24a/chalopin24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:840-875,\u00a02024.",
    "supplemental": ""
  },
  "139_Informationtheoretic generalization bounds for learning from quantum data": {
    "title": "Information-theoretic generalization bounds for learning from quantum data",
    "abstract": "Learning tasks play an increasingly prominent role in quantum information and computation. They range from fundamental problems such as state discrimination and metrology over the framework of quantum probably approximately correct (PAC) learning, to the recently proposed shadow variants of state tomography. However, the many directions of quantum learning theory have so far evolved separately. We propose a mathematical formalism for describing quantum learning by training on classical-quantum data and then testing how well the learned hypothesis generalizes to new data. In this framework, we prove bounds on the expected generalization error of a quantum learner in terms of classical and quantum information-theoretic quantities measuring how strongly the learner\u2019s hypothesis depends on the data seen during training. To achieve this, we use tools from quantum optimal transport and quantum concentration inequalities to establish non-commutative versions of decoupling lemmas that underlie classical information-theoretic generalization bounds. Our framework encompasses and gives intuitive generalization bounds for a variety of quantum learning scenarios such as quantum state discrimination, PAC learning quantum states, quantum parameter estimation, and quantumly PAC learning classical functions. Thereby, our work lays a foundation for a unifying quantum information-theoretic perspective on quantum learning.",
    "url": "https://proceedings.mlr.press/v247/caro24a.html",
    "id": "https://proceedings.mlr.press/v247/caro24a.html",
    "pdf": "https://proceedings.mlr.press/v247/caro24a/caro24a.pdf",
    "authors": {
      "0_Matthias C. Caro": "Matthias C. Caro",
      "1_Tom Gur": "Tom Gur",
      "2_Cambyse Rouz\u00e9": "Cambyse Rouz\u00e9",
      "3_Daniel Stilck Fran\u00e7a": "Daniel Stilck Fran\u00e7a",
      "4_Sathyawageeswar Subramanian": "Sathyawageeswar Subramanian"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/caro24a/caro24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:775-839,\u00a02024.",
    "supplemental": ""
  },
  "140_The Price of Adaptivity in Stochastic Convex Optimization": {
    "title": "The Price of Adaptivity in Stochastic Convex Optimization",
    "abstract": "We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a \u201cprice of adaptivity\u201d (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.",
    "url": "https://proceedings.mlr.press/v247/carmon24a.html",
    "id": "https://proceedings.mlr.press/v247/carmon24a.html",
    "pdf": "https://proceedings.mlr.press/v247/carmon24a/carmon24a.pdf",
    "authors": {
      "0_Yair Carmon": "Yair Carmon",
      "1_Oliver Hinder": "Oliver Hinder"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/carmon24a/carmon24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:772-774,\u00a02024.",
    "supplemental": ""
  },
  "141_Open Problem Tight Characterization of InstanceOptimal Identity Testing": {
    "title": "Open Problem: Tight Characterization of Instance-Optimal Identity Testing",
    "abstract": "In the \u201cinstance-optimal\u201d identity testing introduced by Valiant and Valiant (2014), one is given the (succinct) description of a discrete probability distribution $q$, as well as a a parameter $\\varepsilon\\in(0,1]$ and i.i.d. samples from an (unknown, arbitrary) discrete distribution $p$. The goal is to distinguish with high probability between the cases (i)\u00a0$p=q$ and (ii)\u00a0$\\textrm{TV}(p,q) > \\varepsilon$, using the minimum number of samples possible as a function of (some simple functional of) $q$ and $\\varepsilon$. This is in contrast with the standard formulation of identity testing, where the sample complexity is taken as worst-case over all possible reference distributions $q$. Valiant and Valiant provided upper and lower bounds on this question, where the sample complexity is expressed in terms of the \u201c$\\ell_{2/3}$ norm\u201d of some (truncated version) of the reference distribution $q$. However, these upper and lower bounds do not always match up to constant factors, and can differ by an arbitrary multiplicative gap for some choices of $q$. The question then is: what is the tight characterization of the sample complexity of instance-optimal identity testing? What is the \u201cright\u201d functional $\\Phi(q)$ for it? ",
    "url": "https://proceedings.mlr.press/v247/canonne24a.html",
    "id": "https://proceedings.mlr.press/v247/canonne24a.html",
    "pdf": "https://proceedings.mlr.press/v247/canonne24a/canonne24a.pdf",
    "authors": {
      "0_Cl\u00e9ment Canonne": "Cl\u00e9ment Canonne"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/canonne24a/canonne24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5312-5316,\u00a02024.",
    "supplemental": ""
  },
  "142_ComputationalStatistical Gaps for Improper Learning in Sparse Linear Regression": {
    "title": "Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression",
    "abstract": "We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\\Theta(k \\log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms. We give evidence that efficient algorithms for this task require at least (roughly) $\\Omega(k^2)$ samples. In particular, we show that an improper learning algorithm for sparse linear regression can be used to solve sparse PCA problems (with a negative spike) in their Wishart form, in regimes in which efficient algorithms are widely believed to require at least $\\Omega(k^2)$ samples. We complement our reduction with low-degree and statistical query lower bounds for the sparse PCA problems from which we reduce. Our hardness results apply to the (correlated) random design setting in which the covariates are drawn i.i.d. from a mean-zero Gaussian distribution with unknown covariance.",
    "url": "https://proceedings.mlr.press/v247/buhai24a.html",
    "id": "https://proceedings.mlr.press/v247/buhai24a.html",
    "pdf": "https://proceedings.mlr.press/v247/buhai24a/buhai24a.pdf",
    "authors": {
      "0_Rares-Darius Buhai": "Rares-Darius Buhai",
      "1_Jingqiu Ding": "Jingqiu Ding",
      "2_Stefan Tiegel": "Stefan Tiegel"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/buhai24a/buhai24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:752-771,\u00a02024.",
    "supplemental": ""
  },
  "143_Insufficient Statistics Perturbation Stable Estimators for Private Least Squares Extended Abstract": {
    "title": "Insufficient Statistics Perturbation: Stable Estimators for Private Least Squares Extended Abstract",
    "abstract": "We present a sample- and time-efficient differentially private algorithm for ordinary least squares, with error that depends linearly on the dimension and is independent of the condition number of $X^\\top X$, where $X$ is the design matrix. All prior private algorithms for this task require either $d^{3/2}$ examples, error growing polynomially with the condition number, or exponential time. Our near-optimal accuracy guarantee holds for any dataset with bounded statistical leverage and bounded residuals. Technically, we build on the approach of Brown et al. (2023) for private mean estimation, adding scaled noise to a carefully designed stable nonprivate estimator of the empirical regression vector.",
    "url": "https://proceedings.mlr.press/v247/brown24b.html",
    "id": "https://proceedings.mlr.press/v247/brown24b.html",
    "pdf": "https://proceedings.mlr.press/v247/brown24b/brown24b.pdf",
    "authors": {
      "0_Gavin Brown": "Gavin Brown",
      "1_Jonathan Hayase": "Jonathan Hayase",
      "2_Samuel Hopkins": "Samuel Hopkins",
      "3_Weihao Kong": "Weihao Kong",
      "4_Xiyang Liu": "Xiyang Liu",
      "5_Sewoong Oh": "Sewoong Oh",
      "6_Juan C Perdomo": "Juan C Perdomo",
      "7_Adam Smith": "Adam Smith"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/brown24b/brown24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:750-751,\u00a02024.",
    "supplemental": ""
  },
  "144_Online Stackelberg Optimization via Nonlinear Control": {
    "title": "Online Stackelberg Optimization via Nonlinear Control",
    "abstract": "In repeated interaction problems with adaptive agents, our objective often requires anticipating and optimizing over the space of possible agent responses. We show that many problems of this form can be cast as instances of online (nonlinear) control which satisfy \\textit{local controllability}, with convex losses over a bounded state space which encodes agent behavior, and we introduce a unified algorithmic framework for tractable regret minimization in such cases. When the instance dynamics are known but otherwise arbitrary, we obtain oracle-efficient $O(\\sqrt{T})$ regret by reduction to online convex optimization, which can be made computationally efficient if dynamics are locally \\textit{action-linear}. In the presence of adversarial disturbances to the state, we give tight bounds in terms of either the cumulative or per-round disturbance magnitude (for \\textit{strongly} or \\textit{weakly} locally controllable dynamics, respectively). Additionally, we give sublinear regret results for the cases of unknown locally action-linear dynamics as well as for  the bandit feedback setting. Finally, we demonstrate applications of our framework to well-studied problems including performative prediction, recommendations for adaptive agents, adaptive pricing of real-valued goods, and repeated gameplay against no-regret learners, directly yielding  extensions beyond prior results in each case.",
    "url": "https://proceedings.mlr.press/v247/brown24a.html",
    "id": "https://proceedings.mlr.press/v247/brown24a.html",
    "pdf": "https://proceedings.mlr.press/v247/brown24a/brown24a.pdf",
    "authors": {
      "0_William Brown": "William Brown",
      "1_Christos Papadimitriou": "Christos Papadimitriou",
      "2_Tim Roughgarden": "Tim Roughgarden"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/brown24a/brown24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:697-749,\u00a02024.",
    "supplemental": ""
  },
  "145_Efficient Algorithms for Learning Monophonic Halfspaces in Graphs": {
    "title": "Efficient Algorithms for Learning Monophonic Halfspaces in Graphs",
    "abstract": "We study the problem of learning a binary classifier on the vertices of a graph. In particular, we consider classifiers given by \\emph{monophonic halfspaces}, partitions of the vertices that are convex in a certain abstract sense. Monophonic halfspaces, and related notions such as geodesic halfspaces, have recently attracted interest, and several connections have been drawn between their properties (e.g., their VC dimension) and the structure of the underlying graph $G$. We prove several novel results for learning monophonic halfspaces in the supervised, online, and active settings. Our main result is that a monophonic halfspace can be learned with near-optimal passive sample complexity in time polynomial in $n=|V(G)|$. This requires us to devise a polynomial-time algorithm for consistent hypothesis checking, based on several structural insights on monophonic halfspaces and on a reduction to 2-satisfiability. We prove similar results for the online and active settings. We also show that the concept class can be enumerated with delay $\\mathrm{poly}(n)$, and that empirical risk minimization can be performed in time $2^{\\omega(G)}\\mathrm{poly}(n)$ where $\\omega(G)$ is the clique number of $G$. These results answer open questions from the literature  (Gonz\u00e1lez et al. 2020), and show a contrast with geodesic halfspaces, for which some of the said problems are NP-hard (Seiffarth et al., 2023).",
    "url": "https://proceedings.mlr.press/v247/bressan24b.html",
    "id": "https://proceedings.mlr.press/v247/bressan24b.html",
    "pdf": "https://proceedings.mlr.press/v247/bressan24b/bressan24b.pdf",
    "authors": {
      "0_Marco Bressan": "Marco Bressan",
      "1_Emmanuel Esposito": "Emmanuel Esposito",
      "2_Maximilian Thiessen": "Maximilian Thiessen"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/bressan24b/bressan24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:669-696,\u00a02024.",
    "supplemental": ""
  },
  "146_A Theory of Interpretable Approximations": {
    "title": "A Theory of Interpretable Approximations",
    "abstract": "Can a deep neural network be approximated by a small decision tree based on simple features? This question and its variants are behind the growing demand for machine learning models that are \\emph{interpretable} by humans. In this work we study such questions by introducing \\emph{interpretable approximations}, a notion that captures the idea of approximating a target concept $c$ by a small aggregation of concepts from some base class $\\mathcal{H}$. In particular, we consider the approximation of a binary concept $c$ by decision trees based on a simple class $\\mathcal{H}$ (e.g., of bounded VC dimension), and use the tree depth as a measure of complexity. Our primary contribution is the following remarkable trichotomy. For any given pair of $\\mathcal{H}$ and $c$, exactly one of these cases holds: (i) $c$ cannot be approximated by $\\mathcal{H}$ with arbitrary accuracy; (ii) $c$ can be approximated by $\\mathcal{H}$ with arbitrary accuracy, but there exists no universal rate that bounds the complexity of the approximations as a function of the accuracy; or (iii) there exists a constant $\\kappa$ that depends only on $\\mathcal{H}$ and $c$ such that, for \\emph{any} data distribution and \\emph{any} desired accuracy level, $c$ can be approximated by $\\mathcal{H}$ with a complexity not exceeding $\\kappa$. This taxonomy stands in stark contrast to the landscape of supervised classification, which offers a complex array of distribution-free and universally learnable scenarios. We show that, in the case of interpretable approximations, even a slightly nontrivial a-priori guarantee on the complexity of approximations implies approximations with constant (distribution-free and accuracy-free) complexity. We extend our trichotomy to classes $\\mathcal{H}$ of unbounded VC dimension and give characterizations of interpretability based on the algebra generated by $\\mathcal{H}$.",
    "url": "https://proceedings.mlr.press/v247/bressan24a.html",
    "id": "https://proceedings.mlr.press/v247/bressan24a.html",
    "pdf": "https://proceedings.mlr.press/v247/bressan24a/bressan24a.pdf",
    "authors": {
      "0_Marco Bressan": "Marco Bressan",
      "1_Nicol\u00f2 Cesa-Bianchi": "Nicol\u00f2 Cesa-Bianchi",
      "2_Emmanuel Esposito": "Emmanuel Esposito",
      "3_Yishay Mansour": "Yishay Mansour",
      "4_Shay Moran": "Shay Moran",
      "5_Maximilian Thiessen": "Maximilian Thiessen"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/bressan24a/bressan24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:648-668,\u00a02024.",
    "supplemental": ""
  },
  "147_Thresholds for Reconstruction of Random Hypergraphs From Graph Projections": {
    "title": "Thresholds for Reconstruction of Random Hypergraphs From Graph Projections",
    "abstract": "The graph projection of a hypergraph is a simple graph with the same vertex set and with an edge between each pair of vertices that appear in a hyperedge. We consider the problem of reconstructing a random $d$-uniform hypergraph from its projection. Feasibility of this task depends on $d$ and the density of hyperedges in the random hypergraph. For $d=3$ we precisely determine the threshold, while for $d\\ge 4$ we give bounds. All of our feasibility results are obtained by exhibiting an efficient algorithm for reconstructing the original hypergraph, while infeasibility is information-theoretic. \n\nOur results also apply to mildly inhomogeneous random hypergrahps, including hypergraph stochastic block models (HSBM). A consequence of our results is an optimal HSBM recovery algorithm, improving on Gaudio and Joshi (2023a).\n",
    "url": "https://proceedings.mlr.press/v247/bresler24a.html",
    "id": "https://proceedings.mlr.press/v247/bresler24a.html",
    "pdf": "https://proceedings.mlr.press/v247/bresler24a/bresler24a.pdf",
    "authors": {
      "0_Guy Bresler": "Guy Bresler",
      "1_Chenghao Guo": "Chenghao Guo",
      "2_Yury Polyanskiy": "Yury Polyanskiy"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/bresler24a/bresler24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:632-647,\u00a02024.",
    "supplemental": ""
  },
  "148_Errors are Robustly Tamed in Cumulative Knowledge Processes": {
    "title": "Errors are Robustly Tamed in Cumulative Knowledge Processes",
    "abstract": "We study processes of societal knowledge accumulation, where the validity of a new unit of knowledge depends both on the correctness of its derivation and on the validity of the units it depends on. A fundamental question in this setting is: If a constant fraction of the new derivations is wrong, can investing a constant fraction, bounded away from one, of effort ensure that a constant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer, Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to analyze such questions and showed an affirmative answer to this question. Their study, however, focuses on the simple case where each new unit depends on just one existing unit, and units attach according to a {\\em preferential attachment rule}.  In this work, we consider much more general families of cumulative knowledge processes, where new units may attach according to varied attachment mechanisms and depend on multiple existing units. We also allow a (random) fraction of insertions of adversarial nodes.  We give a robust affirmative answer to the above question by showing that for \\textit{all} of these models, as long as many of the units follow simple heuristics for checking a bounded number of units they depend on, all errors will be eventually eliminated. Our results indicate that preserving the quality of large interdependent collections of units of knowledge is feasible, as long as careful but not too costly checks are performed when new units are derived/deposited.",
    "url": "https://proceedings.mlr.press/v247/brandenberger24a.html",
    "id": "https://proceedings.mlr.press/v247/brandenberger24a.html",
    "pdf": "https://proceedings.mlr.press/v247/brandenberger24a/brandenberger24a.pdf",
    "authors": {
      "0_Anna Brandenberger": "Anna Brandenberger",
      "1_Cassandra Marcussen": "Cassandra Marcussen",
      "2_Elchanan Mossel": "Elchanan Mossel",
      "3_Madhu Sudan": "Madhu Sudan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/brandenberger24a/brandenberger24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:630-631,\u00a02024.",
    "supplemental": ""
  },
  "149_On the Performance of Empirical Risk Minimization with Smoothed Data": {
    "title": "On the Performance of Empirical Risk Minimization with Smoothed Data",
    "abstract": "In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history.   While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases.  This work investigates the more general setting where the base measure is \\emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with square loss when the data are well-specified and smooth.   We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\\tilde O( \\sqrt{\\mathrm{comp}(\\mathcal F) \\cdot T} )$, where $\\mathrm{comp}(\\mathcal{F})$ is the statistical complexity of learning $\\mathcal F$ with iid data.   In so doing, we prove a novel norm comparison bound for smoothed data that comprises the first sharp norm comparison for dependent data applying to arbitrary, nonlinear function classes. We complement these results with a lower bound indicating that our analysis of ERM is essentially tight, establishing a separation in the performance of ERM between smoothed and iid data.",
    "url": "https://proceedings.mlr.press/v247/block24a.html",
    "id": "https://proceedings.mlr.press/v247/block24a.html",
    "pdf": "https://proceedings.mlr.press/v247/block24a/block24a.pdf",
    "authors": {
      "0_Adam Block": "Adam Block",
      "1_Alexander Rakhlin": "Alexander Rakhlin",
      "2_Abhishek Shetty": "Abhishek Shetty"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/block24a/block24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:596-629,\u00a02024.",
    "supplemental": ""
  },
  "150_Correlated Binomial Process": {
    "title": "Correlated Binomial Process",
    "abstract": "Cohen and Kontorovich (COLT 2023) initiated the study of what we call here the Binomial Empirical Process: the maximal empirical mean deviation for sequences of binary random variables (up to rescaling, the empirical mean of each entry of the random sequence is a binomial hence the naming). They almost fully analyzed the case where the binomials are independent, which corresponds to all random variable entries from the sequence being independent. The remaining gap was closed by Blanchard and Vor\u00e1\u010dek (ALT 2024). In this work, we study the much more general and challenging case with correlations. In contradistinction to Gaussian processes, whose behavior is characterized by the covariance structure, we discover that, at least  somewhat surprisingly, for binomial processes covariance does not even characterize convergence. Although a full characterization remains out of reach, we take the first steps with nontrivial upper and lower bounds in terms of covering numbers.",
    "url": "https://proceedings.mlr.press/v247/blanchard24a.html",
    "id": "https://proceedings.mlr.press/v247/blanchard24a.html",
    "pdf": "https://proceedings.mlr.press/v247/blanchard24a/blanchard24a.pdf",
    "authors": {
      "0_Mo\u00efse Blanchard": "Mo\u00efse Blanchard",
      "1_Doron Cohen": "Doron Cohen",
      "2_Aryeh Kontorovich": "Aryeh Kontorovich"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/blanchard24a/blanchard24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:551-595,\u00a02024.",
    "supplemental": ""
  },
  "151_Metric Clustering and MST with Strong and Weak Distance Oracles": {
    "title": "Metric Clustering and MST with Strong and Weak Distance Oracles",
    "abstract": "We study optimization problems in a metric space $(\\mathcal{X},d)$ where we can compute distances in two ways: via a \u201cstrong\u201d oracle that returns exact distances $d(x,y)$, and a \u201cweak\u201d oracle that returns distances $\\tilde{d}(x,y)$ which may be arbitrarily corrupted with some probability. This model captures the increasingly common trade-off between employing both an expensive similarity model (e.g. a large-scale embedding model), and a less accurate but cheaper model. Hence, the goal is to make as few queries to the strong oracle as possible. We consider both \u201cpoint queries\u201d, where the strong oracle is queried on a set of points $S \\subset \\cX $ and returns $d(x,y)$ for all $x,y \\in S$, and \u201cedge queries\u201d where it is queried for individual distances $d(x,y)$.  Our main contributions are optimal algorithms and lower bounds for clustering and Minimum Spanning Tree (MST) in this model. For $k$-centers, $k$-median, and $k$-means, we give constant factor approximation algorithms with only $\\tilde{O}(k)$ strong oracle point queries, and prove that $\\Omega(k)$ queries are required for any bounded approximation. For edge queries, our upper and lower bounds are both $\\tilde{\\Theta}(k^2)$.  Surprisingly, for the MST problem we give a $O(\\sqrt{\\log n})$ approximation algorithm using no strong oracle queries at all, and we prove a matching $\\Omega(\\sqrt{\\log n})$ lower bound which holds even if $\\Tilde{\\Omega}(n)$ strong oracle point queries are allowed. Furthermore, we empirically evaluate our algorithms, and show that their quality is comparable to that of the baseline algorithms that are given all true distances, but while querying the strong oracle on only a small fraction ($<1%$) of points.",
    "url": "https://proceedings.mlr.press/v247/bateni24a.html",
    "id": "https://proceedings.mlr.press/v247/bateni24a.html",
    "pdf": "https://proceedings.mlr.press/v247/bateni24a/bateni24a.pdf",
    "authors": {
      "0_MohammadHossein Bateni": "MohammadHossein Bateni",
      "1_Prathamesh Dharangutte": "Prathamesh Dharangutte",
      "2_Rajesh Jayaram": "Rajesh Jayaram",
      "3_Chen Wang": "Chen Wang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/bateni24a/bateni24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:498-550,\u00a02024.",
    "supplemental": ""
  },
  "152_Detection of L Geometry in Random Geometric Graphs Suboptimality of Triangles and Cluster Expansion": {
    "title": "Detection of $L_\u221e$ Geometry in Random Geometric Graphs: Suboptimality of Triangles and Cluster Expansion",
    "abstract": "In this paper we study the random geometric graph  $\\mathsf{RGG}(n,\\mathbb{T}^d,\\mathsf{Unif},\\sigma^q_p,p)$ with $L_q$ distance where each vertex is sampled uniformly from the $d$-dimensional torus and where the connection radius is chosen so that the marginal edge probability is $p$. In addition to results addressing other questions, we make progress on determining when it is possible to distinguish $\\mathsf{RGG}(n,\\mathbb{T}^d,\\mathsf{Unif},\\sigma^q_p,p)$ from the Erd\u0151s-R\u00e9nyi graph $\\ergraph$. Our strongest result is in the setting $q = \\infty$, in which case $\\mathsf{RGG}(n,\\mathbb{T}^d,\\mathsf{Unif},\\sigma^q_p,p)$ is the \\textsf{AND} of $d$ 1-dimensional random geometric graphs. We derive a formula similar to the \\emph{cluster-expansion} from statistical physics, capturing the compatibility of subgraphs from each of the $d$ 1-dimensional copies, and use it to bound the signed expectations of small subgraphs. We show that counting signed 4-cycles is optimal among all low-degree tests, succeeding with high probability if and only if $d = \\tilde{o}(np).$ In contrast, the signed triangle test is suboptimal and only succeeds when $d = \\tilde{o}((np)^{3/4}).$ Our result stands in  sharp contrast to the existing literature on random geometric graphs (mostly focused on $L_2$ geometry) where the signed triangle statistic is optimal.",
    "url": "https://proceedings.mlr.press/v247/bangachev24a.html",
    "id": "https://proceedings.mlr.press/v247/bangachev24a.html",
    "pdf": "https://proceedings.mlr.press/v247/bangachev24a/bangachev24a.pdf",
    "authors": {
      "0_Kiril Bangachev": "Kiril Bangachev",
      "1_Guy Bresler": "Guy Bresler"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/bangachev24a/bangachev24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:427-497,\u00a02024.",
    "supplemental": ""
  },
  "153_The SMART approach to instanceoptimal online learning": {
    "title": "The SMART approach to instance-optimal online learning",
    "abstract": "We devise an online learning algorithm \u2013 titled Switching via Monotone Adapted Regret Traces (SMART) \u2013 that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy.   We show that the regret of the SMART policy on any input sequence is within a multiplicative factor e/(e-1), approximately 1.58, of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical \u2018best-of-both-worlds\u2019 bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results follow from a reduction of instance optimal online learning to competitive analysis for the ski-rental problem.  We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a 1.43-fraction of the minimum regret achieved by FTL and the minimax-optimal policy. We present a modification of SMART that combines FTL with a \u201csmall-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound. ",
    "url": "https://proceedings.mlr.press/v247/banerjee24a.html",
    "id": "https://proceedings.mlr.press/v247/banerjee24a.html",
    "pdf": "https://proceedings.mlr.press/v247/banerjee24a/banerjee24a.pdf",
    "authors": {
      "0_Siddhartha Banerjee": "Siddhartha Banerjee",
      "1_Alankrita Bhatt": "Alankrita Bhatt",
      "2_Christina Lee Yu": "Christina Lee Yu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/banerjee24a/banerjee24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:426-426,\u00a02024.",
    "supplemental": ""
  },
  "154_Open Problem What is the Complexity of Joint Differential Privacy in Linear Contextual Bandits": {
    "title": "Open Problem: What is the Complexity of Joint Differential Privacy in Linear Contextual Bandits?",
    "abstract": "Contextual bandits serve as a theoretical framework to design recommender systems, which often rely on user-sensitive data, making privacy a critical concern. However, a significant gap remains between the known upper and lower bounds on the regret achievable in linear contextual bandits under Joint Differential Privacy (JDP), which is a popular privacy definition used in this setting. In particular, the best regret upper bound is known to be $O\\left(d \\sqrt{T} \\log(T) + \\textcolor{blue}{d^{3/4} \\sqrt{T \\log(1/\\delta)} / \\sqrt{\\epsilon}} \\right)$, while the lower bound is $\\Omega \\left(\\sqrt{d T \\log(K)} + \\textcolor{blue}{d/(\\epsilon + \\delta)}\\right)$. We discuss the recent progress on this problem, both from the algorithm design and lower bound techniques, and posit the open questions.",
    "url": "https://proceedings.mlr.press/v247/azize24a.html",
    "id": "https://proceedings.mlr.press/v247/azize24a.html",
    "pdf": "https://proceedings.mlr.press/v247/azize24a/azize24a.pdf",
    "authors": {
      "0_Achraf Azize": "Achraf Azize",
      "1_Debabrota Basu": "Debabrota Basu"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/azize24a/azize24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5306-5311,\u00a02024.",
    "supplemental": ""
  },
  "155_Learning Neural Networks with Sparse Activations": {
    "title": "Learning Neural Networks with Sparse Activations",
    "abstract": "A core component present in many successful neural network architectures, is an MLP block of two fully connected layers with a non-linear activation in between. An intriguing phenomenon observed empirically, including in transformer architectures, is that, after training, the activations in the hidden layer of this MLP block tend to be extremely sparse on any given input. Unlike traditional forms of sparsity, where there are neurons/weights which can be deleted from the network, this form of {\\em dynamic} activation sparsity appears to be harder to exploit to get more efficient networks.  Motivated by this we initiate a formal study of PAC learnability of MLP layers that exhibit activation sparsity. We present a variety of results showing that such classes of functions do lead to provable computational and statistical advantages over their non-sparse counterparts. Our hope is that a better theoretical understanding of {\\em sparsely activated} networks would lead to methods that can exploit activation sparsity in practice.",
    "url": "https://proceedings.mlr.press/v247/awasthi24a.html",
    "id": "https://proceedings.mlr.press/v247/awasthi24a.html",
    "pdf": "https://proceedings.mlr.press/v247/awasthi24a/awasthi24a.pdf",
    "authors": {
      "0_Pranjal Awasthi": "Pranjal Awasthi",
      "1_Nishanth Dikkala": "Nishanth Dikkala",
      "2_Pritish Kamath": "Pritish Kamath",
      "3_Raghu Meka": "Raghu Meka"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/awasthi24a/awasthi24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:406-425,\u00a02024.",
    "supplemental": ""
  },
  "156_Universal Rates for Regression Separations between CutOff and Absolute Loss": {
    "title": "Universal Rates for Regression: Separations between Cut-Off and Absolute Loss",
    "abstract": "In this work we initiate the study of regression in the universal rates framework of Bousquet et al. Unlike the traditional uniform learning setting, we are interested in obtaining learning guarantees that hold for all fixed data-generating distributions, but do not hold uniformly across them. We focus on the realizable setting and we consider two different well-studied loss functions: the cut-off loss at scale $\\gamma > 0$, which asks for predictions that are $\\gamma$-close to the correct one, and the absolute loss, which measures how far away the prediction is from the correct one.  Our results show that the landscape of the achievable rates in the two cases is completely different. First we give a trichotomic characterization of the optimal learning rates under the cut-off loss: each class is learnable either at an exponential rate, a (nearly) linear rate or requires arbitrarily slow rates. Moving to the absolute loss, we show that the achievable learning rates are significantly more involved by illustrating that an infinite number of different optimal learning rates is achievable. This is the first time that such a rich landscape of rates is obtained in the universal rates literature.",
    "url": "https://proceedings.mlr.press/v247/attias24a.html",
    "id": "https://proceedings.mlr.press/v247/attias24a.html",
    "pdf": "https://proceedings.mlr.press/v247/attias24a/attias24a.pdf",
    "authors": {
      "0_Idan Attias": "Idan Attias",
      "1_Steve Hanneke": "Steve Hanneke",
      "2_Alkis Kalavasis": "Alkis Kalavasis",
      "3_Amin Karbasi": "Amin Karbasi",
      "4_Grigoris Velegkas": "Grigoris Velegkas"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/attias24a/attias24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:359-405,\u00a02024.",
    "supplemental": ""
  },
  "157_The Best Arm Evades Nearoptimal Multipass Streaming Lower Bounds for Pure Exploration in Multiarmed Bandits": {
    "title": "The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits",
    "abstract": "We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(n/\\Delta^2)$ requires $\\Omega(\\log{(1/\\Delta)}/\\log\\log{(1/\\Delta)})$ passes. Here, $n$ is the number of arms and $\\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\\log(1/\\Delta))$ pass algorithm of Jin et al. [ICML\u201921] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC\u201920].",
    "url": "https://proceedings.mlr.press/v247/assadi24a.html",
    "id": "https://proceedings.mlr.press/v247/assadi24a.html",
    "pdf": "https://proceedings.mlr.press/v247/assadi24a/assadi24a.pdf",
    "authors": {
      "0_Sepehr Assadi": "Sepehr Assadi",
      "1_Chen Wang": "Chen Wang"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/assadi24a/assadi24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:311-358,\u00a02024.",
    "supplemental": ""
  },
  "158_Open Problem Can Local Regularization Learn All Multiclass Problems": {
    "title": "Open Problem: Can Local Regularization Learn All Multiclass Problems?",
    "abstract": "Multiclass classification is the simple generalization of binary classification to arbitrary label sets. Despite its simplicity, it has been remarkably resistant to study: a characterization of multiclass learnability was established only two years ago by Brukhim et al. 2022, and the understanding of optimal learners for multiclass problems remains fairly limited. We ask whether there exists a simple algorithmic template \u2014 akin to empirical risk minimization (ERM) for binary classification \u2014 which characterizes multiclass learning. Namely, we ask whether local regularization, introduced by Asilis et al. 2024, is sufficiently expressive to learn all multiclass problems possible. Towards (negatively) resolving the problem, we propose a hypothesis class which may not be learnable by any such local regularizer.",
    "url": "https://proceedings.mlr.press/v247/asilis24b.html",
    "id": "https://proceedings.mlr.press/v247/asilis24b.html",
    "pdf": "https://proceedings.mlr.press/v247/asilis24b/asilis24b.pdf",
    "authors": {
      "0_Julian Asilis": "Julian Asilis",
      "1_Siddartha Devic": "Siddartha Devic",
      "2_Shaddin Dughmi": "Shaddin Dughmi",
      "3_Vatsal Sharan": "Vatsal Sharan",
      "4_Shang-Hua Teng": "Shang-Hua Teng"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/asilis24b/asilis24b.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:5301-5305,\u00a02024.",
    "supplemental": ""
  },
  "159_Regularization and Optimal Multiclass Learning": {
    "title": "Regularization and Optimal Multiclass Learning",
    "abstract": "The quintessential learning algorithm of empirical risk minimization (ERM) is known to fail in various settings for which uniform convergence does not characterize learning. Relatedly, the practice of machine learning is rife with considerably richer algorithmic techniques, perhaps the most notable of which is regularization. Nevertheless, no such technique or principle has broken away from the pack to characterize optimal learning in these more general settings. The purpose of this work is to precisely characterize the role of regularization in perhaps the simplest setting for which ERM fails: multiclass learning with arbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal learning algorithms that dovetail with tried-and-true algorithmic principles: Occam\u2019s Razor as embodied by structural risk minimization (SRM), the principle of maximum entropy, and Bayesian inference. We also extract from OIGs a combinatorial sequence we term the Hall complexity, which is the first to characterize a problem\u2019s transductive error rate exactly. Lastly, we introduce a generalization of OIGs and the transductive learning setting to the agnostic case, where we show that optimal orientations of Hamming graphs \u2013 judged using nodes\u2019 outdegrees minus a system of node-dependent credits \u2013 characterize optimal learners exactly. We demonstrate that an agnostic version of the Hall complexity again characterizes error rates exactly, and exhibit an optimal learner using maximum entropy programs.",
    "url": "https://proceedings.mlr.press/v247/asilis24a.html",
    "id": "https://proceedings.mlr.press/v247/asilis24a.html",
    "pdf": "https://proceedings.mlr.press/v247/asilis24a/asilis24a.pdf",
    "authors": {
      "0_Julian Asilis": "Julian Asilis",
      "1_Siddartha Devic": "Siddartha Devic",
      "2_Shaddin Dughmi": "Shaddin Dughmi",
      "3_Vatsal Sharan": "Vatsal Sharan",
      "4_Shang-Hua Teng": "Shang-Hua Teng"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/asilis24a/asilis24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:260-310,\u00a02024.",
    "supplemental": ""
  },
  "160_Universally InstanceOptimal Mechanisms for Private Statistical Estimation": {
    "title": "Universally Instance-Optimal Mechanisms for Private Statistical Estimation",
    "abstract": "     We consider the problem of instance-optimal statistical estimation under the constraint of differential privacy where mechanisms must adapt to the difficulty of the input dataset. We prove a new instance specific lower bound using a new divergence and show it characterizes the local minimax optimal rates for private statistical estimation. We propose two new mechanisms that are universally instance-optimal for general estimation problems up to logarithmic factors. Our first mechanism, the total variation mechanism, builds on the exponential mechanism with stable approximations of the total variation distance, and is universally instance-optimal in the high privacy regime $\\epsilon \\leq 1/\\sqrt{n}$. Our second mechanism, the T-mechanism, is based on the T-estimator framework (Birg{\u00e9}, 2006) using the clipped log likelihood ratio as a stable test: it attains instance-optimal rates for any $\\epsilon \\leq 1$ up to logarithmic factors. Finally, we study the implications of our results to robust statistical estimation, and show that our algorithms are universally optimal for this problem, characterizing the optimal minimax rates for robust statistical estimation. ",
    "url": "https://proceedings.mlr.press/v247/asi24a.html",
    "id": "https://proceedings.mlr.press/v247/asi24a.html",
    "pdf": "https://proceedings.mlr.press/v247/asi24a/asi24a.pdf",
    "authors": {
      "0_Hilal Asi": "Hilal Asi",
      "1_John C. Duchi": "John C. Duchi",
      "2_Saminul Haque": "Saminul Haque",
      "3_Zewei Li": "Zewei Li",
      "4_Feng Ruan": "Feng Ruan"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/asi24a/asi24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:221-259,\u00a02024.",
    "supplemental": ""
  },
  "161_Mode Estimation with Partial Feedback": {
    "title": "Mode Estimation with Partial Feedback",
    "abstract": " The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize key aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution with partial feedback. We showcase how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our original problem. ",
    "url": "https://proceedings.mlr.press/v247/arnal24a.html",
    "id": "https://proceedings.mlr.press/v247/arnal24a.html",
    "pdf": "https://proceedings.mlr.press/v247/arnal24a/arnal24a.pdf",
    "authors": {
      "0_Charles Arnal": "Charles Arnal",
      "1_Vivien Cabannes": "Vivien Cabannes",
      "2_Vianney Perchet": "Vianney Perchet"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/arnal24a/arnal24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:219-220,\u00a02024.",
    "supplemental": ""
  },
  "162_Two fundamental limits for uncertainty quantification in predictive inference": {
    "title": "Two fundamental limits for uncertainty quantification in predictive inference",
    "abstract": "We study the statistical hardness of estimating two basic representations of uncertainty in predictive inference: prediction sets and calibration error. First, we show that conformal prediction sets cannot approach a desired weighted conformal coverage level\u2014with respect to a family of binary witness functions with VC dimension $d$\u2014at a minimax rate faster than $O(d^{1/2}n^{-1/2})$. We also show that the algorithm in Gibbs et al. (2023) achieves this rate and that extending our class of conformal sets beyond thresholds of non-conformity scores to include arbitrary convex sets of non-conformity scores only improves the minimax rate by a constant factor. Then, under a similar VC dimension constraint on the witness function class, we show it is not possible to estimate the weighted weak calibration error at a minimax rate faster than $O(d^{1/4}n^{-1/2})$. We show that the algorithm in Kumar et al. (2019) achieves this rate in the particular case of estimating the squared weak calibration error of a predictor that outputs $d$ distinct values.",
    "url": "https://proceedings.mlr.press/v247/areces24a.html",
    "id": "https://proceedings.mlr.press/v247/areces24a.html",
    "pdf": "https://proceedings.mlr.press/v247/areces24a/areces24a.pdf",
    "authors": {
      "0_Felipe Areces": "Felipe Areces",
      "1_Chen Cheng": "Chen Cheng",
      "2_John Duchi": "John Duchi",
      "3_Kuditipudi Rohith": "Kuditipudi Rohith"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/areces24a/areces24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:186-218,\u00a02024.",
    "supplemental": ""
  },
  "163_Fast parallel sampling under isoperimetry": {
    "title": "Fast parallel sampling under isoperimetry",
    "abstract": "We show how to sample in parallel from a distribution $\\pi$ over $\\mathbb{R}^d$ that satisfies a log-Sobolev inequality and has a smooth log-density, by parallelizing the Langevin (resp. underdamped Langevin) algorithms. We show that our algorithm outputs samples from a distribution $\\hat{\\pi}$ that is close to $\\pi$ in Kullback\u2013Leibler (KL) divergence (resp. total variation (TV) distance), while using only $\\log(d)^{O(1)}$ parallel rounds and $\\widetilde{O}(d)$ (resp. $\\widetilde O(\\sqrt d)$) gradient evaluations in total. This constitutes the first parallel sampling algorithms with TV distance guarantees. For our main application, we show how to combine the TV distance guarantees of our algorithms with prior works and obtain RNC sampling-to-counting reductions for families of discrete distribution on the hypercube $\\{\\pm 1\\}^n$  that are closed under exponential tilts and have bounded covariance. Consequently, we obtain an RNC sampler for directed Eulerian tours and asymmetric determinantal point processes, resolving open questions raised in prior works.",
    "url": "https://proceedings.mlr.press/v247/anari24a.html",
    "id": "https://proceedings.mlr.press/v247/anari24a.html",
    "pdf": "https://proceedings.mlr.press/v247/anari24a/anari24a.pdf",
    "authors": {
      "0_Nima Anari": "Nima Anari",
      "1_Sinho Chewi": "Sinho Chewi",
      "2_Thuy-Duong Vuong": "Thuy-Duong Vuong"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/anari24a/anari24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:161-185,\u00a02024.",
    "supplemental": ""
  },
  "164_Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning": {
    "title": "Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning",
    "abstract": "A pervasive phenomenon in machine learning applications is \\emph{distribution shift}, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. This paper studies the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and \\emph{adversarial covariate shift}, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable \\emph{misspecification amplification} where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm\u2014inspired by robust optimization techniques\u2014that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.",
    "url": "https://proceedings.mlr.press/v247/amortila24a.html",
    "id": "https://proceedings.mlr.press/v247/amortila24a.html",
    "pdf": "https://proceedings.mlr.press/v247/amortila24a/amortila24a.pdf",
    "authors": {
      "0_Philip Amortila": "Philip Amortila",
      "1_Tongyi Cao": "Tongyi Cao",
      "2_Akshay Krishnamurthy": "Akshay Krishnamurthy"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/amortila24a/amortila24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:130-160,\u00a02024.",
    "supplemental": ""
  },
  "165_A Unified Characterization of Private Learnability via Graph Theory": {
    "title": "A Unified Characterization of Private Learnability via Graph Theory",
    "abstract": "We provide a unified framework for characterizing pure and approximate differentially private (DP) learnability. The framework uses the language of graph theory: for a concept class $\\mathcal{H}$, we define the contradiction graph $G$ of $\\mathcal{H}$. Its vertices are realizable datasets and two datasets\u00a0$S,S\u2019$ are connected by an edge if they contradict each other (i.e., there is a point $x$ that is labeled differently in\u00a0$S$ and $S\u2019$). Our main finding is that the combinatorial structure of $G$ is deeply related to learning\u00a0$\\mathcal{H}$ under DP. Learning $\\mathcal{H}$ under pure DP is captured by the fractional clique number of $G$. Learning $\\mathcal{H}$ under approximate DP is captured by the clique number of $G$. Consequently, we identify graph-theoretic dimensions that characterize DP learnability: the \\emph{clique dimension} and \\emph{fractional clique dimension}. Along the way, we reveal properties of the contradiction graph which may be of independent interest. We also suggest several open questions and directions for future research.",
    "url": "https://proceedings.mlr.press/v247/alon24a.html",
    "id": "https://proceedings.mlr.press/v247/alon24a.html",
    "pdf": "https://proceedings.mlr.press/v247/alon24a/alon24a.pdf",
    "authors": {
      "0_Noga Alon": "Noga Alon",
      "1_Shay Moran": "Shay Moran",
      "2_Hilla Schefler": "Hilla Schefler",
      "3_Amir Yehudayoff": "Amir Yehudayoff"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/alon24a/alon24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:94-129,\u00a02024.",
    "supplemental": ""
  },
  "166_Metalearning with Very Few Samples Per Task": {
    "title": "Metalearning with Very Few Samples Per Task",
    "abstract": "    Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own.  In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new, previously unseen tasks from the metadistribution. In this work, we consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ of interest can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in \\mathcal{H}$ is a map from features to some representation space that is shared across tasks, and $f_{P} \\in \\mathcal{F}$ is a task-specific classifier from the representation space to labels.  The main question we ask in this work is how much data do we need to metalearn a good representation? Here, the amount of data is measured in terms of both the number of tasks $t$ that we need to see and the number of samples $n$ per task. We focus on the regime where the number of samples per task is extremely small. Our main result shows that, in a distribution-free setting where the feature vectors are in $\\mathbb{R}^d$, the representation is a linear map from $\\mathbb{R}^d \\to \\mathbb{R}^k$, and the task-specific classifiers are halfspaces in $\\mathbb{R}^k$, we can metalearn a representation with error $\\varepsilon$ using just $n = k+2$ samples per task, and $d \\cdot (1/\\varepsilon)^{O(k)}$ tasks.  Learning with so few samples per task is remarkable because metalearning would be impossible with $k+1$ samples per task, and because we cannot even hope to learn an accurate task-specific classifier with just $k+2$ samples per task.  To obtain this result, we develop a sample-and-task-complexity theory for distribution-free metalearning and multitask learning, which identifies what properties of $\\mathcal{F}$ and $\\mathcal{H}$ make metalearning possible with few samples per task.  Our theory also yields a simple characterization of distribution-free multitask learning.  Finally, we give sample-efficient reductions between metalearning and multitask learning, which, when combined with our characterization of multitask learning, give a characterization of metalearning in certain parameter regimes.",
    "url": "https://proceedings.mlr.press/v247/aliakbarpour24a.html",
    "id": "https://proceedings.mlr.press/v247/aliakbarpour24a.html",
    "pdf": "https://proceedings.mlr.press/v247/aliakbarpour24a/aliakbarpour24a.pdf",
    "authors": {
      "0_Maryam Aliakbarpour": "Maryam Aliakbarpour",
      "1_Konstantina Bairaktari": "Konstantina Bairaktari",
      "2_Gavin Brown": "Gavin Brown",
      "3_Adam Smith": "Adam Smith",
      "4_Nathan Srebro": "Nathan Srebro",
      "5_Jonathan Ullman": "Jonathan Ullman"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/aliakbarpour24a/aliakbarpour24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:46-93,\u00a02024.",
    "supplemental": ""
  },
  "167_Conference on Learning Theory 2024 Preface": {
    "title": "Conference on Learning Theory 2024: Preface",
    "abstract": null,
    "url": "https://proceedings.mlr.press/v247/agrawal24a.html",
    "id": "https://proceedings.mlr.press/v247/agrawal24a.html",
    "pdf": "https://proceedings.mlr.press/v247/agrawal24a/agrawal24a.pdf",
    "authors": {
      "0_Shipra Agrawal": "Shipra Agrawal",
      "1_Aaron Roth": "Aaron Roth"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/agrawal24a/agrawal24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:i-i,\u00a02024.",
    "supplemental": ""
  },
  "168_MajorityofThree The Simplest Optimal Learner": {
    "title": "Majority-of-Three: The Simplest Optimal Learner?",
    "abstract": "Developing an optimal PAC learning algorithm in the realizable setting, where empirical risk minimization (ERM) is suboptimal, was a major open problem in learning theory for decades. The problem was finally resolved by Hanneke a few years ago. Unfortunately, Hanneke\u2019s algorithm is quite complex as it returns the majority vote of many ERM classifiers that are trained on carefully selected subsets of the data. It is thus a natural goal to determine the simplest algorithm that is optimal. In this work we study the arguably simplest algorithm that could be optimal:  returning the majority vote of three ERM classifiers. We show that this algorithm achieves the optimal in-expectation bound on its error which is provably unattainable by a single ERM classifier.  Furthermore, we prove a near-optimal high-probability bound on this algorithm\u2019s error.  We conjecture that a better analysis will prove that this algorithm is in fact optimal in the high-probability regime.",
    "url": "https://proceedings.mlr.press/v247/aden-ali24a.html",
    "id": "https://proceedings.mlr.press/v247/aden-ali24a.html",
    "pdf": "https://proceedings.mlr.press/v247/aden-ali24a/aden-ali24a.pdf",
    "authors": {
      "0_Ishaq Aden-Ali": "Ishaq Aden-Ali",
      "1_Mikael M\u00f8ller H\u00f8andgsgaard": "Mikael M\u00f8ller H\u00f8andgsgaard",
      "2_Kasper Green Larsen": "Kasper Green Larsen",
      "3_Nikita Zhivotovskiy": "Nikita Zhivotovskiy"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/aden-ali24a/aden-ali24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:22-45,\u00a02024.",
    "supplemental": ""
  },
  "169_Limits of Approximating the Median Treatment Effect": {
    "title": "Limits of Approximating the Median Treatment Effect",
    "abstract": "Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\\mathbf{a}, \\mathbf{b}$, much of the prior work focused on estimating median$(\\mathbf{a}) -$ median$(\\mathbf{b})$, as it is easier to estimate than the desired estimand of median$(\\mathbf{a-b})$, called the Median Treatment Effect (MTE). In this work, we argue that MTE is not estimable and detail a novel notion of approximation that relies on the sorted order of the values in $\\mathbf{a-b}$: we approximate the median by a value whose quantiles in $\\mathbf{a-b}$ are close to $0.5$ (median). Next, we identify a quantity called \\emph{variability} that exactly captures the complexity of MTE estimation. Using this, we establish that when potential outcomes take values in the set $\\{0,1,\\ldots,k-1\\}$ the worst-case (over inputs $\\mathbf{a,b}$) optimal (over algorithms) approximation factor of the MTE is $\\frac{1}{2}\\cdot \\frac{2k-3}{2k-1}$. Further, by drawing connections to the notions of instance-optimality studied in theoretical computer science, we show that \\emph{every} algorithm for estimating the MTE obtains an approximation error that is no better than the error of an algorithm that computes variability, on roughly a per input basis: hence, variability leads to an almost instance optimal approximation algorithm for estimating the MTE. Finally, we provide a simple linear time algorithm for computing the variability exactly. Unlike much prior works, a particular highlight of our work is that we make no assumptions about how the potential outcome vectors are generated or how they are correlated, except that the potential outcome values are $k$-ary, i.e., take one of $k$ discrete values $\\{0,1,\\ldots,k-1\\}$.",
    "url": "https://proceedings.mlr.press/v247/addanki24a.html",
    "id": "https://proceedings.mlr.press/v247/addanki24a.html",
    "pdf": "https://proceedings.mlr.press/v247/addanki24a/addanki24a.pdf",
    "authors": {
      "0_Raghavendra Addanki": "Raghavendra Addanki",
      "1_Siddharth Bhandari": "Siddharth Bhandari"
    },
    "pmlr_pdf": "https://proceedings.mlr.press/v247/addanki24a/addanki24a.pdf",
    "github_url": "",
    "conf_info": "Proceedings of Thirty Seventh Conference on Learning Theory,\u00a0PMLR 247:1-21,\u00a02024.",
    "supplemental": ""
  }
}